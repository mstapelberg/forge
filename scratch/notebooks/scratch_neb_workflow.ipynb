{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a full NEB workflow for migration energy barrier calculations with NEB and MACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to run a vacancy diffusion workflow using the Forge Package. \n",
    "\n",
    "The workflow is as follows:\n",
    "1. Create a random alloy composition\n",
    "2. Run Monte Carlo to get an energetically favorable order for the structure\n",
    "3. Relax the alloy structure and it's cell\n",
    "4. Pick a vacancy index randomly from the structure\n",
    "5. Sample the nearest and next nearest neighbors of the vacancy\n",
    "6. Run the vacancy diffusion using the DyNEB method on the vacancy and the target index (picked at random from the nearest neighbors)\n",
    "7. Relax the vacancy start and end points (handled in the VacancyDiffusion object)\n",
    "8. Run the NEB simulation \n",
    "9. Plot the migration energy barrier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the necessary packages and modules for FORGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "from ase.build import bulk \n",
    "from forge.workflows.mcmc import MonteCarloAlloySampler\n",
    "from mace.calculators.mace import MACECalculator \n",
    "\n",
    "# For Step 1, getting the composition we want. \n",
    "from forge.analysis.composition import CompositionAnalyzer\n",
    "from math import ceil, floor\n",
    "\n",
    "# For checking the device and getting absolute paths\n",
    "import forge \n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# For creating the vacancy diffusion object in step 4-9\n",
    "from forge.workflows.neb import VacancyDiffusion\n",
    "\n",
    "# For plotting the scaled energies\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the composition and initializing the random alloy structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16da0f0d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set all the seeds\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Composition of our alloy is {'V': 0.8727, 'Cr': 0.0301, 'Ti': 0.0417, 'W': 0.0509, 'Zr': 0.0046}\n"
     ]
    }
   ],
   "source": [
    "N = 6 # sets the size of the supercell\n",
    "num_atoms = 2*N**3\n",
    "\n",
    "# Get the atomic fractions for the alloy correctly rounded to 4 decimal places for the given super cell size\n",
    "x_zr = round(ceil(0.0025 * num_atoms)/num_atoms, 4)\n",
    "x_cr = round(ceil(0.03 * num_atoms)/num_atoms, 4)\n",
    "x_ti = round(ceil(0.04 * num_atoms)/num_atoms, 4)\n",
    "x_w = round(ceil(0.05 * num_atoms)/num_atoms, 4)\n",
    "x_v = round(1 - x_zr - x_cr - x_ti - x_w, 4)\n",
    "\n",
    "# Adjust x_v if the sum of the composition is not 1\n",
    "if x_zr + x_cr + x_ti + x_w + x_v != 1:\n",
    "    x_v = 1 - x_zr - x_cr - x_ti - x_w\n",
    "    \n",
    "composition = {\n",
    "    'V' : x_v,\n",
    "    'Cr' : x_cr,\n",
    "    'Ti' : x_ti,\n",
    "    'W' : x_w,\n",
    "    'Zr' : x_zr\n",
    "}\n",
    "\n",
    "print(f\"The Composition of our alloy is {composition}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the randomly ordered alloy structure using the CompositionAnalyzer class. This creates an ASE atoms object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atoms(symbols='Cr14Ti19V375W22Zr2', pbc=True, cell=[18.06, 18.06, 18.06])\n"
     ]
    }
   ],
   "source": [
    "analyzer = CompositionAnalyzer()\n",
    "atoms = analyzer.create_random_alloy(composition = composition, \n",
    "                                         crystal_type = 'bcc', \n",
    "                                         dimensions=[N,N,N], \n",
    "                                         lattice_constant = 3.01,\n",
    "                                         balance_element = 'V', \n",
    "                                         cubic=True)\n",
    "\n",
    "print(atoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordering and then Relaxing the alloy structure and it's cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this step, we will load the MACE model we want to use, run the Monte Carlo to get an energetically favorable order for the structure, and then relax the alloy structure and it's cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "Running MC: 100%|██████████| 432/432 [01:22<00:00,  5.26steps/s, energy=-3948.239 eV]\n"
     ]
    }
   ],
   "source": [
    "# The model path should point to your MACE model that you want to use.\n",
    "model_path = '../potentials/gen_6_model_0_L0_isolated-2026-01-16_stagetwo.model'\n",
    "\n",
    "# This is some boiler plate code to check the device and enable CUEQ if we are using a GPU.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    enable_cueq = True\n",
    "else:\n",
    "    enable_cueq = False\n",
    "\n",
    "# Define the mace calculator\n",
    "calc = MACECalculator(model_paths=[model_path],\n",
    "                      device=device,\n",
    "                      default_dtype=\"float32\",\n",
    "                      enable_cueq=enable_cueq)\n",
    "\n",
    "# Attach the calculator to the atoms object\n",
    "atoms.calc = calc\n",
    "\n",
    "# Define the temperature and the number of steps per atom\n",
    "temperature = 600+273.15\n",
    "steps_per_atom = 1 # in this test we do only 1, in real life you want to do 50-100 steps per atom\n",
    "total_swaps = steps_per_atom * len(atoms)\n",
    "\n",
    "# Define the Monte Carlo sampler object\n",
    "mc_sampler = MonteCarloAlloySampler(\n",
    "    atoms=atoms,\n",
    "    calculator=calc,\n",
    "    temperature=temperature,\n",
    "    steps=total_swaps,\n",
    ")\n",
    "\n",
    "# Run the Monte Carlo sampler\n",
    "final_atoms = mc_sampler.run_mcmc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a wrapper for the ASE relaxation function to relax the structure and it's cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take in the final atoms and relax the structure \n",
    "from forge.workflows.relax import relax \n",
    "\n",
    "relaxed_atoms = relax(atoms = final_atoms, \n",
    "                     calculator = calc, \n",
    "                     relax_cell = True, \n",
    "                     fmax = 0.04, \n",
    "                     steps = 100, \n",
    "                     optimizer = \"FIRE\",\n",
    "                     verbose = 0,\n",
    "                     interval = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the vacancy diffusion object and running the vacancy diffusion NEB calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step creates the VacancyDiffusion object and runs the vacancy diffusion NEB calculation on a single vacancy and target index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the VacancyDiffusion object\n",
    "\n",
    "neb = VacancyDiffusion(atoms = relaxed_atoms,\n",
    "                       model_path = model_path\n",
    "                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vacancy index: 102\n",
      "Neighbors: [(102, 19), (102, 91), (102, 17), (102, 29), (102, 103), (102, 89), (102, 101), (102, 31), (102, 90)]\n",
      "Target index: 19\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# get the vacancy index randomly from the len of atoms\n",
    "vac_idx = np.random.randint(0, len(atoms))\n",
    "\n",
    "print(f\"Vacancy index: {vac_idx}\")\n",
    "\n",
    "# get the neighbors of the vacancy index, #TODO separate the nearest and next nearest neighbors results, maybe using a dictionary \n",
    "neighbors = neb.sample_neighbors(vacancy_indices=[vac_idx], n_nearest=8, n_next_nearest=1)\n",
    "\n",
    "# print the neighbors\n",
    "print(f\"Neighbors: {neighbors}\")\n",
    "\n",
    "# get the target index from the neighbors\n",
    "tar_idx = neighbors[0][1]\n",
    "\n",
    "print(f\"Target index: {tar_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 15:31:45    -3940.856445        0.438320\n",
      "FIRE:    1 15:31:45    -3940.868652        0.420001\n",
      "FIRE:    2 15:31:46    -3940.891602        0.385125\n",
      "FIRE:    3 15:31:47    -3940.919434        0.336982\n",
      "FIRE:    4 15:31:48    -3940.945801        0.280060\n",
      "FIRE:    5 15:31:50    -3940.966797        0.221654\n",
      "FIRE:    6 15:31:51    -3940.979492        0.187095\n",
      "FIRE:    7 15:31:52    -3940.991699        0.150568\n",
      "FIRE:    8 15:31:53    -3940.998047        0.148041\n",
      "FIRE:    9 15:31:54    -3941.005371        0.138817\n",
      "FIRE:   10 15:31:54    -3941.010254        0.162648\n",
      "FIRE:   11 15:31:55    -3941.014648        0.209600\n",
      "FIRE:   12 15:31:56    -3941.015625        0.203700\n",
      "FIRE:   13 15:31:56    -3941.018555        0.192136\n",
      "FIRE:   14 15:31:57    -3941.021973        0.175383\n",
      "FIRE:   15 15:31:57    -3941.023438        0.154104\n",
      "FIRE:   16 15:31:58    -3941.028320        0.129161\n",
      "FIRE:   17 15:31:59    -3941.029297        0.101588\n",
      "FIRE:   18 15:31:59    -3941.031250        0.074602\n",
      "FIRE:   19 15:32:00    -3941.033691        0.070211\n",
      "FIRE:   20 15:32:00    -3941.034180        0.064952\n",
      "FIRE:   21 15:32:01    -3941.035645        0.058855\n",
      "FIRE:   22 15:32:01    -3941.036621        0.054672\n",
      "FIRE:   23 15:32:02    -3941.038086        0.074088\n",
      "FIRE:   24 15:32:03    -3941.038574        0.084223\n",
      "FIRE:   25 15:32:03    -3941.038574        0.083099\n",
      "FIRE:   26 15:32:04    -3941.040527        0.069155\n",
      "FIRE:   27 15:32:05    -3941.042969        0.042209\n",
      "FIRE:   28 15:32:05    -3941.044434        0.026929\n",
      "FIRE:   29 15:32:06    -3941.044922        0.031626\n",
      "FIRE:   30 15:32:07    -3941.044922        0.029489\n",
      "FIRE:   31 15:32:07    -3941.044922        0.025382\n",
      "FIRE:   32 15:32:08    -3941.046875        0.019644\n",
      "FIRE:   33 15:32:08    -3941.046387        0.012785\n",
      "FIRE:   34 15:32:09    -3941.045410        0.007950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 15:32:10    -3941.067383        0.766821\n",
      "FIRE:    1 15:32:10    -3941.091797        0.716991\n",
      "FIRE:    2 15:32:11    -3941.135742        0.659170\n",
      "FIRE:    3 15:32:12    -3941.191406        0.604567\n",
      "FIRE:    4 15:32:12    -3941.250488        0.538756\n",
      "FIRE:    5 15:32:13    -3941.304199        0.462198\n",
      "FIRE:    6 15:32:13    -3941.349121        0.372946\n",
      "FIRE:    7 15:32:14    -3941.380371        0.270329\n",
      "FIRE:    8 15:32:15    -3941.400391        0.192766\n",
      "FIRE:    9 15:32:15    -3941.408691        0.241210\n",
      "FIRE:   10 15:32:16    -3941.409180        0.234323\n",
      "FIRE:   11 15:32:16    -3941.410156        0.220743\n",
      "FIRE:   12 15:32:17    -3941.416016        0.201174\n",
      "FIRE:   13 15:32:17    -3941.419922        0.180107\n",
      "FIRE:   14 15:32:18    -3941.425781        0.155351\n",
      "FIRE:   15 15:32:18    -3941.429688        0.127934\n",
      "FIRE:   16 15:32:19    -3941.434570        0.099146\n",
      "FIRE:   17 15:32:20    -3941.438477        0.083122\n",
      "FIRE:   18 15:32:20    -3941.440430        0.072861\n",
      "FIRE:   19 15:32:21    -3941.443359        0.083926\n",
      "FIRE:   20 15:32:21    -3941.443359        0.092670\n",
      "FIRE:   21 15:32:22    -3941.447266        0.108892\n",
      "FIRE:   22 15:32:23    -3941.449707        0.116317\n",
      "FIRE:   23 15:32:23    -3941.452637        0.107402\n",
      "FIRE:   24 15:32:24    -3941.456055        0.083379\n",
      "FIRE:   25 15:32:24    -3941.459473        0.049830\n",
      "FIRE:   26 15:32:25    -3941.463379        0.036213\n",
      "FIRE:   27 15:32:25    -3941.465820        0.038271\n",
      "FIRE:   28 15:32:26    -3941.467285        0.053629\n",
      "FIRE:   29 15:32:26    -3941.470215        0.058797\n",
      "FIRE:   30 15:32:27    -3941.470215        0.056588\n",
      "FIRE:   31 15:32:27    -3941.470215        0.052395\n",
      "FIRE:   32 15:32:28    -3941.471191        0.046651\n",
      "FIRE:   33 15:32:28    -3941.471680        0.039905\n",
      "FIRE:   34 15:32:29    -3941.471191        0.032711\n",
      "FIRE:   35 15:32:30    -3941.474121        0.025523\n",
      "FIRE:   36 15:32:30    -3941.474609        0.018566\n",
      "FIRE:   37 15:32:31    -3941.474121        0.020775\n",
      "FIRE:   38 15:32:31    -3941.474609        0.023699\n",
      "FIRE:   39 15:32:32    -3941.474121        0.027772\n",
      "FIRE:   40 15:32:32    -3941.477051        0.029824\n",
      "FIRE:   41 15:32:33    -3941.476074        0.023258\n",
      "FIRE:   42 15:32:33    -3941.475586        0.013106\n",
      "FIRE:   43 15:32:34    -3941.477539        0.021762\n",
      "FIRE:   44 15:32:35    -3941.479980        0.024895\n",
      "FIRE:   45 15:32:35    -3941.480469        0.020025\n",
      "FIRE:   46 15:32:36    -3941.482910        0.023041\n",
      "FIRE:   47 15:32:36    -3941.484375        0.021491\n",
      "FIRE:   48 15:32:37    -3941.483887        0.011419\n",
      "FIRE:   49 15:32:37    -3941.484863        0.011241\n",
      "FIRE:   50 15:32:38    -3941.485840        0.015291\n",
      "FIRE:   51 15:32:38    -3941.485352        0.010025\n",
      "FIRE:   52 15:32:39    -3941.485840        0.010219\n",
      "FIRE:   53 15:32:40    -3941.488281        0.011353\n",
      "FIRE:   54 15:32:40    -3941.487305        0.005007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 15:32:45    -3939.788574        1.833366\n",
      "FIRE:    1 15:32:48    -3939.880371        1.481022\n",
      "FIRE:    2 15:32:51    -3940.016113        1.110751\n",
      "FIRE:    3 15:32:53    -3940.132812        0.721057\n",
      "FIRE:    4 15:32:56    -3940.195801        0.394750\n",
      "FIRE:    5 15:32:59    -3940.211426        0.698655\n",
      "FIRE:    6 15:33:01    -3940.207520        0.808282\n",
      "FIRE:    7 15:33:04    -3940.212891        0.779367\n",
      "FIRE:    8 15:33:07    -3940.219238        0.722558\n",
      "FIRE:    9 15:33:10    -3940.229980        0.639917\n",
      "FIRE:   10 15:33:12    -3940.242676        0.534530\n",
      "FIRE:   11 15:33:15    -3940.251953        0.410625\n",
      "FIRE:   12 15:33:18    -3940.262207        0.273816\n",
      "FIRE:   13 15:33:20    -3940.269043        0.231196\n",
      "FIRE:   14 15:33:23    -3940.273926        0.220954\n",
      "FIRE:   15 15:33:26    -3940.274902        0.211873\n",
      "FIRE:   16 15:33:29    -3940.271973        0.289038\n",
      "FIRE:   17 15:33:32    -3940.270996        0.354046\n",
      "FIRE:   18 15:33:35    -3940.274414        0.352980\n",
      "FIRE:   19 15:33:38    -3940.277832        0.279550\n",
      "FIRE:   20 15:33:41    -3940.286621        0.224838\n",
      "FIRE:   21 15:33:43    -3940.292969        0.144563\n",
      "FIRE:   22 15:33:46    -3940.296387        0.198781\n",
      "FIRE:   23 15:33:49    -3940.298828        0.238219\n",
      "FIRE:   24 15:33:51    -3940.303223        0.196337\n",
      "FIRE:   25 15:33:54    -3940.305664        0.189552\n",
      "FIRE:   26 15:33:57    -3940.308105        0.173173\n",
      "FIRE:   27 15:34:00    -3940.313965        0.189997\n",
      "FIRE:   28 15:34:02    -3940.317383        0.141390\n",
      "FIRE:   29 15:34:04    -3940.315918        0.137030\n",
      "FIRE:   30 15:34:07    -3940.319824        0.174289\n",
      "FIRE:   31 15:34:10    -3940.321289        0.122218\n",
      "FIRE:   32 15:34:13    -3940.323730        0.083043\n",
      "FIRE:   33 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   34 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   35 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   36 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   37 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   38 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   39 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   40 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   41 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   42 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   43 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   44 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   45 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   46 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   47 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   48 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   49 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   50 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   51 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   52 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   53 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   54 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   55 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   56 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   57 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   58 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   59 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   60 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   61 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   62 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   63 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   64 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   65 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   66 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   67 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   68 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   69 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   70 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   71 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   72 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   73 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   74 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   75 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   76 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   77 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   78 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   79 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   80 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   81 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   82 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   83 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   84 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   85 15:34:16    -3940.325684        0.026206\n",
      "FIRE:   86 15:34:17    -3940.325684        0.026206\n",
      "FIRE:   87 15:34:17    -3940.325684        0.026206\n",
      "FIRE:   88 15:34:17    -3940.325684        0.026206\n",
      "FIRE:   89 15:34:17    -3940.325684        0.026206\n",
      "FIRE:   90 15:34:17    -3940.325684        0.026206\n",
      "FIRE:   91 15:34:17    -3940.325684        0.026206\n",
      "FIRE:   92 15:34:17    -3940.325684        0.026206\n",
      "FIRE:   93 15:34:17    -3940.325684        0.026206\n",
      "FIRE:   94 15:34:17    -3940.325684        0.026206\n",
      "FIRE:   95 15:34:17    -3940.325684        0.026206\n",
      "FIRE:   96 15:34:17    -3940.325684        0.026206\n",
      "FIRE:   97 15:34:17    -3940.325684        0.026206\n",
      "FIRE:   98 15:34:17    -3940.325684        0.026206\n",
      "FIRE:   99 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  100 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  101 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  102 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  103 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  104 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  105 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  106 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  107 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  108 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  109 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  110 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  111 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  112 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  113 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  114 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  115 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  116 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  117 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  118 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  119 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  120 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  121 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  122 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  123 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  124 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  125 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  126 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  127 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  128 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  129 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  130 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  131 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  132 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  133 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  134 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  135 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  136 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  137 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  138 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  139 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  140 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  141 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  142 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  143 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  144 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  145 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  146 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  147 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  148 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  149 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  150 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  151 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  152 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  153 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  154 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  155 15:34:17    -3940.325684        0.026206\n",
      "FIRE:  156 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  157 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  158 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  159 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  160 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  161 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  162 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  163 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  164 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  165 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  166 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  167 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  168 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  169 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  170 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  171 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  172 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  173 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  174 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  175 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  176 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  177 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  178 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  179 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  180 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  181 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  182 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  183 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  184 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  185 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  186 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  187 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  188 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  189 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  190 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  191 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  192 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  193 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  194 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  195 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  196 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  197 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  198 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  199 15:34:18    -3940.325684        0.026206\n",
      "FIRE:  200 15:34:18    -3940.325684        0.026206\n"
     ]
    }
   ],
   "source": [
    "# run the vacancy diffusion\n",
    "results = neb.run_single(vacancy_index=vac_idx,\n",
    "               target_index=tar_idx,\n",
    "               num_images=5,\n",
    "               neb_method=\"dyneb\",\n",
    "               climb=True,\n",
    "               save_xyz=True,\n",
    "               output_dir=Path(\".\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vacancy_element': 'V',\n",
       " 'target_element': 'W',\n",
       " 'vacancy_index': '102',\n",
       " 'target_index': '101',\n",
       " 'barrier': 0.819580078125,\n",
       " 'energies': [-3941.60546875,\n",
       "  -3941.510986328125,\n",
       "  -3941.022705078125,\n",
       "  -3940.785888671875,\n",
       "  -3940.919677734375,\n",
       "  -3941.236572265625,\n",
       "  -3941.342529296875],\n",
       " 'converged': False,\n",
       " 'n_steps': 200,\n",
       " 'success': True,\n",
       " 'error': None,\n",
       " 'is_nearest_neighbor': True}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is the output of the single vacancy diffusion calculation\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we plot the relative energies of the vacancy diffusion calculation \n",
    "scaled_energies = [results['energies'][i] - results['energies'][0] for i in range(len(results['energies']))]\n",
    "\n",
    "# plot the scaled energies\n",
    "plt.plot(scaled_energies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Image Index')\n",
    "plt.ylabel('Energy Barrier (eV)')\n",
    "plt.title(f'Migration Energy Barrier of Vac Swap with {results[\"target_element\"]}. Barrier = {round(results[\"barrier\"], 3)} eV')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running multiple vacancy diffusion calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most vacancy diffusion calculations, you will want to run multiple vacancy diffusion calculations to infer the compositions potential energy landscape. Caution, this is best done on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_neb = VacancyDiffusion(atoms=relaxed_atoms, model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vacancy indices: [102 348 270]\n"
     ]
    }
   ],
   "source": [
    "# pick 3 vacancy indices randomly from the len of atoms\n",
    "multi_vac_indices = np.random.randint(0, len(atoms), 3)\n",
    "\n",
    "print(f\"Vacancy indices: {multi_vac_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 42 NEB calculations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:26:30    -3941.007324        0.692459\n",
      "FIRE:    1 16:26:31    -3941.028320        0.678389\n",
      "FIRE:    2 16:26:31    -3941.067627        0.651009\n",
      "FIRE:    3 16:26:32    -3941.120361        0.611337\n",
      "FIRE:    4 16:26:32    -3941.175049        0.559792\n",
      "FIRE:    5 16:26:33    -3941.228271        0.495358\n",
      "FIRE:    6 16:26:34    -3941.273682        0.415651\n",
      "FIRE:    7 16:26:34    -3941.307861        0.318694\n",
      "FIRE:    8 16:26:35    -3941.328369        0.194057\n",
      "FIRE:    9 16:26:36    -3941.337402        0.197563\n",
      "FIRE:   10 16:26:36    -3941.337891        0.194195\n",
      "FIRE:   11 16:26:37    -3941.340820        0.187571\n",
      "FIRE:   12 16:26:38    -3941.342285        0.177868\n",
      "FIRE:   13 16:26:38    -3941.346924        0.165367\n",
      "FIRE:   14 16:26:39    -3941.351318        0.150440\n",
      "FIRE:   15 16:26:40    -3941.353271        0.133518\n",
      "FIRE:   16 16:26:41    -3941.359375        0.115132\n",
      "FIRE:   17 16:26:41    -3941.363037        0.107068\n",
      "FIRE:   18 16:26:42    -3941.366211        0.118154\n",
      "FIRE:   19 16:26:42    -3941.368652        0.127921\n",
      "FIRE:   20 16:26:43    -3941.370361        0.134562\n",
      "FIRE:   21 16:26:44    -3941.373535        0.136186\n",
      "FIRE:   22 16:26:44    -3941.376953        0.130991\n",
      "FIRE:   23 16:26:45    -3941.379639        0.117262\n",
      "FIRE:   24 16:26:46    -3941.382080        0.093717\n",
      "FIRE:   25 16:26:47    -3941.384766        0.060334\n",
      "FIRE:   26 16:26:47    -3941.389404        0.032276\n",
      "FIRE:   27 16:26:48    -3941.388672        0.030074\n",
      "FIRE:   28 16:26:49    -3941.388916        0.028146\n",
      "FIRE:   29 16:26:49    -3941.389160        0.024664\n",
      "FIRE:   30 16:26:50    -3941.389404        0.021829\n",
      "FIRE:   31 16:26:51    -3941.390137        0.018290\n",
      "FIRE:   32 16:26:51    -3941.390381        0.015166\n",
      "FIRE:   33 16:26:52    -3941.390625        0.014932\n",
      "FIRE:   34 16:26:53    -3941.390869        0.014343\n",
      "FIRE:   35 16:26:53    -3941.390381        0.013138\n",
      "FIRE:   36 16:26:54    -3941.392578        0.016066\n",
      "FIRE:   37 16:26:55    -3941.392090        0.016565\n",
      "FIRE:   38 16:26:55    -3941.393555        0.014327\n",
      "FIRE:   39 16:26:56    -3941.393799        0.010541\n",
      "FIRE:   40 16:26:57    -3941.394531        0.010776\n",
      "FIRE:   41 16:26:57    -3941.394531        0.010711\n",
      "FIRE:   42 16:26:58    -3941.395508        0.012117\n",
      "FIRE:   43 16:26:58    -3941.396240        0.013518\n",
      "FIRE:   44 16:26:59    -3941.396729        0.020387\n",
      "FIRE:   45 16:26:59    -3941.397217        0.023888\n",
      "FIRE:   46 16:27:00    -3941.397705        0.020446\n",
      "FIRE:   47 16:27:01    -3941.398682        0.009571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:27:01    -3940.756104        1.056289\n",
      "FIRE:    1 16:27:02    -3940.807129        1.005210\n",
      "FIRE:    2 16:27:03    -3940.898193        0.914375\n",
      "FIRE:    3 16:27:03    -3941.013916        0.801175\n",
      "FIRE:    4 16:27:04    -3941.133545        0.680005\n",
      "FIRE:    5 16:27:05    -3941.244629        0.555268\n",
      "FIRE:    6 16:27:06    -3941.339600        0.421921\n",
      "FIRE:    7 16:27:06    -3941.412842        0.399527\n",
      "FIRE:    8 16:27:07    -3941.468018        0.386945\n",
      "FIRE:    9 16:27:07    -3941.486816        0.350118\n",
      "FIRE:   10 16:27:08    -3941.489990        0.338630\n",
      "FIRE:   11 16:27:09    -3941.496826        0.316164\n",
      "FIRE:   12 16:27:09    -3941.503418        0.283677\n",
      "FIRE:   13 16:27:10    -3941.512939        0.242585\n",
      "FIRE:   14 16:27:10    -3941.522949        0.194633\n",
      "FIRE:   15 16:27:11    -3941.531250        0.147850\n",
      "FIRE:   16 16:27:12    -3941.538086        0.110649\n",
      "FIRE:   17 16:27:12    -3941.543213        0.085745\n",
      "FIRE:   18 16:27:13    -3941.545898        0.083961\n",
      "FIRE:   19 16:27:14    -3941.547607        0.113417\n",
      "FIRE:   20 16:27:14    -3941.548096        0.112861\n",
      "FIRE:   21 16:27:15    -3941.547607        0.111747\n",
      "FIRE:   22 16:27:15    -3941.547363        0.110089\n",
      "FIRE:   23 16:27:16    -3941.548340        0.107907\n",
      "FIRE:   24 16:27:16    -3941.548096        0.105213\n",
      "FIRE:   25 16:27:17    -3941.549805        0.102033\n",
      "FIRE:   26 16:27:17    -3941.550049        0.098401\n",
      "FIRE:   27 16:27:18    -3941.551270        0.093897\n",
      "FIRE:   28 16:27:18    -3941.552979        0.088405\n",
      "FIRE:   29 16:27:19    -3941.554932        0.081830\n",
      "FIRE:   30 16:27:19    -3941.554688        0.074147\n",
      "FIRE:   31 16:27:20    -3941.554688        0.065485\n",
      "FIRE:   32 16:27:20    -3941.557373        0.056154\n",
      "FIRE:   33 16:27:21    -3941.557861        0.046646\n",
      "FIRE:   34 16:27:22    -3941.560059        0.043209\n",
      "FIRE:   35 16:27:22    -3941.560547        0.042262\n",
      "FIRE:   36 16:27:23    -3941.560547        0.033581\n",
      "FIRE:   37 16:27:23    -3941.562988        0.023550\n",
      "FIRE:   38 16:27:24    -3941.565430        0.016725\n",
      "FIRE:   39 16:27:24    -3941.566162        0.027750\n",
      "FIRE:   40 16:27:25    -3941.566895        0.033695\n",
      "FIRE:   41 16:27:25    -3941.565918        0.025007\n",
      "FIRE:   42 16:27:26    -3941.568604        0.019187\n",
      "FIRE:   43 16:27:26    -3941.569092        0.029476\n",
      "FIRE:   44 16:27:27    -3941.569336        0.040207\n",
      "FIRE:   45 16:27:28    -3941.570312        0.041455\n",
      "FIRE:   46 16:27:28    -3941.571533        0.031361\n",
      "FIRE:   47 16:27:29    -3941.572998        0.020021\n",
      "FIRE:   48 16:27:29    -3941.571533        0.015790\n",
      "FIRE:   49 16:27:30    -3941.573975        0.014953\n",
      "FIRE:   50 16:27:30    -3941.574219        0.012970\n",
      "FIRE:   51 16:27:31    -3941.573242        0.009947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:27:35    -3941.149414        0.410028\n",
      "FIRE:    1 16:27:38    -3941.158936        0.368901\n",
      "FIRE:    2 16:27:42    -3941.174805        0.292293\n",
      "FIRE:    3 16:27:44    -3941.191895        0.217470\n",
      "FIRE:    4 16:27:47    -3941.205322        0.188714\n",
      "FIRE:    5 16:27:50    -3941.213379        0.153375\n",
      "FIRE:    6 16:27:53    -3941.216553        0.123712\n",
      "FIRE:    7 16:27:56    -3941.215332        0.168635\n",
      "FIRE:    8 16:27:59    -3941.217773        0.177078\n",
      "FIRE:    9 16:28:03    -3941.220703        0.158979\n",
      "FIRE:   10 16:28:06    -3941.225586        0.128447\n",
      "FIRE:   11 16:28:08    -3941.227295        0.080230\n",
      "FIRE:   12 16:28:11    -3941.231445        0.081436\n",
      "FIRE:   13 16:28:15    -3941.233398        0.096948\n",
      "FIRE:   14 16:28:18    -3941.236572        0.089193\n",
      "FIRE:   15 16:28:21    -3941.240967        0.079499\n",
      "FIRE:   16 16:28:24    -3941.242920        0.073129\n",
      "FIRE:   17 16:28:26    -3941.242920        0.067989\n",
      "FIRE:   18 16:28:27    -3941.245850        0.061373\n",
      "FIRE:   19 16:28:28    -3941.248291        0.052909\n",
      "FIRE:   20 16:28:30    -3941.250488        0.053766\n",
      "FIRE:   21 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   22 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   23 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   24 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   25 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   26 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   27 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   28 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   29 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   30 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   31 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   32 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   33 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   34 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   35 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   36 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   37 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   38 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   39 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   40 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   41 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   42 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   43 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   44 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   45 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   46 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   47 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   48 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   49 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   50 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   51 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   52 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   53 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   54 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   55 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   56 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   57 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   58 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   59 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   60 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   61 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   62 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   63 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   64 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   65 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   66 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   67 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   68 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   69 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   70 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   71 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   72 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   73 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   74 16:28:31    -3941.250977        0.047051\n",
      "FIRE:   75 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   76 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   77 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   78 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   79 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   80 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   81 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   82 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   83 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   84 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   85 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   86 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   87 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   88 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   89 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   90 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   91 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   92 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   93 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   94 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   95 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   96 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   97 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   98 16:28:32    -3941.250977        0.047051\n",
      "FIRE:   99 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  100 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  101 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  102 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  103 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  104 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  105 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  106 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  107 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  108 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  109 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  110 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  111 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  112 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  113 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  114 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  115 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  116 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  117 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  118 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  119 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  120 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  121 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  122 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  123 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  124 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  125 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  126 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  127 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  128 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  129 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  130 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  131 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  132 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  133 16:28:32    -3941.250977        0.047051\n",
      "FIRE:  134 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  135 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  136 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  137 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  138 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  139 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  140 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  141 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  142 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  143 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  144 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  145 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  146 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  147 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  148 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  149 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  150 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  151 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  152 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  153 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  154 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  155 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  156 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  157 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  158 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  159 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  160 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  161 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  162 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  163 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  164 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  165 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  166 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  167 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  168 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  169 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  170 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  171 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  172 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  173 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  174 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  175 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  176 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  177 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  178 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  179 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  180 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  181 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  182 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  183 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  184 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  185 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  186 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  187 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  188 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  189 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  190 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  191 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  192 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  193 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  194 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  195 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  196 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  197 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  198 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  199 16:28:33    -3941.250977        0.047051\n",
      "FIRE:  200 16:28:33    -3941.250977        0.047051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:28:34    -3941.007324        0.692459\n",
      "FIRE:    1 16:28:35    -3941.028320        0.678390\n",
      "FIRE:    2 16:28:35    -3941.067627        0.651008\n",
      "FIRE:    3 16:28:36    -3941.120361        0.611337\n",
      "FIRE:    4 16:28:36    -3941.175293        0.559793\n",
      "FIRE:    5 16:28:37    -3941.228271        0.495360\n",
      "FIRE:    6 16:28:38    -3941.273682        0.415650\n",
      "FIRE:    7 16:28:38    -3941.307861        0.318693\n",
      "FIRE:    8 16:28:39    -3941.328369        0.194059\n",
      "FIRE:    9 16:28:40    -3941.337402        0.197561\n",
      "FIRE:   10 16:28:40    -3941.337646        0.194201\n",
      "FIRE:   11 16:28:41    -3941.340576        0.187569\n",
      "FIRE:   12 16:28:42    -3941.342285        0.177866\n",
      "FIRE:   13 16:28:42    -3941.346924        0.165367\n",
      "FIRE:   14 16:28:43    -3941.351318        0.150440\n",
      "FIRE:   15 16:28:43    -3941.353516        0.133516\n",
      "FIRE:   16 16:28:44    -3941.359131        0.115132\n",
      "FIRE:   17 16:28:44    -3941.362793        0.107067\n",
      "FIRE:   18 16:28:45    -3941.366455        0.118154\n",
      "FIRE:   19 16:28:45    -3941.368652        0.127923\n",
      "FIRE:   20 16:28:46    -3941.370361        0.134559\n",
      "FIRE:   21 16:28:46    -3941.373779        0.136186\n",
      "FIRE:   22 16:28:47    -3941.376953        0.130989\n",
      "FIRE:   23 16:28:47    -3941.379883        0.117262\n",
      "FIRE:   24 16:28:48    -3941.382080        0.093720\n",
      "FIRE:   25 16:28:49    -3941.384766        0.060332\n",
      "FIRE:   26 16:28:49    -3941.389404        0.032275\n",
      "FIRE:   27 16:28:50    -3941.388428        0.030072\n",
      "FIRE:   28 16:28:50    -3941.389160        0.028143\n",
      "FIRE:   29 16:28:51    -3941.389404        0.024668\n",
      "FIRE:   30 16:28:51    -3941.389404        0.021829\n",
      "FIRE:   31 16:28:52    -3941.389648        0.018288\n",
      "FIRE:   32 16:28:52    -3941.390381        0.015163\n",
      "FIRE:   33 16:28:53    -3941.390625        0.014934\n",
      "FIRE:   34 16:28:53    -3941.390869        0.014341\n",
      "FIRE:   35 16:28:54    -3941.390381        0.013137\n",
      "FIRE:   36 16:28:54    -3941.392578        0.016069\n",
      "FIRE:   37 16:28:55    -3941.391846        0.016566\n",
      "FIRE:   38 16:28:55    -3941.393311        0.014313\n",
      "FIRE:   39 16:28:56    -3941.393555        0.010547\n",
      "FIRE:   40 16:28:56    -3941.394531        0.010783\n",
      "FIRE:   41 16:28:57    -3941.394531        0.010708\n",
      "FIRE:   42 16:28:58    -3941.395508        0.012111\n",
      "FIRE:   43 16:28:58    -3941.396240        0.013522\n",
      "FIRE:   44 16:28:59    -3941.396729        0.020385\n",
      "FIRE:   45 16:28:59    -3941.397217        0.023890\n",
      "FIRE:   46 16:29:00    -3941.397705        0.020445\n",
      "FIRE:   47 16:29:00    -3941.399170        0.009574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:29:01    -3940.736816        0.677781\n",
      "FIRE:    1 16:29:01    -3940.752686        0.631459\n",
      "FIRE:    2 16:29:02    -3940.783691        0.543985\n",
      "FIRE:    3 16:29:02    -3940.819336        0.424662\n",
      "FIRE:    4 16:29:03    -3940.856445        0.285475\n",
      "FIRE:    5 16:29:03    -3940.890137        0.207099\n",
      "FIRE:    6 16:29:04    -3940.910889        0.192664\n",
      "FIRE:    7 16:29:05    -3940.924072        0.185963\n",
      "FIRE:    8 16:29:05    -3940.931885        0.179131\n",
      "FIRE:    9 16:29:06    -3940.933594        0.215480\n",
      "FIRE:   10 16:29:06    -3940.935791        0.209364\n",
      "FIRE:   11 16:29:07    -3940.938232        0.197317\n",
      "FIRE:   12 16:29:07    -3940.941895        0.179720\n",
      "FIRE:   13 16:29:08    -3940.945312        0.157132\n",
      "FIRE:   14 16:29:08    -3940.948486        0.130283\n",
      "FIRE:   15 16:29:09    -3940.951172        0.100101\n",
      "FIRE:   16 16:29:10    -3940.953613        0.088927\n",
      "FIRE:   17 16:29:10    -3940.956543        0.077274\n",
      "FIRE:   18 16:29:11    -3940.959229        0.064332\n",
      "FIRE:   19 16:29:11    -3940.960938        0.078182\n",
      "FIRE:   20 16:29:12    -3940.959473        0.086855\n",
      "FIRE:   21 16:29:12    -3940.959229        0.085724\n",
      "FIRE:   22 16:29:13    -3940.959473        0.083476\n",
      "FIRE:   23 16:29:14    -3940.958740        0.080162\n",
      "FIRE:   24 16:29:14    -3940.960449        0.075842\n",
      "FIRE:   25 16:29:15    -3940.960693        0.070597\n",
      "FIRE:   26 16:29:16    -3940.960938        0.064545\n",
      "FIRE:   27 16:29:16    -3940.961426        0.057811\n",
      "FIRE:   28 16:29:17    -3940.961426        0.049768\n",
      "FIRE:   29 16:29:17    -3940.962158        0.040433\n",
      "FIRE:   30 16:29:18    -3940.961670        0.038695\n",
      "FIRE:   31 16:29:18    -3940.962402        0.038556\n",
      "FIRE:   32 16:29:19    -3940.963867        0.037942\n",
      "FIRE:   33 16:29:19    -3940.966064        0.036551\n",
      "FIRE:   34 16:29:20    -3940.964355        0.034065\n",
      "FIRE:   35 16:29:20    -3940.965820        0.030201\n",
      "FIRE:   36 16:29:21    -3940.963623        0.024887\n",
      "FIRE:   37 16:29:21    -3940.965576        0.018350\n",
      "FIRE:   38 16:29:22    -3940.965576        0.012905\n",
      "FIRE:   39 16:29:22    -3940.965820        0.015291\n",
      "FIRE:   40 16:29:23    -3940.966553        0.018408\n",
      "FIRE:   41 16:29:24    -3940.966553        0.018771\n",
      "FIRE:   42 16:29:24    -3940.966797        0.018373\n",
      "FIRE:   43 16:29:25    -3940.966309        0.017601\n",
      "FIRE:   44 16:29:25    -3940.966797        0.016508\n",
      "FIRE:   45 16:29:26    -3940.966553        0.015159\n",
      "FIRE:   46 16:29:26    -3940.966309        0.013634\n",
      "FIRE:   47 16:29:27    -3940.967529        0.012032\n",
      "FIRE:   48 16:29:27    -3940.965332        0.010442\n",
      "FIRE:   49 16:29:28    -3940.965332        0.010024\n",
      "FIRE:   50 16:29:28    -3940.967041        0.009924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:29:33    -3939.965576        1.565346\n",
      "FIRE:    1 16:29:36    -3940.045898        1.386017\n",
      "FIRE:    2 16:29:39    -3940.166504        1.107740\n",
      "FIRE:    3 16:29:43    -3940.274902        0.764049\n",
      "FIRE:    4 16:29:46    -3940.339355        0.442877\n",
      "FIRE:    5 16:29:49    -3940.362061        0.605689\n",
      "FIRE:    6 16:29:52    -3940.363525        0.673025\n",
      "FIRE:    7 16:29:55    -3940.361084        0.559787\n",
      "FIRE:    8 16:29:58    -3940.365967        0.540350\n",
      "FIRE:    9 16:30:01    -3940.373047        0.502115\n",
      "FIRE:   10 16:30:05    -3940.382568        0.446453\n",
      "FIRE:   11 16:30:07    -3940.393555        0.375434\n",
      "FIRE:   12 16:30:10    -3940.402100        0.308563\n",
      "FIRE:   13 16:30:13    -3940.413818        0.244434\n",
      "FIRE:   14 16:30:17    -3940.422363        0.173458\n",
      "FIRE:   15 16:30:21    -3940.428711        0.173906\n",
      "FIRE:   16 16:30:24    -3940.434326        0.168332\n",
      "FIRE:   17 16:30:28    -3940.437988        0.178487\n",
      "FIRE:   18 16:30:30    -3940.435547        0.221922\n",
      "FIRE:   19 16:30:33    -3940.434570        0.272862\n",
      "FIRE:   20 16:30:36    -3940.435547        0.305517\n",
      "FIRE:   21 16:30:39    -3940.439209        0.292931\n",
      "FIRE:   22 16:30:42    -3940.443604        0.227249\n",
      "FIRE:   23 16:30:44    -3940.451172        0.125980\n",
      "FIRE:   24 16:30:47    -3940.455322        0.115803\n",
      "FIRE:   25 16:30:50    -3940.458496        0.154893\n",
      "FIRE:   26 16:30:53    -3940.459961        0.210483\n",
      "FIRE:   27 16:30:55    -3940.464355        0.190404\n",
      "FIRE:   28 16:30:58    -3940.471924        0.122027\n",
      "FIRE:   29 16:31:01    -3940.476562        0.107576\n",
      "FIRE:   30 16:31:04    -3940.477539        0.128502\n",
      "FIRE:   31 16:31:07    -3940.484131        0.100719\n",
      "FIRE:   32 16:31:10    -3940.486572        0.117380\n",
      "FIRE:   33 16:31:13    -3940.491211        0.111842\n",
      "FIRE:   34 16:31:16    -3940.491943        0.088275\n",
      "FIRE:   35 16:31:18    -3940.493164        0.067099\n",
      "FIRE:   36 16:31:19    -3940.493896        0.063533\n",
      "FIRE:   37 16:31:20    -3940.494141        0.059473\n",
      "FIRE:   38 16:31:20    -3940.494629        0.059893\n",
      "FIRE:   39 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   40 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   41 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   42 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   43 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   44 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   45 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   46 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   47 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   48 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   49 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   50 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   51 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   52 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   53 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   54 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   55 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   56 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   57 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   58 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   59 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   60 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   61 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   62 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   63 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   64 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   65 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   66 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   67 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   68 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   69 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   70 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   71 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   72 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   73 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   74 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   75 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   76 16:31:21    -3940.496582        0.041647\n",
      "FIRE:   77 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   78 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   79 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   80 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   81 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   82 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   83 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   84 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   85 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   86 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   87 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   88 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   89 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   90 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   91 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   92 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   93 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   94 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   95 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   96 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   97 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   98 16:31:22    -3940.496582        0.041647\n",
      "FIRE:   99 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  100 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  101 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  102 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  103 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  104 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  105 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  106 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  107 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  108 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  109 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  110 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  111 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  112 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  113 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  114 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  115 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  116 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  117 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  118 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  119 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  120 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  121 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  122 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  123 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  124 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  125 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  126 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  127 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  128 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  129 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  130 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  131 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  132 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  133 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  134 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  135 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  136 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  137 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  138 16:31:22    -3940.496582        0.041647\n",
      "FIRE:  139 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  140 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  141 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  142 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  143 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  144 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  145 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  146 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  147 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  148 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  149 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  150 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  151 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  152 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  153 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  154 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  155 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  156 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  157 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  158 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  159 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  160 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  161 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  162 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  163 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  164 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  165 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  166 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  167 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  168 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  169 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  170 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  171 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  172 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  173 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  174 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  175 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  176 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  177 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  178 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  179 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  180 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  181 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  182 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  183 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  184 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  185 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  186 16:31:23    -3940.496582        0.041647\n",
      "FIRE:  187 16:31:24    -3940.496582        0.041647\n",
      "FIRE:  188 16:31:24    -3940.496582        0.041647\n",
      "FIRE:  189 16:31:24    -3940.496582        0.041647\n",
      "FIRE:  190 16:31:24    -3940.496582        0.041647\n",
      "FIRE:  191 16:31:24    -3940.496582        0.041647\n",
      "FIRE:  192 16:31:24    -3940.496582        0.041647\n",
      "FIRE:  193 16:31:24    -3940.496582        0.041647\n",
      "FIRE:  194 16:31:24    -3940.496582        0.041647\n",
      "FIRE:  195 16:31:24    -3940.496582        0.041647\n",
      "FIRE:  196 16:31:24    -3940.496582        0.041647\n",
      "FIRE:  197 16:31:24    -3940.496582        0.041647\n",
      "FIRE:  198 16:31:24    -3940.496582        0.041647\n",
      "FIRE:  199 16:31:24    -3940.496582        0.041647\n",
      "FIRE:  200 16:31:24    -3940.496582        0.041647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:31:25    -3941.007324        0.692459\n",
      "FIRE:    1 16:31:25    -3941.028320        0.678389\n",
      "FIRE:    2 16:31:26    -3941.067627        0.651009\n",
      "FIRE:    3 16:31:27    -3941.120361        0.611337\n",
      "FIRE:    4 16:31:28    -3941.175049        0.559793\n",
      "FIRE:    5 16:31:28    -3941.228271        0.495358\n",
      "FIRE:    6 16:31:29    -3941.273926        0.415650\n",
      "FIRE:    7 16:31:29    -3941.307861        0.318693\n",
      "FIRE:    8 16:31:30    -3941.328125        0.194059\n",
      "FIRE:    9 16:31:31    -3941.337402        0.197561\n",
      "FIRE:   10 16:31:31    -3941.337646        0.194200\n",
      "FIRE:   11 16:31:32    -3941.340576        0.187569\n",
      "FIRE:   12 16:31:33    -3941.342041        0.177867\n",
      "FIRE:   13 16:31:33    -3941.347168        0.165368\n",
      "FIRE:   14 16:31:34    -3941.351318        0.150440\n",
      "FIRE:   15 16:31:35    -3941.353516        0.133517\n",
      "FIRE:   16 16:31:35    -3941.359375        0.115133\n",
      "FIRE:   17 16:31:36    -3941.362793        0.107067\n",
      "FIRE:   18 16:31:37    -3941.366455        0.118154\n",
      "FIRE:   19 16:31:37    -3941.369141        0.127922\n",
      "FIRE:   20 16:31:38    -3941.370361        0.134558\n",
      "FIRE:   21 16:31:39    -3941.373779        0.136186\n",
      "FIRE:   22 16:31:39    -3941.376953        0.130991\n",
      "FIRE:   23 16:31:40    -3941.379639        0.117263\n",
      "FIRE:   24 16:31:40    -3941.382324        0.093717\n",
      "FIRE:   25 16:31:41    -3941.384766        0.060333\n",
      "FIRE:   26 16:31:42    -3941.389160        0.032273\n",
      "FIRE:   27 16:31:43    -3941.388428        0.030082\n",
      "FIRE:   28 16:31:43    -3941.388916        0.028150\n",
      "FIRE:   29 16:31:44    -3941.389160        0.024668\n",
      "FIRE:   30 16:31:44    -3941.389160        0.021832\n",
      "FIRE:   31 16:31:45    -3941.390137        0.018286\n",
      "FIRE:   32 16:31:46    -3941.390381        0.015162\n",
      "FIRE:   33 16:31:47    -3941.390625        0.014931\n",
      "FIRE:   34 16:31:48    -3941.391113        0.014342\n",
      "FIRE:   35 16:31:49    -3941.390381        0.013137\n",
      "FIRE:   36 16:31:49    -3941.393066        0.016070\n",
      "FIRE:   37 16:31:50    -3941.392090        0.016569\n",
      "FIRE:   38 16:31:50    -3941.393555        0.014316\n",
      "FIRE:   39 16:31:51    -3941.393799        0.010544\n",
      "FIRE:   40 16:31:51    -3941.394531        0.010776\n",
      "FIRE:   41 16:31:52    -3941.395020        0.010706\n",
      "FIRE:   42 16:31:53    -3941.395508        0.012112\n",
      "FIRE:   43 16:31:53    -3941.396240        0.013521\n",
      "FIRE:   44 16:31:54    -3941.396729        0.020386\n",
      "FIRE:   45 16:31:55    -3941.397217        0.023887\n",
      "FIRE:   46 16:31:55    -3941.397705        0.020442\n",
      "FIRE:   47 16:31:56    -3941.398926        0.009568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:31:56    -3940.799561        0.457249\n",
      "FIRE:    1 16:31:57    -3940.818848        0.451781\n",
      "FIRE:    2 16:31:58    -3940.855713        0.440367\n",
      "FIRE:    3 16:31:58    -3940.903809        0.421745\n",
      "FIRE:    4 16:31:59    -3940.955078        0.393407\n",
      "FIRE:    5 16:32:00    -3941.009521        0.351378\n",
      "FIRE:    6 16:32:00    -3941.053467        0.291023\n",
      "FIRE:    7 16:32:01    -3941.089111        0.217134\n",
      "FIRE:    8 16:32:01    -3941.114502        0.179756\n",
      "FIRE:    9 16:32:02    -3941.125488        0.140259\n",
      "FIRE:   10 16:32:03    -3941.126465        0.136192\n",
      "FIRE:   11 16:32:03    -3941.128174        0.128227\n",
      "FIRE:   12 16:32:04    -3941.131104        0.117935\n",
      "FIRE:   13 16:32:05    -3941.134033        0.108943\n",
      "FIRE:   14 16:32:05    -3941.137695        0.098094\n",
      "FIRE:   15 16:32:06    -3941.138184        0.085599\n",
      "FIRE:   16 16:32:06    -3941.142822        0.071729\n",
      "FIRE:   17 16:32:07    -3941.145508        0.057468\n",
      "FIRE:   18 16:32:08    -3941.146729        0.072542\n",
      "FIRE:   19 16:32:09    -3941.147461        0.087465\n",
      "FIRE:   20 16:32:09    -3941.148438        0.090946\n",
      "FIRE:   21 16:32:10    -3941.148438        0.089515\n",
      "FIRE:   22 16:32:11    -3941.147705        0.086686\n",
      "FIRE:   23 16:32:12    -3941.146729        0.082510\n",
      "FIRE:   24 16:32:13    -3941.146484        0.077075\n",
      "FIRE:   25 16:32:14    -3941.146973        0.070506\n",
      "FIRE:   26 16:32:15    -3941.148193        0.062935\n",
      "FIRE:   27 16:32:16    -3941.148682        0.054555\n",
      "FIRE:   28 16:32:16    -3941.148438        0.044603\n",
      "FIRE:   29 16:32:17    -3941.148682        0.033160\n",
      "FIRE:   30 16:32:18    -3941.149170        0.021732\n",
      "FIRE:   31 16:32:18    -3941.149170        0.021276\n",
      "FIRE:   32 16:32:19    -3941.150391        0.020809\n",
      "FIRE:   33 16:32:20    -3941.150391        0.019883\n",
      "FIRE:   34 16:32:20    -3941.150146        0.019922\n",
      "FIRE:   35 16:32:21    -3941.150146        0.020994\n",
      "FIRE:   36 16:32:23    -3941.149658        0.019194\n",
      "FIRE:   37 16:32:23    -3941.151123        0.014365\n",
      "FIRE:   38 16:32:24    -3941.150146        0.010487\n",
      "FIRE:   39 16:32:25    -3941.149902        0.013092\n",
      "FIRE:   40 16:32:26    -3941.151367        0.015836\n",
      "FIRE:   41 16:32:26    -3941.151123        0.015404\n",
      "FIRE:   42 16:32:27    -3941.149658        0.014566\n",
      "FIRE:   43 16:32:28    -3941.150391        0.013372\n",
      "FIRE:   44 16:32:29    -3941.151611        0.011895\n",
      "FIRE:   45 16:32:29    -3941.151123        0.010231\n",
      "FIRE:   46 16:32:30    -3941.152100        0.008494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:32:35    -3940.234863        1.374753\n",
      "FIRE:    1 16:32:39    -3940.288818        1.216211\n",
      "FIRE:    2 16:32:42    -3940.376709        0.940645\n",
      "FIRE:    3 16:32:45    -3940.464111        0.622747\n",
      "FIRE:    4 16:32:48    -3940.527832        0.363459\n",
      "FIRE:    5 16:32:51    -3940.553711        0.285885\n",
      "FIRE:    6 16:32:54    -3940.556152        0.347079\n",
      "FIRE:    7 16:32:57    -3940.551514        0.422330\n",
      "FIRE:    8 16:33:00    -3940.553711        0.411301\n",
      "FIRE:    9 16:33:03    -3940.558594        0.389519\n",
      "FIRE:   10 16:33:05    -3940.564941        0.357549\n",
      "FIRE:   11 16:33:08    -3940.571045        0.316226\n",
      "FIRE:   12 16:33:11    -3940.580566        0.268408\n",
      "FIRE:   13 16:33:15    -3940.588135        0.217487\n",
      "FIRE:   14 16:33:19    -3940.595703        0.161059\n",
      "FIRE:   15 16:33:24    -3940.601562        0.117504\n",
      "FIRE:   16 16:33:29    -3940.604980        0.107408\n",
      "FIRE:   17 16:33:32    -3940.607178        0.093557\n",
      "FIRE:   18 16:33:35    -3940.606445        0.156453\n",
      "FIRE:   19 16:33:38    -3940.606201        0.209043\n",
      "FIRE:   20 16:33:42    -3940.607666        0.233353\n",
      "FIRE:   21 16:33:45    -3940.612061        0.229748\n",
      "FIRE:   22 16:33:48    -3940.616211        0.188596\n",
      "FIRE:   23 16:33:53    -3940.622314        0.111739\n",
      "FIRE:   24 16:33:56    -3940.626709        0.060973\n",
      "FIRE:   25 16:33:59    -3940.628418        0.106052\n",
      "FIRE:   26 16:34:01    -3940.628906        0.151276\n",
      "FIRE:   27 16:34:04    -3940.634521        0.155851\n",
      "FIRE:   28 16:34:07    -3940.637695        0.114758\n",
      "FIRE:   29 16:34:10    -3940.640625        0.107306\n",
      "FIRE:   30 16:34:13    -3940.644287        0.103530\n",
      "FIRE:   31 16:34:16    -3940.646484        0.101722\n",
      "FIRE:   32 16:34:18    -3940.649414        0.092841\n",
      "FIRE:   33 16:34:20    -3940.649658        0.077858\n",
      "FIRE:   34 16:34:22    -3940.653320        0.051825\n",
      "FIRE:   35 16:34:22    -3940.653320        0.048386\n",
      "FIRE:   36 16:34:22    -3940.653320        0.048386\n",
      "FIRE:   37 16:34:22    -3940.653320        0.048386\n",
      "FIRE:   38 16:34:22    -3940.653320        0.048386\n",
      "FIRE:   39 16:34:22    -3940.653320        0.048386\n",
      "FIRE:   40 16:34:22    -3940.653320        0.048386\n",
      "FIRE:   41 16:34:22    -3940.653320        0.048386\n",
      "FIRE:   42 16:34:22    -3940.653320        0.048386\n",
      "FIRE:   43 16:34:22    -3940.653320        0.048386\n",
      "FIRE:   44 16:34:22    -3940.653320        0.048386\n",
      "FIRE:   45 16:34:22    -3940.653320        0.048386\n",
      "FIRE:   46 16:34:22    -3940.653320        0.048386\n",
      "FIRE:   47 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   48 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   49 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   50 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   51 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   52 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   53 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   54 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   55 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   56 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   57 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   58 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   59 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   60 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   61 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   62 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   63 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   64 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   65 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   66 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   67 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   68 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   69 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   70 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   71 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   72 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   73 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   74 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   75 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   76 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   77 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   78 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   79 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   80 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   81 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   82 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   83 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   84 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   85 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   86 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   87 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   88 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   89 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   90 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   91 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   92 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   93 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   94 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   95 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   96 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   97 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   98 16:34:23    -3940.653320        0.048386\n",
      "FIRE:   99 16:34:23    -3940.653320        0.048386\n",
      "FIRE:  100 16:34:23    -3940.653320        0.048386\n",
      "FIRE:  101 16:34:23    -3940.653320        0.048386\n",
      "FIRE:  102 16:34:23    -3940.653320        0.048386\n",
      "FIRE:  103 16:34:23    -3940.653320        0.048386\n",
      "FIRE:  104 16:34:23    -3940.653320        0.048386\n",
      "FIRE:  105 16:34:23    -3940.653320        0.048386\n",
      "FIRE:  106 16:34:23    -3940.653320        0.048386\n",
      "FIRE:  107 16:34:23    -3940.653320        0.048386\n",
      "FIRE:  108 16:34:23    -3940.653320        0.048386\n",
      "FIRE:  109 16:34:23    -3940.653320        0.048386\n",
      "FIRE:  110 16:34:23    -3940.653320        0.048386\n",
      "FIRE:  111 16:34:23    -3940.653320        0.048386\n",
      "FIRE:  112 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  113 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  114 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  115 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  116 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  117 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  118 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  119 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  120 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  121 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  122 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  123 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  124 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  125 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  126 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  127 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  128 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  129 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  130 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  131 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  132 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  133 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  134 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  135 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  136 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  137 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  138 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  139 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  140 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  141 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  142 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  143 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  144 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  145 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  146 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  147 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  148 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  149 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  150 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  151 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  152 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  153 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  154 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  155 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  156 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  157 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  158 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  159 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  160 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  161 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  162 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  163 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  164 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  165 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  166 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  167 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  168 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  169 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  170 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  171 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  172 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  173 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  174 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  175 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  176 16:34:24    -3940.653320        0.048386\n",
      "FIRE:  177 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  178 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  179 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  180 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  181 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  182 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  183 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  184 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  185 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  186 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  187 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  188 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  189 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  190 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  191 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  192 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  193 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  194 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  195 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  196 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  197 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  198 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  199 16:34:25    -3940.653320        0.048386\n",
      "FIRE:  200 16:34:25    -3940.653320        0.048386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:34:26    -3941.007324        0.692459\n",
      "FIRE:    1 16:34:26    -3941.028320        0.678390\n",
      "FIRE:    2 16:34:27    -3941.067627        0.651009\n",
      "FIRE:    3 16:34:27    -3941.120361        0.611338\n",
      "FIRE:    4 16:34:28    -3941.175049        0.559793\n",
      "FIRE:    5 16:34:29    -3941.228271        0.495358\n",
      "FIRE:    6 16:34:29    -3941.273438        0.415651\n",
      "FIRE:    7 16:34:30    -3941.308105        0.318692\n",
      "FIRE:    8 16:34:30    -3941.328125        0.194059\n",
      "FIRE:    9 16:34:31    -3941.337402        0.197565\n",
      "FIRE:   10 16:34:31    -3941.337646        0.194198\n",
      "FIRE:   11 16:34:32    -3941.340576        0.187570\n",
      "FIRE:   12 16:34:33    -3941.342285        0.177868\n",
      "FIRE:   13 16:34:33    -3941.346924        0.165364\n",
      "FIRE:   14 16:34:34    -3941.351318        0.150439\n",
      "FIRE:   15 16:34:34    -3941.353516        0.133516\n",
      "FIRE:   16 16:34:35    -3941.359131        0.115131\n",
      "FIRE:   17 16:34:35    -3941.363037        0.107066\n",
      "FIRE:   18 16:34:36    -3941.366455        0.118153\n",
      "FIRE:   19 16:34:37    -3941.368652        0.127923\n",
      "FIRE:   20 16:34:37    -3941.370361        0.134559\n",
      "FIRE:   21 16:34:38    -3941.373779        0.136187\n",
      "FIRE:   22 16:34:39    -3941.376953        0.130990\n",
      "FIRE:   23 16:34:39    -3941.379395        0.117263\n",
      "FIRE:   24 16:34:40    -3941.382080        0.093717\n",
      "FIRE:   25 16:34:40    -3941.384766        0.060333\n",
      "FIRE:   26 16:34:41    -3941.389160        0.032277\n",
      "FIRE:   27 16:34:42    -3941.388672        0.030081\n",
      "FIRE:   28 16:34:42    -3941.389160        0.028147\n",
      "FIRE:   29 16:34:43    -3941.389160        0.024665\n",
      "FIRE:   30 16:34:43    -3941.389404        0.021829\n",
      "FIRE:   31 16:34:44    -3941.390137        0.018283\n",
      "FIRE:   32 16:34:44    -3941.390381        0.015162\n",
      "FIRE:   33 16:34:45    -3941.390869        0.014932\n",
      "FIRE:   34 16:34:46    -3941.391357        0.014341\n",
      "FIRE:   35 16:34:46    -3941.390381        0.013137\n",
      "FIRE:   36 16:34:47    -3941.392822        0.016069\n",
      "FIRE:   37 16:34:47    -3941.392090        0.016565\n",
      "FIRE:   38 16:34:48    -3941.393555        0.014317\n",
      "FIRE:   39 16:34:48    -3941.393799        0.010549\n",
      "FIRE:   40 16:34:49    -3941.394531        0.010776\n",
      "FIRE:   41 16:34:50    -3941.394775        0.010709\n",
      "FIRE:   42 16:34:50    -3941.395752        0.012113\n",
      "FIRE:   43 16:34:51    -3941.396240        0.013521\n",
      "FIRE:   44 16:34:51    -3941.396729        0.020388\n",
      "FIRE:   45 16:34:52    -3941.396973        0.023888\n",
      "FIRE:   46 16:34:52    -3941.397217        0.020444\n",
      "FIRE:   47 16:34:53    -3941.398682        0.009572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:34:54    -3940.703857        0.588955\n",
      "FIRE:    1 16:34:54    -3940.721436        0.548132\n",
      "FIRE:    2 16:34:55    -3940.753662        0.470570\n",
      "FIRE:    3 16:34:55    -3940.797607        0.368604\n",
      "FIRE:    4 16:34:56    -3940.839600        0.356140\n",
      "FIRE:    5 16:34:56    -3940.876953        0.335182\n",
      "FIRE:    6 16:34:57    -3940.907959        0.300794\n",
      "FIRE:    7 16:34:58    -3940.928711        0.248496\n",
      "FIRE:    8 16:34:58    -3940.947021        0.195075\n",
      "FIRE:    9 16:34:59    -3940.958496        0.223366\n",
      "FIRE:   10 16:34:59    -3940.958252        0.202435\n",
      "FIRE:   11 16:35:00    -3940.959229        0.195864\n",
      "FIRE:   12 16:35:00    -3940.960938        0.182993\n",
      "FIRE:   13 16:35:01    -3940.964600        0.164359\n",
      "FIRE:   14 16:35:02    -3940.970703        0.140751\n",
      "FIRE:   15 16:35:02    -3940.974365        0.118958\n",
      "FIRE:   16 16:35:03    -3940.978027        0.098244\n",
      "FIRE:   17 16:35:04    -3940.981445        0.076674\n",
      "FIRE:   18 16:35:04    -3940.983643        0.053287\n",
      "FIRE:   19 16:35:05    -3940.984863        0.038418\n",
      "FIRE:   20 16:35:05    -3940.985352        0.048858\n",
      "FIRE:   21 16:35:06    -3940.985352        0.048144\n",
      "FIRE:   22 16:35:07    -3940.985107        0.046740\n",
      "FIRE:   23 16:35:07    -3940.985596        0.044662\n",
      "FIRE:   24 16:35:08    -3940.985352        0.041952\n",
      "FIRE:   25 16:35:08    -3940.985352        0.038665\n",
      "FIRE:   26 16:35:09    -3940.985840        0.034874\n",
      "FIRE:   27 16:35:10    -3940.985840        0.030651\n",
      "FIRE:   28 16:35:10    -3940.986572        0.025600\n",
      "FIRE:   29 16:35:11    -3940.987061        0.023714\n",
      "FIRE:   30 16:35:11    -3940.987061        0.022169\n",
      "FIRE:   31 16:35:12    -3940.986572        0.021413\n",
      "FIRE:   32 16:35:12    -3940.985596        0.021013\n",
      "FIRE:   33 16:35:13    -3940.986084        0.020214\n",
      "FIRE:   34 16:35:14    -3940.986816        0.018914\n",
      "FIRE:   35 16:35:14    -3940.986572        0.017021\n",
      "FIRE:   36 16:35:15    -3940.986572        0.015259\n",
      "FIRE:   37 16:35:16    -3940.988037        0.014140\n",
      "FIRE:   38 16:35:16    -3940.987549        0.012672\n",
      "FIRE:   39 16:35:17    -3940.988525        0.011082\n",
      "FIRE:   40 16:35:17    -3940.989014        0.012959\n",
      "FIRE:   41 16:35:18    -3940.989990        0.013665\n",
      "FIRE:   42 16:35:18    -3940.989258        0.012375\n",
      "FIRE:   43 16:35:19    -3940.989258        0.011442\n",
      "FIRE:   44 16:35:20    -3940.989258        0.011599\n",
      "FIRE:   45 16:35:20    -3940.989258        0.006832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:35:25    -3940.385010        1.322684\n",
      "FIRE:    1 16:35:28    -3940.433105        1.084884\n",
      "FIRE:    2 16:35:31    -3940.503174        0.677604\n",
      "FIRE:    3 16:35:33    -3940.566895        0.460382\n",
      "FIRE:    4 16:35:36    -3940.606201        0.291438\n",
      "FIRE:    5 16:35:40    -3940.622559        0.520411\n",
      "FIRE:    6 16:35:42    -3940.624512        0.616800\n",
      "FIRE:    7 16:35:45    -3940.624268        0.550900\n",
      "FIRE:    8 16:35:48    -3940.624512        0.355572\n",
      "FIRE:    9 16:35:51    -3940.627686        0.329675\n",
      "FIRE:   10 16:35:54    -3940.629395        0.320655\n",
      "FIRE:   11 16:35:56    -3940.633545        0.303052\n",
      "FIRE:   12 16:35:59    -3940.639160        0.277978\n",
      "FIRE:   13 16:36:02    -3940.645508        0.245704\n",
      "FIRE:   14 16:36:05    -3940.652832        0.207196\n",
      "FIRE:   15 16:36:08    -3940.660645        0.163708\n",
      "FIRE:   16 16:36:11    -3940.666748        0.116945\n",
      "FIRE:   17 16:36:15    -3940.671143        0.098696\n",
      "FIRE:   18 16:36:17    -3940.673340        0.097863\n",
      "FIRE:   19 16:36:19    -3940.674561        0.096788\n",
      "FIRE:   20 16:36:20    -3940.673340        0.113420\n",
      "FIRE:   21 16:36:23    -3940.671143        0.145628\n",
      "FIRE:   22 16:36:25    -3940.671143        0.156389\n",
      "FIRE:   23 16:36:28    -3940.672363        0.142983\n",
      "FIRE:   24 16:36:31    -3940.677002        0.106849\n",
      "FIRE:   25 16:36:34    -3940.680664        0.107029\n",
      "FIRE:   26 16:36:37    -3940.683105        0.076614\n",
      "FIRE:   27 16:36:40    -3940.688232        0.079075\n",
      "FIRE:   28 16:36:42    -3940.690430        0.076455\n",
      "FIRE:   29 16:36:45    -3940.692871        0.069617\n",
      "FIRE:   30 16:36:47    -3940.697021        0.065971\n",
      "FIRE:   31 16:36:49    -3940.702881        0.095211\n",
      "FIRE:   32 16:36:50    -3940.703857        0.078969\n",
      "FIRE:   33 16:36:52    -3940.704834        0.065308\n",
      "FIRE:   34 16:36:54    -3940.705078        0.064682\n",
      "FIRE:   35 16:36:54    -3940.705811        0.061643\n",
      "FIRE:   36 16:36:55    -3940.705566        0.057115\n",
      "FIRE:   37 16:36:55    -3940.705811        0.050469\n",
      "FIRE:   38 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   39 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   40 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   41 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   42 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   43 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   44 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   45 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   46 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   47 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   48 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   49 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   50 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   51 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   52 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   53 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   54 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   55 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   56 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   57 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   58 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   59 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   60 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   61 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   62 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   63 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   64 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   65 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   66 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   67 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   68 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   69 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   70 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   71 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   72 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   73 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   74 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   75 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   76 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   77 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   78 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   79 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   80 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   81 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   82 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   83 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   84 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   85 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   86 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   87 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   88 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   89 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   90 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   91 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   92 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   93 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   94 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   95 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   96 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   97 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   98 16:36:57    -3940.705811        0.041352\n",
      "FIRE:   99 16:36:57    -3940.705811        0.041352\n",
      "FIRE:  100 16:36:57    -3940.705811        0.041352\n",
      "FIRE:  101 16:36:57    -3940.705811        0.041352\n",
      "FIRE:  102 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  103 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  104 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  105 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  106 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  107 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  108 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  109 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  110 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  111 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  112 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  113 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  114 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  115 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  116 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  117 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  118 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  119 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  120 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  121 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  122 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  123 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  124 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  125 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  126 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  127 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  128 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  129 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  130 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  131 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  132 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  133 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  134 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  135 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  136 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  137 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  138 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  139 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  140 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  141 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  142 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  143 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  144 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  145 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  146 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  147 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  148 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  149 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  150 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  151 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  152 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  153 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  154 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  155 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  156 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  157 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  158 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  159 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  160 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  161 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  162 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  163 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  164 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  165 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  166 16:36:58    -3940.705811        0.041352\n",
      "FIRE:  167 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  168 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  169 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  170 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  171 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  172 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  173 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  174 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  175 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  176 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  177 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  178 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  179 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  180 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  181 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  182 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  183 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  184 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  185 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  186 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  187 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  188 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  189 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  190 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  191 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  192 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  193 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  194 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  195 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  196 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  197 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  198 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  199 16:36:59    -3940.705811        0.041352\n",
      "FIRE:  200 16:36:59    -3940.705811        0.041352\n",
      "Progress: 4/42 calculations completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:37:00    -3941.007324        0.692459\n",
      "FIRE:    1 16:37:00    -3941.028320        0.678388\n",
      "FIRE:    2 16:37:01    -3941.067627        0.651008\n",
      "FIRE:    3 16:37:01    -3941.120361        0.611338\n",
      "FIRE:    4 16:37:02    -3941.175293        0.559793\n",
      "FIRE:    5 16:37:03    -3941.228271        0.495361\n",
      "FIRE:    6 16:37:03    -3941.273438        0.415650\n",
      "FIRE:    7 16:37:04    -3941.307861        0.318694\n",
      "FIRE:    8 16:37:04    -3941.328125        0.194059\n",
      "FIRE:    9 16:37:05    -3941.337402        0.197562\n",
      "FIRE:   10 16:37:06    -3941.337402        0.194199\n",
      "FIRE:   11 16:37:06    -3941.340332        0.187569\n",
      "FIRE:   12 16:37:07    -3941.342285        0.177869\n",
      "FIRE:   13 16:37:07    -3941.346924        0.165368\n",
      "FIRE:   14 16:37:08    -3941.351318        0.150439\n",
      "FIRE:   15 16:37:08    -3941.353516        0.133519\n",
      "FIRE:   16 16:37:09    -3941.359131        0.115131\n",
      "FIRE:   17 16:37:10    -3941.363525        0.107067\n",
      "FIRE:   18 16:37:10    -3941.366211        0.118153\n",
      "FIRE:   19 16:37:11    -3941.368652        0.127923\n",
      "FIRE:   20 16:37:11    -3941.370361        0.134559\n",
      "FIRE:   21 16:37:12    -3941.373535        0.136185\n",
      "FIRE:   22 16:37:13    -3941.376953        0.130989\n",
      "FIRE:   23 16:37:13    -3941.379395        0.117262\n",
      "FIRE:   24 16:37:14    -3941.382080        0.093717\n",
      "FIRE:   25 16:37:14    -3941.384521        0.060334\n",
      "FIRE:   26 16:37:15    -3941.389160        0.032277\n",
      "FIRE:   27 16:37:16    -3941.388184        0.030079\n",
      "FIRE:   28 16:37:16    -3941.389160        0.028148\n",
      "FIRE:   29 16:37:17    -3941.389404        0.024669\n",
      "FIRE:   30 16:37:17    -3941.389160        0.021834\n",
      "FIRE:   31 16:37:18    -3941.389893        0.018290\n",
      "FIRE:   32 16:37:19    -3941.390381        0.015164\n",
      "FIRE:   33 16:37:19    -3941.390625        0.014933\n",
      "FIRE:   34 16:37:20    -3941.391357        0.014338\n",
      "FIRE:   35 16:37:20    -3941.390137        0.013137\n",
      "FIRE:   36 16:37:21    -3941.393066        0.016069\n",
      "FIRE:   37 16:37:21    -3941.392090        0.016571\n",
      "FIRE:   38 16:37:22    -3941.393066        0.014316\n",
      "FIRE:   39 16:37:23    -3941.393555        0.010547\n",
      "FIRE:   40 16:37:23    -3941.394531        0.010776\n",
      "FIRE:   41 16:37:24    -3941.395020        0.010710\n",
      "FIRE:   42 16:37:24    -3941.395752        0.012114\n",
      "FIRE:   43 16:37:25    -3941.396240        0.013520\n",
      "FIRE:   44 16:37:25    -3941.396973        0.020389\n",
      "FIRE:   45 16:37:26    -3941.397217        0.023887\n",
      "FIRE:   46 16:37:27    -3941.397461        0.020443\n",
      "FIRE:   47 16:37:27    -3941.398682        0.009568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:37:28    -3940.830322        0.537275\n",
      "FIRE:    1 16:37:29    -3940.852783        0.523189\n",
      "FIRE:    2 16:37:29    -3940.896973        0.494721\n",
      "FIRE:    3 16:37:30    -3940.952881        0.450891\n",
      "FIRE:    4 16:37:31    -3941.016113        0.389485\n",
      "FIRE:    5 16:37:31    -3941.076904        0.324708\n",
      "FIRE:    6 16:37:32    -3941.128662        0.244902\n",
      "FIRE:    7 16:37:32    -3941.162598        0.156246\n",
      "FIRE:    8 16:37:33    -3941.174561        0.128664\n",
      "FIRE:    9 16:37:34    -3941.175293        0.126394\n",
      "FIRE:   10 16:37:34    -3941.175049        0.121896\n",
      "FIRE:   11 16:37:35    -3941.178955        0.115251\n",
      "FIRE:   12 16:37:35    -3941.181152        0.106584\n",
      "FIRE:   13 16:37:36    -3941.181641        0.096048\n",
      "FIRE:   14 16:37:36    -3941.184570        0.083832\n",
      "FIRE:   15 16:37:37    -3941.185059        0.070158\n",
      "FIRE:   16 16:37:38    -3941.188232        0.053687\n",
      "FIRE:   17 16:37:38    -3941.188477        0.034463\n",
      "FIRE:   18 16:37:39    -3941.189941        0.028672\n",
      "FIRE:   19 16:37:40    -3941.189697        0.031485\n",
      "FIRE:   20 16:37:40    -3941.190430        0.031005\n",
      "FIRE:   21 16:37:41    -3941.190186        0.030048\n",
      "FIRE:   22 16:37:41    -3941.190918        0.028632\n",
      "FIRE:   23 16:37:42    -3941.190430        0.026794\n",
      "FIRE:   24 16:37:43    -3941.189941        0.024869\n",
      "FIRE:   25 16:37:43    -3941.189941        0.023411\n",
      "FIRE:   26 16:37:44    -3941.190430        0.022254\n",
      "FIRE:   27 16:37:44    -3941.191406        0.021394\n",
      "FIRE:   28 16:37:45    -3941.191162        0.020327\n",
      "FIRE:   29 16:37:46    -3941.191650        0.019021\n",
      "FIRE:   30 16:37:46    -3941.192139        0.017447\n",
      "FIRE:   31 16:37:47    -3941.191406        0.015597\n",
      "FIRE:   32 16:37:47    -3941.192383        0.013518\n",
      "FIRE:   33 16:37:48    -3941.192627        0.013412\n",
      "FIRE:   34 16:37:49    -3941.192383        0.014787\n",
      "FIRE:   35 16:37:49    -3941.191650        0.014643\n",
      "FIRE:   36 16:37:50    -3941.192383        0.012767\n",
      "FIRE:   37 16:37:51    -3941.191650        0.010470\n",
      "FIRE:   38 16:37:51    -3941.191895        0.008614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:37:56    -3940.608398        0.795078\n",
      "FIRE:    1 16:37:59    -3940.637451        0.703892\n",
      "FIRE:    2 16:38:02    -3940.684082        0.556643\n",
      "FIRE:    3 16:38:05    -3940.733887        0.367724\n",
      "FIRE:    4 16:38:07    -3940.767334        0.213629\n",
      "FIRE:    5 16:38:10    -3940.782227        0.189201\n",
      "FIRE:    6 16:38:13    -3940.783203        0.301753\n",
      "FIRE:    7 16:38:16    -3940.785156        0.292410\n",
      "FIRE:    8 16:38:19    -3940.787109        0.274021\n",
      "FIRE:    9 16:38:22    -3940.789307        0.247138\n",
      "FIRE:   10 16:38:24    -3940.794678        0.212682\n",
      "FIRE:   11 16:38:27    -3940.797363        0.171869\n",
      "FIRE:   12 16:38:30    -3940.801270        0.126336\n",
      "FIRE:   13 16:38:33    -3940.804688        0.086633\n",
      "FIRE:   14 16:38:36    -3940.807861        0.085555\n",
      "FIRE:   15 16:38:39    -3940.810059        0.083655\n",
      "FIRE:   16 16:38:41    -3940.811035        0.090748\n",
      "FIRE:   17 16:38:43    -3940.812988        0.128595\n",
      "FIRE:   18 16:38:45    -3940.814941        0.145581\n",
      "FIRE:   19 16:38:48    -3940.817383        0.136736\n",
      "FIRE:   20 16:38:51    -3940.819580        0.121143\n",
      "FIRE:   21 16:38:54    -3940.822510        0.094288\n",
      "FIRE:   22 16:38:57    -3940.829102        0.061043\n",
      "FIRE:   23 16:38:59    -3940.829102        0.050099\n",
      "FIRE:   24 16:38:59    -3940.829102        0.081211\n",
      "FIRE:   25 16:39:00    -3940.829102        0.090282\n",
      "FIRE:   26 16:39:02    -3940.829102        0.067889\n",
      "FIRE:   27 16:39:03    -3940.829102        0.052338\n",
      "FIRE:   28 16:39:03    -3940.829102        0.055471\n",
      "FIRE:   29 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   30 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   31 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   32 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   33 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   34 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   35 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   36 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   37 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   38 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   39 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   40 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   41 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   42 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   43 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   44 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   45 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   46 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   47 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   48 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   49 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   50 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   51 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   52 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   53 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   54 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   55 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   56 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   57 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   58 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   59 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   60 16:39:04    -3940.829102        0.047003\n",
      "FIRE:   61 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   62 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   63 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   64 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   65 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   66 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   67 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   68 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   69 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   70 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   71 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   72 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   73 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   74 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   75 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   76 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   77 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   78 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   79 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   80 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   81 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   82 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   83 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   84 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   85 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   86 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   87 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   88 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   89 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   90 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   91 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   92 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   93 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   94 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   95 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   96 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   97 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   98 16:39:05    -3940.829102        0.047003\n",
      "FIRE:   99 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  100 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  101 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  102 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  103 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  104 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  105 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  106 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  107 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  108 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  109 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  110 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  111 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  112 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  113 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  114 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  115 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  116 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  117 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  118 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  119 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  120 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  121 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  122 16:39:05    -3940.829102        0.047003\n",
      "FIRE:  123 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  124 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  125 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  126 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  127 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  128 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  129 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  130 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  131 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  132 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  133 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  134 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  135 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  136 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  137 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  138 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  139 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  140 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  141 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  142 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  143 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  144 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  145 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  146 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  147 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  148 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  149 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  150 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  151 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  152 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  153 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  154 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  155 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  156 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  157 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  158 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  159 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  160 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  161 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  162 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  163 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  164 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  165 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  166 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  167 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  168 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  169 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  170 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  171 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  172 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  173 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  174 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  175 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  176 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  177 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  178 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  179 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  180 16:39:06    -3940.829102        0.047003\n",
      "FIRE:  181 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  182 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  183 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  184 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  185 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  186 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  187 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  188 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  189 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  190 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  191 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  192 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  193 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  194 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  195 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  196 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  197 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  198 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  199 16:39:07    -3940.829102        0.047003\n",
      "FIRE:  200 16:39:07    -3940.829102        0.047003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:39:07    -3941.007324        0.692459\n",
      "FIRE:    1 16:39:08    -3941.028320        0.678390\n",
      "FIRE:    2 16:39:09    -3941.067627        0.651009\n",
      "FIRE:    3 16:39:09    -3941.120361        0.611338\n",
      "FIRE:    4 16:39:10    -3941.175293        0.559793\n",
      "FIRE:    5 16:39:10    -3941.228271        0.495359\n",
      "FIRE:    6 16:39:11    -3941.273682        0.415651\n",
      "FIRE:    7 16:39:12    -3941.307861        0.318692\n",
      "FIRE:    8 16:39:12    -3941.328369        0.194059\n",
      "FIRE:    9 16:39:13    -3941.337402        0.197564\n",
      "FIRE:   10 16:39:13    -3941.337646        0.194201\n",
      "FIRE:   11 16:39:14    -3941.340576        0.187570\n",
      "FIRE:   12 16:39:14    -3941.342285        0.177868\n",
      "FIRE:   13 16:39:15    -3941.347168        0.165366\n",
      "FIRE:   14 16:39:16    -3941.351318        0.150441\n",
      "FIRE:   15 16:39:16    -3941.353516        0.133518\n",
      "FIRE:   16 16:39:17    -3941.359375        0.115131\n",
      "FIRE:   17 16:39:17    -3941.363281        0.107067\n",
      "FIRE:   18 16:39:18    -3941.366455        0.118154\n",
      "FIRE:   19 16:39:18    -3941.368896        0.127923\n",
      "FIRE:   20 16:39:19    -3941.370361        0.134559\n",
      "FIRE:   21 16:39:20    -3941.373535        0.136186\n",
      "FIRE:   22 16:39:20    -3941.376953        0.130990\n",
      "FIRE:   23 16:39:21    -3941.379883        0.117264\n",
      "FIRE:   24 16:39:21    -3941.381836        0.093720\n",
      "FIRE:   25 16:39:22    -3941.384521        0.060333\n",
      "FIRE:   26 16:39:22    -3941.389404        0.032277\n",
      "FIRE:   27 16:39:23    -3941.388428        0.030074\n",
      "FIRE:   28 16:39:24    -3941.389160        0.028148\n",
      "FIRE:   29 16:39:24    -3941.389648        0.024670\n",
      "FIRE:   30 16:39:25    -3941.389404        0.021830\n",
      "FIRE:   31 16:39:25    -3941.390137        0.018288\n",
      "FIRE:   32 16:39:26    -3941.390381        0.015165\n",
      "FIRE:   33 16:39:26    -3941.390625        0.014934\n",
      "FIRE:   34 16:39:27    -3941.391113        0.014344\n",
      "FIRE:   35 16:39:27    -3941.390381        0.013138\n",
      "FIRE:   36 16:39:28    -3941.392822        0.016065\n",
      "FIRE:   37 16:39:28    -3941.391846        0.016570\n",
      "FIRE:   38 16:39:29    -3941.393311        0.014317\n",
      "FIRE:   39 16:39:30    -3941.394043        0.010546\n",
      "FIRE:   40 16:39:30    -3941.394531        0.010775\n",
      "FIRE:   41 16:39:31    -3941.394531        0.010710\n",
      "FIRE:   42 16:39:31    -3941.395508        0.012111\n",
      "FIRE:   43 16:39:32    -3941.396240        0.013524\n",
      "FIRE:   44 16:39:32    -3941.396484        0.020388\n",
      "FIRE:   45 16:39:33    -3941.396973        0.023886\n",
      "FIRE:   46 16:39:33    -3941.397949        0.020446\n",
      "FIRE:   47 16:39:34    -3941.398926        0.009574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:39:35    -3940.723145        0.641058\n",
      "FIRE:    1 16:39:35    -3940.749512        0.596387\n",
      "FIRE:    2 16:39:36    -3940.800537        0.511864\n",
      "FIRE:    3 16:39:36    -3940.864502        0.441535\n",
      "FIRE:    4 16:39:37    -3940.931641        0.379308\n",
      "FIRE:    5 16:39:38    -3940.990479        0.308637\n",
      "FIRE:    6 16:39:38    -3941.032471        0.252777\n",
      "FIRE:    7 16:39:39    -3941.054688        0.174516\n",
      "FIRE:    8 16:39:40    -3941.059082        0.260056\n",
      "FIRE:    9 16:39:41    -3941.060791        0.252816\n",
      "FIRE:   10 16:39:41    -3941.060547        0.238576\n",
      "FIRE:   11 16:39:42    -3941.062744        0.217806\n",
      "FIRE:   12 16:39:42    -3941.065430        0.191225\n",
      "FIRE:   13 16:39:43    -3941.070068        0.159778\n",
      "FIRE:   14 16:39:43    -3941.073975        0.124649\n",
      "FIRE:   15 16:39:44    -3941.078613        0.087383\n",
      "FIRE:   16 16:39:44    -3941.078857        0.054478\n",
      "FIRE:   17 16:39:45    -3941.080811        0.046707\n",
      "FIRE:   18 16:39:46    -3941.081787        0.046006\n",
      "FIRE:   19 16:39:46    -3941.082031        0.045229\n",
      "FIRE:   20 16:39:47    -3941.082031        0.043692\n",
      "FIRE:   21 16:39:47    -3941.082520        0.041422\n",
      "FIRE:   22 16:39:48    -3941.081787        0.038467\n",
      "FIRE:   23 16:39:48    -3941.081055        0.034877\n",
      "FIRE:   24 16:39:49    -3941.080566        0.030739\n",
      "FIRE:   25 16:39:49    -3941.082031        0.026464\n",
      "FIRE:   26 16:39:50    -3941.081543        0.023239\n",
      "FIRE:   27 16:39:51    -3941.081299        0.021307\n",
      "FIRE:   28 16:39:51    -3941.082275        0.019849\n",
      "FIRE:   29 16:39:52    -3941.082031        0.018077\n",
      "FIRE:   30 16:39:52    -3941.083008        0.015950\n",
      "FIRE:   31 16:39:53    -3941.082520        0.020174\n",
      "FIRE:   32 16:39:53    -3941.082275        0.025046\n",
      "FIRE:   33 16:39:54    -3941.082275        0.027048\n",
      "FIRE:   34 16:39:55    -3941.083252        0.025354\n",
      "FIRE:   35 16:39:55    -3941.082764        0.019583\n",
      "FIRE:   36 16:39:56    -3941.083496        0.010082\n",
      "FIRE:   37 16:39:56    -3941.083740        0.007435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:40:01    -3940.493652        1.146904\n",
      "FIRE:    1 16:40:04    -3940.538086        0.944843\n",
      "FIRE:    2 16:40:07    -3940.605957        0.593722\n",
      "FIRE:    3 16:40:10    -3940.671143        0.387454\n",
      "FIRE:    4 16:40:12    -3940.710693        0.269748\n",
      "FIRE:    5 16:40:16    -3940.725586        0.497555\n",
      "FIRE:    6 16:40:19    -3940.723877        0.566663\n",
      "FIRE:    7 16:40:22    -3940.722412        0.471335\n",
      "FIRE:    8 16:40:25    -3940.724121        0.454819\n",
      "FIRE:    9 16:40:28    -3940.729248        0.423242\n",
      "FIRE:   10 16:40:31    -3940.733643        0.377292\n",
      "FIRE:   11 16:40:34    -3940.740234        0.318693\n",
      "FIRE:   12 16:40:38    -3940.745361        0.249809\n",
      "FIRE:   13 16:40:42    -3940.752197        0.193999\n",
      "FIRE:   14 16:40:44    -3940.757080        0.196324\n",
      "FIRE:   15 16:40:47    -3940.761719        0.197273\n",
      "FIRE:   16 16:40:50    -3940.763428        0.195389\n",
      "FIRE:   17 16:40:52    -3940.762207        0.188948\n",
      "FIRE:   18 16:40:55    -3940.761963        0.201937\n",
      "FIRE:   19 16:40:57    -3940.763428        0.200833\n",
      "FIRE:   20 16:41:01    -3940.763672        0.183804\n",
      "FIRE:   21 16:41:05    -3940.766357        0.174363\n",
      "FIRE:   22 16:41:09    -3940.770508        0.134327\n",
      "FIRE:   23 16:41:12    -3940.774902        0.095251\n",
      "FIRE:   24 16:41:15    -3940.778809        0.116005\n",
      "FIRE:   25 16:41:17    -3940.781494        0.105204\n",
      "FIRE:   26 16:41:20    -3940.783936        0.116239\n",
      "FIRE:   27 16:41:22    -3940.791504        0.097190\n",
      "FIRE:   28 16:41:25    -3940.796631        0.086546\n",
      "FIRE:   29 16:41:27    -3940.802734        0.115494\n",
      "FIRE:   30 16:41:30    -3940.807373        0.139751\n",
      "FIRE:   31 16:41:32    -3940.810547        0.106088\n",
      "FIRE:   32 16:41:35    -3940.812988        0.124562\n",
      "FIRE:   33 16:41:36    -3940.816650        0.097020\n",
      "FIRE:   34 16:41:39    -3940.821289        0.111983\n",
      "FIRE:   35 16:41:41    -3940.825195        0.127840\n",
      "FIRE:   36 16:41:43    -3940.827637        0.152305\n",
      "FIRE:   37 16:41:47    -3940.829834        0.101376\n",
      "FIRE:   38 16:41:51    -3940.828613        0.131684\n",
      "FIRE:   39 16:41:54    -3940.830322        0.050959\n",
      "FIRE:   40 16:41:55    -3940.830811        0.057567\n",
      "FIRE:   41 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   42 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   43 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   44 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   45 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   46 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   47 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   48 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   49 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   50 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   51 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   52 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   53 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   54 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   55 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   56 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   57 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   58 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   59 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   60 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   61 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   62 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   63 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   64 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   65 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   66 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   67 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   68 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   69 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   70 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   71 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   72 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   73 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   74 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   75 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   76 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   77 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   78 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   79 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   80 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   81 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   82 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   83 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   84 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   85 16:41:56    -3940.830811        0.049522\n",
      "FIRE:   86 16:41:57    -3940.830811        0.049522\n",
      "FIRE:   87 16:41:57    -3940.830811        0.049522\n",
      "FIRE:   88 16:41:57    -3940.830811        0.049522\n",
      "FIRE:   89 16:41:57    -3940.830811        0.049522\n",
      "FIRE:   90 16:41:57    -3940.830811        0.049522\n",
      "FIRE:   91 16:41:57    -3940.830811        0.049522\n",
      "FIRE:   92 16:41:57    -3940.830811        0.049522\n",
      "FIRE:   93 16:41:57    -3940.830811        0.049522\n",
      "FIRE:   94 16:41:57    -3940.830811        0.049522\n",
      "FIRE:   95 16:41:57    -3940.830811        0.049522\n",
      "FIRE:   96 16:41:57    -3940.830811        0.049522\n",
      "FIRE:   97 16:41:57    -3940.830811        0.049522\n",
      "FIRE:   98 16:41:57    -3940.830811        0.049522\n",
      "FIRE:   99 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  100 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  101 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  102 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  103 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  104 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  105 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  106 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  107 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  108 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  109 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  110 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  111 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  112 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  113 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  114 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  115 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  116 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  117 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  118 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  119 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  120 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  121 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  122 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  123 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  124 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  125 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  126 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  127 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  128 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  129 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  130 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  131 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  132 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  133 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  134 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  135 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  136 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  137 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  138 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  139 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  140 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  141 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  142 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  143 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  144 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  145 16:41:57    -3940.830811        0.049522\n",
      "FIRE:  146 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  147 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  148 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  149 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  150 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  151 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  152 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  153 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  154 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  155 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  156 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  157 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  158 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  159 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  160 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  161 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  162 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  163 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  164 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  165 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  166 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  167 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  168 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  169 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  170 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  171 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  172 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  173 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  174 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  175 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  176 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  177 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  178 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  179 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  180 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  181 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  182 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  183 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  184 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  185 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  186 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  187 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  188 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  189 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  190 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  191 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  192 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  193 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  194 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  195 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  196 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  197 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  198 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  199 16:41:58    -3940.830811        0.049522\n",
      "FIRE:  200 16:41:58    -3940.830811        0.049522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:41:59    -3941.007324        0.692459\n",
      "FIRE:    1 16:42:00    -3941.028320        0.678390\n",
      "FIRE:    2 16:42:00    -3941.067627        0.651009\n",
      "FIRE:    3 16:42:01    -3941.120361        0.611337\n",
      "FIRE:    4 16:42:02    -3941.175293        0.559792\n",
      "FIRE:    5 16:42:02    -3941.228271        0.495357\n",
      "FIRE:    6 16:42:03    -3941.273438        0.415650\n",
      "FIRE:    7 16:42:03    -3941.307861        0.318695\n",
      "FIRE:    8 16:42:04    -3941.328369        0.194059\n",
      "FIRE:    9 16:42:05    -3941.337402        0.197563\n",
      "FIRE:   10 16:42:05    -3941.337402        0.194197\n",
      "FIRE:   11 16:42:06    -3941.340576        0.187569\n",
      "FIRE:   12 16:42:06    -3941.342285        0.177868\n",
      "FIRE:   13 16:42:07    -3941.347168        0.165365\n",
      "FIRE:   14 16:42:07    -3941.351318        0.150440\n",
      "FIRE:   15 16:42:08    -3941.353516        0.133518\n",
      "FIRE:   16 16:42:08    -3941.359131        0.115131\n",
      "FIRE:   17 16:42:09    -3941.363037        0.107068\n",
      "FIRE:   18 16:42:10    -3941.366455        0.118155\n",
      "FIRE:   19 16:42:11    -3941.368896        0.127924\n",
      "FIRE:   20 16:42:11    -3941.370361        0.134560\n",
      "FIRE:   21 16:42:12    -3941.373779        0.136187\n",
      "FIRE:   22 16:42:12    -3941.376709        0.130990\n",
      "FIRE:   23 16:42:13    -3941.379639        0.117263\n",
      "FIRE:   24 16:42:13    -3941.382080        0.093717\n",
      "FIRE:   25 16:42:14    -3941.384766        0.060333\n",
      "FIRE:   26 16:42:15    -3941.389404        0.032277\n",
      "FIRE:   27 16:42:15    -3941.388428        0.030082\n",
      "FIRE:   28 16:42:16    -3941.389160        0.028151\n",
      "FIRE:   29 16:42:17    -3941.389404        0.024668\n",
      "FIRE:   30 16:42:17    -3941.388916        0.021832\n",
      "FIRE:   31 16:42:18    -3941.389648        0.018285\n",
      "FIRE:   32 16:42:18    -3941.390381        0.015164\n",
      "FIRE:   33 16:42:19    -3941.390869        0.014931\n",
      "FIRE:   34 16:42:19    -3941.390869        0.014341\n",
      "FIRE:   35 16:42:20    -3941.390381        0.013136\n",
      "FIRE:   36 16:42:20    -3941.392822        0.016070\n",
      "FIRE:   37 16:42:21    -3941.392090        0.016567\n",
      "FIRE:   38 16:42:21    -3941.393311        0.014317\n",
      "FIRE:   39 16:42:22    -3941.393555        0.010547\n",
      "FIRE:   40 16:42:23    -3941.394531        0.010778\n",
      "FIRE:   41 16:42:23    -3941.395020        0.010707\n",
      "FIRE:   42 16:42:24    -3941.395508        0.012110\n",
      "FIRE:   43 16:42:24    -3941.395996        0.013519\n",
      "FIRE:   44 16:42:25    -3941.396973        0.020386\n",
      "FIRE:   45 16:42:26    -3941.397217        0.023885\n",
      "FIRE:   46 16:42:26    -3941.397705        0.020444\n",
      "FIRE:   47 16:42:27    -3941.399170        0.009573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:42:27    -3940.732666        0.703181\n",
      "FIRE:    1 16:42:28    -3940.753418        0.655965\n",
      "FIRE:    2 16:42:29    -3940.790039        0.566603\n",
      "FIRE:    3 16:42:29    -3940.838623        0.444096\n",
      "FIRE:    4 16:42:30    -3940.886963        0.299945\n",
      "FIRE:    5 16:42:30    -3940.931641        0.276185\n",
      "FIRE:    6 16:42:31    -3940.968994        0.254240\n",
      "FIRE:    7 16:42:31    -3940.991455        0.218624\n",
      "FIRE:    8 16:42:32    -3941.008789        0.223689\n",
      "FIRE:    9 16:42:33    -3941.015869        0.272065\n",
      "FIRE:   10 16:42:33    -3941.017578        0.264249\n",
      "FIRE:   11 16:42:34    -3941.020020        0.248863\n",
      "FIRE:   12 16:42:34    -3941.024170        0.226438\n",
      "FIRE:   13 16:42:35    -3941.028564        0.197712\n",
      "FIRE:   14 16:42:35    -3941.031982        0.163724\n",
      "FIRE:   15 16:42:36    -3941.037109        0.125727\n",
      "FIRE:   16 16:42:36    -3941.039551        0.085342\n",
      "FIRE:   17 16:42:37    -3941.041504        0.051355\n",
      "FIRE:   18 16:42:38    -3941.043457        0.073485\n",
      "FIRE:   19 16:42:38    -3941.045166        0.087283\n",
      "FIRE:   20 16:42:39    -3941.044678        0.086237\n",
      "FIRE:   21 16:42:40    -3941.045410        0.084162\n",
      "FIRE:   22 16:42:40    -3941.045410        0.081096\n",
      "FIRE:   23 16:42:41    -3941.045166        0.077088\n",
      "FIRE:   24 16:42:41    -3941.045166        0.072212\n",
      "FIRE:   25 16:42:42    -3941.044678        0.066556\n",
      "FIRE:   26 16:42:42    -3941.044189        0.060231\n",
      "FIRE:   27 16:42:43    -3941.044678        0.052627\n",
      "FIRE:   28 16:42:44    -3941.045166        0.043697\n",
      "FIRE:   29 16:42:44    -3941.047363        0.033577\n",
      "FIRE:   30 16:42:45    -3941.046631        0.022670\n",
      "FIRE:   31 16:42:45    -3941.046143        0.022980\n",
      "FIRE:   32 16:42:46    -3941.046875        0.023774\n",
      "FIRE:   33 16:42:46    -3941.047119        0.023749\n",
      "FIRE:   34 16:42:47    -3941.046631        0.021976\n",
      "FIRE:   35 16:42:47    -3941.047607        0.019773\n",
      "FIRE:   36 16:42:48    -3941.047607        0.013525\n",
      "FIRE:   37 16:42:48    -3941.049072        0.009472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:42:56    -3940.296387        1.283820\n",
      "FIRE:    1 16:42:59    -3940.343750        1.141547\n",
      "FIRE:    2 16:43:02    -3940.417480        0.892964\n",
      "FIRE:    3 16:43:05    -3940.493408        0.603091\n",
      "FIRE:    4 16:43:08    -3940.549072        0.359688\n",
      "FIRE:    5 16:43:11    -3940.576904        0.272627\n",
      "FIRE:    6 16:43:14    -3940.579102        0.319052\n",
      "FIRE:    7 16:43:17    -3940.576172        0.382602\n",
      "FIRE:    8 16:43:20    -3940.577393        0.372418\n",
      "FIRE:    9 16:43:22    -3940.582275        0.352309\n",
      "FIRE:   10 16:43:25    -3940.586670        0.322808\n",
      "FIRE:   11 16:43:28    -3940.593750        0.287183\n",
      "FIRE:   12 16:43:31    -3940.600830        0.245496\n",
      "FIRE:   13 16:43:34    -3940.607178        0.197498\n",
      "FIRE:   14 16:43:36    -3940.613281        0.155528\n",
      "FIRE:   15 16:43:39    -3940.618408        0.148831\n",
      "FIRE:   16 16:43:42    -3940.622314        0.138860\n",
      "FIRE:   17 16:43:45    -3940.623779        0.126577\n",
      "FIRE:   18 16:43:47    -3940.623535        0.147440\n",
      "FIRE:   19 16:43:50    -3940.624268        0.195207\n",
      "FIRE:   20 16:43:52    -3940.625000        0.217083\n",
      "FIRE:   21 16:43:55    -3940.626221        0.207356\n",
      "FIRE:   22 16:43:57    -3940.631592        0.165869\n",
      "FIRE:   23 16:44:00    -3940.638428        0.106537\n",
      "FIRE:   24 16:44:03    -3940.643066        0.112364\n",
      "FIRE:   25 16:44:06    -3940.646484        0.094122\n",
      "FIRE:   26 16:44:09    -3940.646973        0.133150\n",
      "FIRE:   27 16:44:11    -3940.651123        0.132858\n",
      "FIRE:   28 16:44:14    -3940.655029        0.102927\n",
      "FIRE:   29 16:44:18    -3940.661865        0.087151\n",
      "FIRE:   30 16:44:21    -3940.665039        0.078055\n",
      "FIRE:   31 16:44:23    -3940.671143        0.091808\n",
      "FIRE:   32 16:44:26    -3940.676758        0.085910\n",
      "FIRE:   33 16:44:28    -3940.681396        0.092399\n",
      "FIRE:   34 16:44:30    -3940.683594        0.085988\n",
      "FIRE:   35 16:44:32    -3940.685059        0.086554\n",
      "FIRE:   36 16:44:34    -3940.685059        0.093216\n",
      "FIRE:   37 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   38 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   39 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   40 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   41 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   42 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   43 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   44 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   45 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   46 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   47 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   48 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   49 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   50 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   51 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   52 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   53 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   54 16:44:34    -3940.685059        0.049908\n",
      "FIRE:   55 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   56 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   57 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   58 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   59 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   60 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   61 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   62 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   63 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   64 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   65 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   66 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   67 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   68 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   69 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   70 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   71 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   72 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   73 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   74 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   75 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   76 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   77 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   78 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   79 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   80 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   81 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   82 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   83 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   84 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   85 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   86 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   87 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   88 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   89 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   90 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   91 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   92 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   93 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   94 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   95 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   96 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   97 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   98 16:44:35    -3940.685059        0.049908\n",
      "FIRE:   99 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  100 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  101 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  102 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  103 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  104 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  105 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  106 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  107 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  108 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  109 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  110 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  111 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  112 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  113 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  114 16:44:35    -3940.685059        0.049908\n",
      "FIRE:  115 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  116 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  117 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  118 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  119 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  120 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  121 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  122 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  123 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  124 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  125 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  126 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  127 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  128 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  129 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  130 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  131 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  132 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  133 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  134 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  135 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  136 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  137 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  138 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  139 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  140 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  141 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  142 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  143 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  144 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  145 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  146 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  147 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  148 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  149 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  150 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  151 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  152 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  153 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  154 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  155 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  156 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  157 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  158 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  159 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  160 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  161 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  162 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  163 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  164 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  165 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  166 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  167 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  168 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  169 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  170 16:44:36    -3940.685059        0.049908\n",
      "FIRE:  171 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  172 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  173 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  174 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  175 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  176 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  177 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  178 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  179 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  180 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  181 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  182 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  183 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  184 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  185 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  186 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  187 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  188 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  189 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  190 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  191 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  192 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  193 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  194 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  195 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  196 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  197 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  198 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  199 16:44:37    -3940.685059        0.049908\n",
      "FIRE:  200 16:44:37    -3940.685059        0.049908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:44:38    -3941.007324        0.692459\n",
      "FIRE:    1 16:44:39    -3941.028320        0.678390\n",
      "FIRE:    2 16:44:40    -3941.067627        0.651009\n",
      "FIRE:    3 16:44:40    -3941.120361        0.611338\n",
      "FIRE:    4 16:44:41    -3941.175293        0.559791\n",
      "FIRE:    5 16:44:42    -3941.228271        0.495359\n",
      "FIRE:    6 16:44:43    -3941.273682        0.415650\n",
      "FIRE:    7 16:44:44    -3941.307861        0.318694\n",
      "FIRE:    8 16:44:44    -3941.328125        0.194059\n",
      "FIRE:    9 16:44:45    -3941.337646        0.197562\n",
      "FIRE:   10 16:44:45    -3941.337646        0.194199\n",
      "FIRE:   11 16:44:46    -3941.340576        0.187570\n",
      "FIRE:   12 16:44:47    -3941.342529        0.177867\n",
      "FIRE:   13 16:44:47    -3941.347168        0.165367\n",
      "FIRE:   14 16:44:48    -3941.351318        0.150442\n",
      "FIRE:   15 16:44:48    -3941.353027        0.133517\n",
      "FIRE:   16 16:44:49    -3941.359131        0.115130\n",
      "FIRE:   17 16:44:49    -3941.363281        0.107068\n",
      "FIRE:   18 16:44:50    -3941.366455        0.118154\n",
      "FIRE:   19 16:44:50    -3941.368652        0.127923\n",
      "FIRE:   20 16:44:51    -3941.370361        0.134559\n",
      "FIRE:   21 16:44:52    -3941.373535        0.136187\n",
      "FIRE:   22 16:44:52    -3941.376953        0.130990\n",
      "FIRE:   23 16:44:53    -3941.379639        0.117262\n",
      "FIRE:   24 16:44:53    -3941.382080        0.093719\n",
      "FIRE:   25 16:44:54    -3941.384521        0.060334\n",
      "FIRE:   26 16:44:54    -3941.389404        0.032277\n",
      "FIRE:   27 16:44:55    -3941.388672        0.030077\n",
      "FIRE:   28 16:44:55    -3941.389160        0.028151\n",
      "FIRE:   29 16:44:56    -3941.389160        0.024668\n",
      "FIRE:   30 16:44:57    -3941.389160        0.021826\n",
      "FIRE:   31 16:44:57    -3941.390137        0.018287\n",
      "FIRE:   32 16:44:58    -3941.390137        0.015163\n",
      "FIRE:   33 16:44:58    -3941.390869        0.014932\n",
      "FIRE:   34 16:44:59    -3941.390869        0.014340\n",
      "FIRE:   35 16:44:59    -3941.390381        0.013138\n",
      "FIRE:   36 16:45:00    -3941.393066        0.016071\n",
      "FIRE:   37 16:45:01    -3941.392090        0.016568\n",
      "FIRE:   38 16:45:01    -3941.393311        0.014317\n",
      "FIRE:   39 16:45:02    -3941.393799        0.010545\n",
      "FIRE:   40 16:45:02    -3941.394531        0.010777\n",
      "FIRE:   41 16:45:03    -3941.395020        0.010708\n",
      "FIRE:   42 16:45:03    -3941.395508        0.012112\n",
      "FIRE:   43 16:45:04    -3941.395752        0.013519\n",
      "FIRE:   44 16:45:05    -3941.396729        0.020386\n",
      "FIRE:   45 16:45:05    -3941.396973        0.023884\n",
      "FIRE:   46 16:45:06    -3941.397705        0.020444\n",
      "FIRE:   47 16:45:06    -3941.398926        0.009575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:45:07    -3940.758301        0.664482\n",
      "FIRE:    1 16:45:07    -3940.780029        0.619622\n",
      "FIRE:    2 16:45:08    -3940.819824        0.534645\n",
      "FIRE:    3 16:45:09    -3940.870117        0.417916\n",
      "FIRE:    4 16:45:09    -3940.923340        0.324987\n",
      "FIRE:    5 16:45:10    -3940.975098        0.304187\n",
      "FIRE:    6 16:45:10    -3941.011719        0.269211\n",
      "FIRE:    7 16:45:11    -3941.036865        0.215383\n",
      "FIRE:    8 16:45:12    -3941.043945        0.225613\n",
      "FIRE:    9 16:45:12    -3941.045654        0.219181\n",
      "FIRE:   10 16:45:13    -3941.047119        0.206509\n",
      "FIRE:   11 16:45:13    -3941.049805        0.187991\n",
      "FIRE:   12 16:45:14    -3941.052246        0.164223\n",
      "FIRE:   13 16:45:14    -3941.053955        0.135992\n",
      "FIRE:   14 16:45:15    -3941.056641        0.107892\n",
      "FIRE:   15 16:45:16    -3941.058350        0.099389\n",
      "FIRE:   16 16:45:17    -3941.060547        0.089040\n",
      "FIRE:   17 16:45:17    -3941.062012        0.076722\n",
      "FIRE:   18 16:45:18    -3941.063965        0.062702\n",
      "FIRE:   19 16:45:19    -3941.063721        0.078713\n",
      "FIRE:   20 16:45:20    -3941.063477        0.077524\n",
      "FIRE:   21 16:45:20    -3941.064209        0.075162\n",
      "FIRE:   22 16:45:21    -3941.064453        0.071674\n",
      "FIRE:   23 16:45:21    -3941.065186        0.067125\n",
      "FIRE:   24 16:45:22    -3941.065186        0.061603\n",
      "FIRE:   25 16:45:22    -3941.065430        0.055214\n",
      "FIRE:   26 16:45:23    -3941.065674        0.048093\n",
      "FIRE:   27 16:45:23    -3941.065186        0.043833\n",
      "FIRE:   28 16:45:24    -3941.065430        0.042545\n",
      "FIRE:   29 16:45:25    -3941.064941        0.040931\n",
      "FIRE:   30 16:45:25    -3941.065430        0.038918\n",
      "FIRE:   31 16:45:26    -3941.064209        0.036421\n",
      "FIRE:   32 16:45:26    -3941.065430        0.033388\n",
      "FIRE:   33 16:45:27    -3941.066162        0.029819\n",
      "FIRE:   34 16:45:27    -3941.067383        0.025685\n",
      "FIRE:   35 16:45:28    -3941.066650        0.023546\n",
      "FIRE:   36 16:45:28    -3941.067383        0.017777\n",
      "FIRE:   37 16:45:29    -3941.066895        0.015567\n",
      "FIRE:   38 16:45:30    -3941.066650        0.014113\n",
      "FIRE:   39 16:45:30    -3941.068115        0.018917\n",
      "FIRE:   40 16:45:31    -3941.066650        0.022396\n",
      "FIRE:   41 16:45:31    -3941.069092        0.017262\n",
      "FIRE:   42 16:45:32    -3941.067383        0.011902\n",
      "FIRE:   43 16:45:32    -3941.068604        0.010914\n",
      "FIRE:   44 16:45:33    -3941.068115        0.009820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:45:41    -3940.321533        1.419034\n",
      "FIRE:    1 16:45:44    -3940.377930        1.155554\n",
      "FIRE:    2 16:45:47    -3940.460693        0.709393\n",
      "FIRE:    3 16:45:49    -3940.530762        0.453145\n",
      "FIRE:    4 16:45:52    -3940.573242        0.287385\n",
      "FIRE:    5 16:45:55    -3940.588379        0.562244\n",
      "FIRE:    6 16:45:58    -3940.590088        0.650392\n",
      "FIRE:    7 16:46:00    -3940.592041        0.625880\n",
      "FIRE:    8 16:46:03    -3940.597168        0.577776\n",
      "FIRE:    9 16:46:06    -3940.602295        0.507877\n",
      "FIRE:   10 16:46:09    -3940.610596        0.418921\n",
      "FIRE:   11 16:46:12    -3940.619141        0.314593\n",
      "FIRE:   12 16:46:15    -3940.623535        0.199664\n",
      "FIRE:   13 16:46:18    -3940.630127        0.084653\n",
      "FIRE:   14 16:46:21    -3940.634033        0.080598\n",
      "FIRE:   15 16:46:24    -3940.635254        0.175699\n",
      "FIRE:   16 16:46:26    -3940.636719        0.260878\n",
      "FIRE:   17 16:46:29    -3940.636230        0.311813\n",
      "FIRE:   18 16:46:32    -3940.638672        0.309345\n",
      "FIRE:   19 16:46:35    -3940.642822        0.247570\n",
      "FIRE:   20 16:46:37    -3940.649658        0.141927\n",
      "FIRE:   21 16:46:41    -3940.653564        0.097789\n",
      "FIRE:   22 16:46:43    -3940.656982        0.160785\n",
      "FIRE:   23 16:46:45    -3940.658203        0.200354\n",
      "FIRE:   24 16:46:48    -3940.660889        0.175363\n",
      "FIRE:   25 16:46:51    -3940.664551        0.143820\n",
      "FIRE:   26 16:46:54    -3940.666992        0.133800\n",
      "FIRE:   27 16:46:57    -3940.672119        0.141042\n",
      "FIRE:   28 16:46:59    -3940.675537        0.143263\n",
      "FIRE:   29 16:47:02    -3940.676758        0.108813\n",
      "FIRE:   30 16:47:03    -3940.680176        0.119931\n",
      "FIRE:   31 16:47:05    -3940.683105        0.104133\n",
      "FIRE:   32 16:47:07    -3940.686768        0.136610\n",
      "FIRE:   33 16:47:10    -3940.690918        0.133084\n",
      "FIRE:   34 16:47:12    -3940.693604        0.123581\n",
      "FIRE:   35 16:47:15    -3940.695557        0.122636\n",
      "FIRE:   36 16:47:18    -3940.696777        0.094917\n",
      "FIRE:   37 16:47:21    -3940.696045        0.128876\n",
      "FIRE:   38 16:47:23    -3940.698242        0.018753\n",
      "Progress: 8/42 calculations completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:47:24    -3941.007324        0.692459\n",
      "FIRE:    1 16:47:25    -3941.028320        0.678389\n",
      "FIRE:    2 16:47:25    -3941.067627        0.651008\n",
      "FIRE:    3 16:47:26    -3941.120361        0.611337\n",
      "FIRE:    4 16:47:26    -3941.175049        0.559791\n",
      "FIRE:    5 16:47:27    -3941.228027        0.495359\n",
      "FIRE:    6 16:47:28    -3941.273926        0.415650\n",
      "FIRE:    7 16:47:28    -3941.307861        0.318694\n",
      "FIRE:    8 16:47:29    -3941.327881        0.194060\n",
      "FIRE:    9 16:47:29    -3941.337402        0.197562\n",
      "FIRE:   10 16:47:30    -3941.337402        0.194199\n",
      "FIRE:   11 16:47:30    -3941.340332        0.187569\n",
      "FIRE:   12 16:47:31    -3941.342285        0.177868\n",
      "FIRE:   13 16:47:31    -3941.346924        0.165364\n",
      "FIRE:   14 16:47:32    -3941.351318        0.150439\n",
      "FIRE:   15 16:47:33    -3941.353271        0.133520\n",
      "FIRE:   16 16:47:33    -3941.359131        0.115131\n",
      "FIRE:   17 16:47:34    -3941.363037        0.107067\n",
      "FIRE:   18 16:47:34    -3941.366455        0.118153\n",
      "FIRE:   19 16:47:35    -3941.368408        0.127923\n",
      "FIRE:   20 16:47:36    -3941.370361        0.134559\n",
      "FIRE:   21 16:47:36    -3941.373535        0.136186\n",
      "FIRE:   22 16:47:37    -3941.376709        0.130989\n",
      "FIRE:   23 16:47:37    -3941.379639        0.117262\n",
      "FIRE:   24 16:47:38    -3941.382080        0.093717\n",
      "FIRE:   25 16:47:39    -3941.384766        0.060334\n",
      "FIRE:   26 16:47:40    -3941.389160        0.032275\n",
      "FIRE:   27 16:47:40    -3941.388672        0.030074\n",
      "FIRE:   28 16:47:41    -3941.388916        0.028148\n",
      "FIRE:   29 16:47:41    -3941.389160        0.024668\n",
      "FIRE:   30 16:47:42    -3941.389160        0.021832\n",
      "FIRE:   31 16:47:42    -3941.389893        0.018288\n",
      "FIRE:   32 16:47:43    -3941.390381        0.015162\n",
      "FIRE:   33 16:47:43    -3941.390869        0.014931\n",
      "FIRE:   34 16:47:44    -3941.391113        0.014337\n",
      "FIRE:   35 16:47:45    -3941.390381        0.013134\n",
      "FIRE:   36 16:47:45    -3941.392822        0.016070\n",
      "FIRE:   37 16:47:46    -3941.392090        0.016569\n",
      "FIRE:   38 16:47:46    -3941.393799        0.014317\n",
      "FIRE:   39 16:47:47    -3941.393555        0.010545\n",
      "FIRE:   40 16:47:47    -3941.394531        0.010778\n",
      "FIRE:   41 16:47:48    -3941.394531        0.010712\n",
      "FIRE:   42 16:47:49    -3941.395508        0.012113\n",
      "FIRE:   43 16:47:49    -3941.396240        0.013520\n",
      "FIRE:   44 16:47:50    -3941.396729        0.020390\n",
      "FIRE:   45 16:47:50    -3941.396973        0.023887\n",
      "FIRE:   46 16:47:51    -3941.397949        0.020441\n",
      "FIRE:   47 16:47:51    -3941.398682        0.009568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:47:52    -3941.072510        0.760792\n",
      "FIRE:    1 16:47:53    -3941.103271        0.745930\n",
      "FIRE:    2 16:47:53    -3941.157471        0.716892\n",
      "FIRE:    3 16:47:54    -3941.232910        0.674238\n",
      "FIRE:    4 16:47:54    -3941.312988        0.617323\n",
      "FIRE:    5 16:47:55    -3941.391602        0.543766\n",
      "FIRE:    6 16:47:55    -3941.459717        0.450409\n",
      "FIRE:    7 16:47:56    -3941.505371        0.336243\n",
      "FIRE:    8 16:47:57    -3941.531494        0.214321\n",
      "FIRE:    9 16:47:57    -3941.537354        0.282250\n",
      "FIRE:   10 16:47:58    -3941.540283        0.276778\n",
      "FIRE:   11 16:47:58    -3941.545654        0.266033\n",
      "FIRE:   12 16:47:59    -3941.551758        0.250369\n",
      "FIRE:   13 16:47:59    -3941.558105        0.230365\n",
      "FIRE:   14 16:48:00    -3941.565918        0.206904\n",
      "FIRE:   15 16:48:01    -3941.571777        0.182817\n",
      "FIRE:   16 16:48:01    -3941.581055        0.156585\n",
      "FIRE:   17 16:48:02    -3941.587646        0.126108\n",
      "FIRE:   18 16:48:02    -3941.593750        0.139844\n",
      "FIRE:   19 16:48:03    -3941.597900        0.154011\n",
      "FIRE:   20 16:48:04    -3941.603271        0.163204\n",
      "FIRE:   21 16:48:04    -3941.607422        0.169148\n",
      "FIRE:   22 16:48:05    -3941.611328        0.177697\n",
      "FIRE:   23 16:48:05    -3941.620361        0.172991\n",
      "FIRE:   24 16:48:06    -3941.625244        0.153307\n",
      "FIRE:   25 16:48:07    -3941.633057        0.118691\n",
      "FIRE:   26 16:48:07    -3941.639160        0.073306\n",
      "FIRE:   27 16:48:08    -3941.641846        0.061661\n",
      "FIRE:   28 16:48:08    -3941.645752        0.108325\n",
      "FIRE:   29 16:48:09    -3941.646729        0.104723\n",
      "FIRE:   30 16:48:09    -3941.648193        0.097724\n",
      "FIRE:   31 16:48:10    -3941.649170        0.087736\n",
      "FIRE:   32 16:48:11    -3941.649170        0.075331\n",
      "FIRE:   33 16:48:11    -3941.651123        0.061239\n",
      "FIRE:   34 16:48:12    -3941.651367        0.046408\n",
      "FIRE:   35 16:48:12    -3941.651855        0.038566\n",
      "FIRE:   36 16:48:13    -3941.652588        0.044808\n",
      "FIRE:   37 16:48:13    -3941.652344        0.048756\n",
      "FIRE:   38 16:48:14    -3941.654541        0.049023\n",
      "FIRE:   39 16:48:15    -3941.654785        0.044571\n",
      "FIRE:   40 16:48:15    -3941.655029        0.035140\n",
      "FIRE:   41 16:48:16    -3941.655762        0.022190\n",
      "FIRE:   42 16:48:17    -3941.656738        0.032910\n",
      "FIRE:   43 16:48:18    -3941.656250        0.032196\n",
      "FIRE:   44 16:48:19    -3941.658447        0.021831\n",
      "FIRE:   45 16:48:20    -3941.657959        0.024654\n",
      "FIRE:   46 16:48:20    -3941.660400        0.022129\n",
      "FIRE:   47 16:48:21    -3941.661377        0.016649\n",
      "FIRE:   48 16:48:21    -3941.660889        0.012644\n",
      "FIRE:   49 16:48:22    -3941.661377        0.011680\n",
      "FIRE:   50 16:48:22    -3941.661865        0.009965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:48:27    -3935.673828       10.326112\n",
      "FIRE:    1 16:48:30    -3937.119873        6.112913\n",
      "FIRE:    2 16:48:33    -3938.140869        3.981565\n",
      "FIRE:    3 16:48:36    -3938.779297        2.584895\n",
      "FIRE:    4 16:48:38    -3939.134766        1.733662\n",
      "FIRE:    5 16:48:41    -3939.282959        1.134655\n",
      "FIRE:    6 16:48:44    -3939.298584        0.843220\n",
      "FIRE:    7 16:48:47    -3939.263916        1.168556\n",
      "FIRE:    8 16:48:50    -3939.233643        1.454452\n",
      "FIRE:    9 16:48:53    -3939.254883        1.395214\n",
      "FIRE:   10 16:48:55    -3939.293213        1.279484\n",
      "FIRE:   11 16:48:58    -3939.343018        1.112495\n",
      "FIRE:   12 16:49:01    -3939.395264        0.901623\n",
      "FIRE:   13 16:49:04    -3939.445068        0.656308\n",
      "FIRE:   14 16:49:07    -3939.483887        0.389092\n",
      "FIRE:   15 16:49:10    -3939.505371        0.229715\n",
      "FIRE:   16 16:49:13    -3939.511719        0.203574\n",
      "FIRE:   17 16:49:15    -3939.502441        0.415325\n",
      "FIRE:   18 16:49:18    -3939.489990        0.607113\n",
      "FIRE:   19 16:49:21    -3939.491943        0.597842\n",
      "FIRE:   20 16:49:24    -3939.493408        0.579452\n",
      "FIRE:   21 16:49:27    -3939.497314        0.552267\n",
      "FIRE:   22 16:49:30    -3939.502441        0.516732\n",
      "FIRE:   23 16:49:33    -3939.510254        0.473480\n",
      "FIRE:   24 16:49:36    -3939.514404        0.423273\n",
      "FIRE:   25 16:49:38    -3939.520508        0.367034\n",
      "FIRE:   26 16:49:42    -3939.525146        0.299167\n",
      "FIRE:   27 16:49:44    -3939.530518        0.219298\n",
      "FIRE:   28 16:49:47    -3939.532715        0.132123\n",
      "FIRE:   29 16:49:50    -3939.534180        0.124894\n",
      "FIRE:   30 16:49:52    -3939.533691        0.168430\n",
      "FIRE:   31 16:49:55    -3939.532471        0.226668\n",
      "FIRE:   32 16:49:58    -3939.530029        0.252261\n",
      "FIRE:   33 16:50:00    -3939.530762        0.251794\n",
      "FIRE:   34 16:50:03    -3939.533203        0.243082\n",
      "FIRE:   35 16:50:06    -3939.536377        0.188350\n",
      "FIRE:   36 16:50:08    -3939.538574        0.135362\n",
      "FIRE:   37 16:50:11    -3939.537109        0.113656\n",
      "FIRE:   38 16:50:12    -3939.532959        0.144711\n",
      "FIRE:   39 16:50:14    -3939.532715        0.137376\n",
      "FIRE:   40 16:50:15    -3939.533691        0.123157\n",
      "FIRE:   41 16:50:17    -3939.534912        0.102902\n",
      "FIRE:   42 16:50:18    -3939.537109        0.079020\n",
      "FIRE:   43 16:50:19    -3939.536621        0.076976\n",
      "FIRE:   44 16:50:20    -3939.536865        0.073259\n",
      "FIRE:   45 16:50:21    -3939.536865        0.067743\n",
      "FIRE:   46 16:50:21    -3939.536865        0.071778\n",
      "FIRE:   47 16:50:22    -3939.536133        0.094766\n",
      "FIRE:   48 16:50:23    -3939.536621        0.105616\n",
      "FIRE:   49 16:50:24    -3939.536133        0.100909\n",
      "FIRE:   50 16:50:24    -3939.536133        0.078215\n",
      "FIRE:   51 16:50:25    -3939.537354        0.054422\n",
      "FIRE:   52 16:50:25    -3939.537354        0.037160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:50:26    -3941.007324        0.692459\n",
      "FIRE:    1 16:50:26    -3941.028320        0.678390\n",
      "FIRE:    2 16:50:27    -3941.067627        0.651009\n",
      "FIRE:    3 16:50:28    -3941.120361        0.611337\n",
      "FIRE:    4 16:50:28    -3941.175049        0.559792\n",
      "FIRE:    5 16:50:29    -3941.228027        0.495357\n",
      "FIRE:    6 16:50:29    -3941.273438        0.415650\n",
      "FIRE:    7 16:50:30    -3941.307861        0.318694\n",
      "FIRE:    8 16:50:30    -3941.328369        0.194061\n",
      "FIRE:    9 16:50:31    -3941.337158        0.197561\n",
      "FIRE:   10 16:50:31    -3941.337646        0.194200\n",
      "FIRE:   11 16:50:32    -3941.340576        0.187568\n",
      "FIRE:   12 16:50:33    -3941.342285        0.177868\n",
      "FIRE:   13 16:50:33    -3941.346924        0.165366\n",
      "FIRE:   14 16:50:34    -3941.351318        0.150441\n",
      "FIRE:   15 16:50:34    -3941.353271        0.133518\n",
      "FIRE:   16 16:50:35    -3941.359375        0.115132\n",
      "FIRE:   17 16:50:36    -3941.363281        0.107066\n",
      "FIRE:   18 16:50:36    -3941.366699        0.118152\n",
      "FIRE:   19 16:50:37    -3941.368896        0.127922\n",
      "FIRE:   20 16:50:37    -3941.370605        0.134560\n",
      "FIRE:   21 16:50:38    -3941.373535        0.136187\n",
      "FIRE:   22 16:50:38    -3941.376953        0.130990\n",
      "FIRE:   23 16:50:39    -3941.379639        0.117264\n",
      "FIRE:   24 16:50:40    -3941.382080        0.093718\n",
      "FIRE:   25 16:50:40    -3941.384766        0.060334\n",
      "FIRE:   26 16:50:41    -3941.389160        0.032277\n",
      "FIRE:   27 16:50:42    -3941.388428        0.030077\n",
      "FIRE:   28 16:50:42    -3941.389160        0.028145\n",
      "FIRE:   29 16:50:43    -3941.389160        0.024667\n",
      "FIRE:   30 16:50:43    -3941.389160        0.021832\n",
      "FIRE:   31 16:50:44    -3941.389648        0.018289\n",
      "FIRE:   32 16:50:44    -3941.390381        0.015163\n",
      "FIRE:   33 16:50:45    -3941.390869        0.014932\n",
      "FIRE:   34 16:50:45    -3941.391113        0.014342\n",
      "FIRE:   35 16:50:46    -3941.390137        0.013137\n",
      "FIRE:   36 16:50:47    -3941.392822        0.016071\n",
      "FIRE:   37 16:50:47    -3941.392090        0.016567\n",
      "FIRE:   38 16:50:48    -3941.393555        0.014314\n",
      "FIRE:   39 16:50:48    -3941.393555        0.010545\n",
      "FIRE:   40 16:50:49    -3941.394531        0.010779\n",
      "FIRE:   41 16:50:49    -3941.394531        0.010707\n",
      "FIRE:   42 16:50:50    -3941.395752        0.012112\n",
      "FIRE:   43 16:50:51    -3941.395996        0.013521\n",
      "FIRE:   44 16:50:51    -3941.396973        0.020386\n",
      "FIRE:   45 16:50:52    -3941.397217        0.023888\n",
      "FIRE:   46 16:50:52    -3941.397461        0.020443\n",
      "FIRE:   47 16:50:53    -3941.398926        0.009574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:50:54    -3941.045898        0.653776\n",
      "FIRE:    1 16:50:54    -3941.069336        0.642195\n",
      "FIRE:    2 16:50:55    -3941.114502        0.619772\n",
      "FIRE:    3 16:50:55    -3941.170654        0.587507\n",
      "FIRE:    4 16:50:56    -3941.235352        0.545870\n",
      "FIRE:    5 16:50:56    -3941.300049        0.493936\n",
      "FIRE:    6 16:50:57    -3941.355469        0.429080\n",
      "FIRE:    7 16:50:58    -3941.397461        0.348434\n",
      "FIRE:    8 16:50:58    -3941.424805        0.241043\n",
      "FIRE:    9 16:50:59    -3941.431885        0.174932\n",
      "FIRE:   10 16:50:59    -3941.433350        0.169542\n",
      "FIRE:   11 16:51:00    -3941.437500        0.159703\n",
      "FIRE:   12 16:51:01    -3941.440430        0.152030\n",
      "FIRE:   13 16:51:01    -3941.444092        0.142190\n",
      "FIRE:   14 16:51:02    -3941.448730        0.130479\n",
      "FIRE:   15 16:51:03    -3941.453857        0.128764\n",
      "FIRE:   16 16:51:04    -3941.458984        0.133360\n",
      "FIRE:   17 16:51:05    -3941.462402        0.138084\n",
      "FIRE:   18 16:51:06    -3941.467041        0.142232\n",
      "FIRE:   19 16:51:07    -3941.467285        0.144679\n",
      "FIRE:   20 16:51:08    -3941.470459        0.144077\n",
      "FIRE:   21 16:51:08    -3941.472656        0.139089\n",
      "FIRE:   22 16:51:09    -3941.477539        0.128414\n",
      "FIRE:   23 16:51:09    -3941.479736        0.110806\n",
      "FIRE:   24 16:51:10    -3941.483643        0.085408\n",
      "FIRE:   25 16:51:11    -3941.486816        0.052533\n",
      "FIRE:   26 16:51:11    -3941.489990        0.034905\n",
      "FIRE:   27 16:51:12    -3941.489502        0.039494\n",
      "FIRE:   28 16:51:13    -3941.491455        0.036973\n",
      "FIRE:   29 16:51:13    -3941.492432        0.032100\n",
      "FIRE:   30 16:51:14    -3941.491699        0.025214\n",
      "FIRE:   31 16:51:14    -3941.493408        0.016994\n",
      "FIRE:   32 16:51:15    -3941.493408        0.013100\n",
      "FIRE:   33 16:51:15    -3941.493652        0.011837\n",
      "FIRE:   34 16:51:16    -3941.494629        0.014538\n",
      "FIRE:   35 16:51:17    -3941.494873        0.016616\n",
      "FIRE:   36 16:51:18    -3941.493652        0.020296\n",
      "FIRE:   37 16:51:18    -3941.493652        0.020963\n",
      "FIRE:   38 16:51:19    -3941.495117        0.017945\n",
      "FIRE:   39 16:51:20    -3941.495850        0.012094\n",
      "FIRE:   40 16:51:21    -3941.497070        0.008804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:51:26    -3935.969971        9.511429\n",
      "FIRE:    1 16:51:30    -3937.245605        5.899363\n",
      "FIRE:    2 16:51:34    -3938.225830        3.611962\n",
      "FIRE:    3 16:51:39    -3938.839111        2.393247\n",
      "FIRE:    4 16:51:42    -3939.177734        1.657361\n",
      "FIRE:    5 16:51:45    -3939.313721        1.128725\n",
      "FIRE:    6 16:51:48    -3939.320312        0.869786\n",
      "FIRE:    7 16:51:51    -3939.274170        1.136570\n",
      "FIRE:    8 16:51:54    -3939.234863        1.376862\n",
      "FIRE:    9 16:51:57    -3939.255127        1.320951\n",
      "FIRE:   10 16:51:59    -3939.292969        1.211786\n",
      "FIRE:   11 16:52:02    -3939.340820        1.054454\n",
      "FIRE:   12 16:52:04    -3939.391602        0.858705\n",
      "FIRE:   13 16:52:07    -3939.438965        0.642857\n",
      "FIRE:   14 16:52:09    -3939.476318        0.406899\n",
      "FIRE:   15 16:52:13    -3939.499023        0.255962\n",
      "FIRE:   16 16:52:18    -3939.507324        0.211038\n",
      "FIRE:   17 16:52:21    -3939.500488        0.365588\n",
      "FIRE:   18 16:52:23    -3939.487061        0.587048\n",
      "FIRE:   19 16:52:26    -3939.487793        0.578460\n",
      "FIRE:   20 16:52:29    -3939.490234        0.561409\n",
      "FIRE:   21 16:52:31    -3939.493652        0.536169\n",
      "FIRE:   22 16:52:34    -3939.500244        0.503120\n",
      "FIRE:   23 16:52:37    -3939.505615        0.462798\n",
      "FIRE:   24 16:52:40    -3939.510254        0.415844\n",
      "FIRE:   25 16:52:42    -3939.514404        0.363053\n",
      "FIRE:   26 16:52:45    -3939.520020        0.299049\n",
      "FIRE:   27 16:52:48    -3939.526123        0.223247\n",
      "FIRE:   28 16:52:50    -3939.528564        0.136335\n",
      "FIRE:   29 16:52:53    -3939.529053        0.091980\n",
      "FIRE:   30 16:52:56    -3939.529053        0.150242\n",
      "FIRE:   31 16:52:58    -3939.528320        0.199225\n",
      "FIRE:   32 16:53:01    -3939.529053        0.219201\n",
      "FIRE:   33 16:53:04    -3939.527588        0.246259\n",
      "FIRE:   34 16:53:06    -3939.529541        0.242492\n",
      "FIRE:   35 16:53:09    -3939.533936        0.192995\n",
      "FIRE:   36 16:53:12    -3939.535889        0.120154\n",
      "FIRE:   37 16:53:14    -3939.535889        0.114928\n",
      "FIRE:   38 16:53:16    -3939.531738        0.136745\n",
      "FIRE:   39 16:53:18    -3939.532471        0.128915\n",
      "FIRE:   40 16:53:21    -3939.533203        0.120693\n",
      "FIRE:   41 16:53:22    -3939.533447        0.118680\n",
      "FIRE:   42 16:53:23    -3939.535645        0.116129\n",
      "FIRE:   43 16:53:24    -3939.533691        0.112335\n",
      "FIRE:   44 16:53:25    -3939.534424        0.108002\n",
      "FIRE:   45 16:53:26    -3939.534180        0.102834\n",
      "FIRE:   46 16:53:27    -3939.533447        0.096027\n",
      "FIRE:   47 16:53:28    -3939.534180        0.088027\n",
      "FIRE:   48 16:53:29    -3939.532715        0.101851\n",
      "FIRE:   49 16:53:30    -3939.533203        0.101147\n",
      "FIRE:   50 16:53:30    -3939.532227        0.083165\n",
      "FIRE:   51 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   52 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   53 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   54 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   55 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   56 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   57 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   58 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   59 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   60 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   61 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   62 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   63 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   64 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   65 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   66 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   67 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   68 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   69 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   70 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   71 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   72 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   73 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   74 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   75 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   76 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   77 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   78 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   79 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   80 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   81 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   82 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   83 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   84 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   85 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   86 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   87 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   88 16:53:31    -3939.532959        0.047383\n",
      "FIRE:   89 16:53:32    -3939.532959        0.047383\n",
      "FIRE:   90 16:53:32    -3939.532959        0.047383\n",
      "FIRE:   91 16:53:32    -3939.532959        0.047383\n",
      "FIRE:   92 16:53:32    -3939.532959        0.047383\n",
      "FIRE:   93 16:53:32    -3939.532959        0.047383\n",
      "FIRE:   94 16:53:32    -3939.532959        0.047383\n",
      "FIRE:   95 16:53:32    -3939.532959        0.047383\n",
      "FIRE:   96 16:53:32    -3939.532959        0.047383\n",
      "FIRE:   97 16:53:32    -3939.532959        0.047383\n",
      "FIRE:   98 16:53:32    -3939.532959        0.047383\n",
      "FIRE:   99 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  100 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  101 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  102 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  103 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  104 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  105 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  106 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  107 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  108 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  109 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  110 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  111 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  112 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  113 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  114 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  115 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  116 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  117 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  118 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  119 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  120 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  121 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  122 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  123 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  124 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  125 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  126 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  127 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  128 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  129 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  130 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  131 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  132 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  133 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  134 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  135 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  136 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  137 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  138 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  139 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  140 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  141 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  142 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  143 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  144 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  145 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  146 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  147 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  148 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  149 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  150 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  151 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  152 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  153 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  154 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  155 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  156 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  157 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  158 16:53:32    -3939.532959        0.047383\n",
      "FIRE:  159 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  160 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  161 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  162 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  163 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  164 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  165 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  166 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  167 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  168 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  169 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  170 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  171 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  172 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  173 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  174 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  175 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  176 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  177 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  178 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  179 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  180 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  181 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  182 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  183 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  184 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  185 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  186 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  187 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  188 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  189 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  190 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  191 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  192 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  193 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  194 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  195 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  196 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  197 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  198 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  199 16:53:33    -3939.532959        0.047383\n",
      "FIRE:  200 16:53:33    -3939.532959        0.047383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:53:34    -3941.007324        0.692459\n",
      "FIRE:    1 16:53:34    -3941.028320        0.678390\n",
      "FIRE:    2 16:53:35    -3941.067627        0.651008\n",
      "FIRE:    3 16:53:35    -3941.120361        0.611337\n",
      "FIRE:    4 16:53:36    -3941.175537        0.559793\n",
      "FIRE:    5 16:53:36    -3941.228027        0.495358\n",
      "FIRE:    6 16:53:37    -3941.273682        0.415650\n",
      "FIRE:    7 16:53:37    -3941.307861        0.318693\n",
      "FIRE:    8 16:53:38    -3941.328369        0.194059\n",
      "FIRE:    9 16:53:39    -3941.337646        0.197561\n",
      "FIRE:   10 16:53:39    -3941.337646        0.194197\n",
      "FIRE:   11 16:53:40    -3941.340576        0.187570\n",
      "FIRE:   12 16:53:40    -3941.342529        0.177867\n",
      "FIRE:   13 16:53:41    -3941.347168        0.165367\n",
      "FIRE:   14 16:53:41    -3941.351318        0.150441\n",
      "FIRE:   15 16:53:42    -3941.353271        0.133519\n",
      "FIRE:   16 16:53:42    -3941.358887        0.115132\n",
      "FIRE:   17 16:53:43    -3941.363037        0.107067\n",
      "FIRE:   18 16:53:43    -3941.366211        0.118154\n",
      "FIRE:   19 16:53:44    -3941.368896        0.127924\n",
      "FIRE:   20 16:53:44    -3941.370361        0.134560\n",
      "FIRE:   21 16:53:45    -3941.373535        0.136188\n",
      "FIRE:   22 16:53:46    -3941.376953        0.130987\n",
      "FIRE:   23 16:53:46    -3941.379395        0.117264\n",
      "FIRE:   24 16:53:47    -3941.382080        0.093719\n",
      "FIRE:   25 16:53:48    -3941.384521        0.060331\n",
      "FIRE:   26 16:53:48    -3941.389404        0.032277\n",
      "FIRE:   27 16:53:49    -3941.388428        0.030073\n",
      "FIRE:   28 16:53:50    -3941.389160        0.028142\n",
      "FIRE:   29 16:53:51    -3941.389160        0.024662\n",
      "FIRE:   30 16:53:51    -3941.389160        0.021830\n",
      "FIRE:   31 16:53:52    -3941.390137        0.018287\n",
      "FIRE:   32 16:53:53    -3941.390381        0.015166\n",
      "FIRE:   33 16:53:53    -3941.390869        0.014936\n",
      "FIRE:   34 16:53:54    -3941.391113        0.014343\n",
      "FIRE:   35 16:53:54    -3941.390381        0.013137\n",
      "FIRE:   36 16:53:55    -3941.392822        0.016069\n",
      "FIRE:   37 16:53:55    -3941.392090        0.016567\n",
      "FIRE:   38 16:53:56    -3941.393311        0.014315\n",
      "FIRE:   39 16:53:56    -3941.393555        0.010540\n",
      "FIRE:   40 16:53:57    -3941.394531        0.010781\n",
      "FIRE:   41 16:53:57    -3941.394775        0.010706\n",
      "FIRE:   42 16:53:58    -3941.395752        0.012115\n",
      "FIRE:   43 16:53:58    -3941.395752        0.013522\n",
      "FIRE:   44 16:53:59    -3941.396729        0.020386\n",
      "FIRE:   45 16:53:59    -3941.396973        0.023885\n",
      "FIRE:   46 16:54:00    -3941.397461        0.020443\n",
      "FIRE:   47 16:54:00    -3941.399170        0.009575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:54:01    -3940.968994        0.708957\n",
      "FIRE:    1 16:54:02    -3940.990723        0.694364\n",
      "FIRE:    2 16:54:02    -3941.031006        0.666064\n",
      "FIRE:    3 16:54:03    -3941.083008        0.625296\n",
      "FIRE:    4 16:54:03    -3941.140869        0.572825\n",
      "FIRE:    5 16:54:04    -3941.198730        0.508129\n",
      "FIRE:    6 16:54:04    -3941.245850        0.429422\n",
      "FIRE:    7 16:54:05    -3941.285156        0.335624\n",
      "FIRE:    8 16:54:05    -3941.312744        0.218795\n",
      "FIRE:    9 16:54:06    -3941.325195        0.182241\n",
      "FIRE:   10 16:54:07    -3941.326172        0.177027\n",
      "FIRE:   11 16:54:07    -3941.327637        0.166796\n",
      "FIRE:   12 16:54:08    -3941.331055        0.154489\n",
      "FIRE:   13 16:54:08    -3941.334229        0.143807\n",
      "FIRE:   14 16:54:09    -3941.337891        0.131210\n",
      "FIRE:   15 16:54:09    -3941.342285        0.117166\n",
      "FIRE:   16 16:54:10    -3941.345703        0.103174\n",
      "FIRE:   17 16:54:10    -3941.349365        0.108250\n",
      "FIRE:   18 16:54:11    -3941.351562        0.113729\n",
      "FIRE:   19 16:54:11    -3941.354492        0.118070\n",
      "FIRE:   20 16:54:12    -3941.356201        0.119403\n",
      "FIRE:   21 16:54:12    -3941.359131        0.116065\n",
      "FIRE:   22 16:54:13    -3941.362793        0.106802\n",
      "FIRE:   23 16:54:13    -3941.365234        0.090776\n",
      "FIRE:   24 16:54:14    -3941.367920        0.067628\n",
      "FIRE:   25 16:54:14    -3941.370850        0.044255\n",
      "FIRE:   26 16:54:15    -3941.371338        0.037764\n",
      "FIRE:   27 16:54:15    -3941.371582        0.039055\n",
      "FIRE:   28 16:54:16    -3941.371338        0.036734\n",
      "FIRE:   29 16:54:16    -3941.372803        0.032235\n",
      "FIRE:   30 16:54:17    -3941.373291        0.028448\n",
      "FIRE:   31 16:54:17    -3941.373047        0.025992\n",
      "FIRE:   32 16:54:18    -3941.372559        0.023231\n",
      "FIRE:   33 16:54:18    -3941.373047        0.020378\n",
      "FIRE:   34 16:54:19    -3941.374023        0.017659\n",
      "FIRE:   35 16:54:19    -3941.375000        0.015037\n",
      "FIRE:   36 16:54:20    -3941.374512        0.018389\n",
      "FIRE:   37 16:54:20    -3941.375244        0.019023\n",
      "FIRE:   38 16:54:21    -3941.374512        0.015673\n",
      "FIRE:   39 16:54:21    -3941.374512        0.009094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:54:25    -3937.782471        4.261192\n",
      "FIRE:    1 16:54:28    -3938.334717        3.477052\n",
      "FIRE:    2 16:54:31    -3939.038574        2.299853\n",
      "FIRE:    3 16:54:34    -3939.480225        1.378905\n",
      "FIRE:    4 16:54:36    -3939.702881        0.863146\n",
      "FIRE:    5 16:54:39    -3939.763428        0.498776\n",
      "FIRE:    6 16:54:42    -3939.738281        0.914083\n",
      "FIRE:    7 16:54:45    -3939.748291        0.877506\n",
      "FIRE:    8 16:54:47    -3939.763672        0.806261\n",
      "FIRE:    9 16:54:50    -3939.786377        0.704049\n",
      "FIRE:   10 16:54:52    -3939.812012        0.576200\n",
      "FIRE:   11 16:54:55    -3939.837646        0.429419\n",
      "FIRE:   12 16:54:57    -3939.857666        0.290164\n",
      "FIRE:   13 16:55:00    -3939.874756        0.148252\n",
      "FIRE:   14 16:55:03    -3939.885010        0.154264\n",
      "FIRE:   15 16:55:05    -3939.890381        0.210613\n",
      "FIRE:   16 16:55:08    -3939.888916        0.323412\n",
      "FIRE:   17 16:55:10    -3939.888184        0.388435\n",
      "FIRE:   18 16:55:13    -3939.888184        0.406920\n",
      "FIRE:   19 16:55:16    -3939.897461        0.348912\n",
      "FIRE:   20 16:55:18    -3939.906250        0.219588\n",
      "FIRE:   21 16:55:21    -3939.913818        0.209972\n",
      "FIRE:   22 16:55:23    -3939.912109        0.177974\n",
      "FIRE:   23 16:55:26    -3939.913818        0.167882\n",
      "FIRE:   24 16:55:28    -3939.914795        0.148465\n",
      "FIRE:   25 16:55:31    -3939.915283        0.121296\n",
      "FIRE:   26 16:55:34    -3939.916992        0.088928\n",
      "FIRE:   27 16:55:36    -3939.920166        0.058069\n",
      "FIRE:   28 16:55:39    -3939.920898        0.044950\n",
      "FIRE:   29 16:55:39    -3939.920898        0.044950\n",
      "FIRE:   30 16:55:39    -3939.920898        0.044950\n",
      "FIRE:   31 16:55:39    -3939.920898        0.044950\n",
      "FIRE:   32 16:55:39    -3939.920898        0.044950\n",
      "FIRE:   33 16:55:39    -3939.920898        0.044950\n",
      "FIRE:   34 16:55:39    -3939.920898        0.044950\n",
      "FIRE:   35 16:55:39    -3939.920898        0.044950\n",
      "FIRE:   36 16:55:39    -3939.920898        0.044950\n",
      "FIRE:   37 16:55:39    -3939.920898        0.044950\n",
      "FIRE:   38 16:55:39    -3939.920898        0.044950\n",
      "FIRE:   39 16:55:39    -3939.920898        0.044950\n",
      "FIRE:   40 16:55:39    -3939.920898        0.044950\n",
      "FIRE:   41 16:55:39    -3939.920898        0.044950\n",
      "FIRE:   42 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   43 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   44 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   45 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   46 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   47 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   48 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   49 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   50 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   51 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   52 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   53 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   54 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   55 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   56 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   57 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   58 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   59 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   60 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   61 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   62 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   63 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   64 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   65 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   66 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   67 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   68 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   69 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   70 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   71 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   72 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   73 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   74 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   75 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   76 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   77 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   78 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   79 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   80 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   81 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   82 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   83 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   84 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   85 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   86 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   87 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   88 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   89 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   90 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   91 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   92 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   93 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   94 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   95 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   96 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   97 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   98 16:55:40    -3939.920898        0.044950\n",
      "FIRE:   99 16:55:40    -3939.920898        0.044950\n",
      "FIRE:  100 16:55:40    -3939.920898        0.044950\n",
      "FIRE:  101 16:55:40    -3939.920898        0.044950\n",
      "FIRE:  102 16:55:40    -3939.920898        0.044950\n",
      "FIRE:  103 16:55:40    -3939.920898        0.044950\n",
      "FIRE:  104 16:55:40    -3939.920898        0.044950\n",
      "FIRE:  105 16:55:40    -3939.920898        0.044950\n",
      "FIRE:  106 16:55:40    -3939.920898        0.044950\n",
      "FIRE:  107 16:55:40    -3939.920898        0.044950\n",
      "FIRE:  108 16:55:40    -3939.920898        0.044950\n",
      "FIRE:  109 16:55:40    -3939.920898        0.044950\n",
      "FIRE:  110 16:55:40    -3939.920898        0.044950\n",
      "FIRE:  111 16:55:40    -3939.920898        0.044950\n",
      "FIRE:  112 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  113 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  114 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  115 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  116 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  117 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  118 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  119 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  120 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  121 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  122 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  123 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  124 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  125 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  126 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  127 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  128 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  129 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  130 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  131 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  132 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  133 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  134 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  135 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  136 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  137 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  138 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  139 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  140 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  141 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  142 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  143 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  144 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  145 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  146 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  147 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  148 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  149 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  150 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  151 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  152 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  153 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  154 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  155 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  156 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  157 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  158 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  159 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  160 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  161 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  162 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  163 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  164 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  165 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  166 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  167 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  168 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  169 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  170 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  171 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  172 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  173 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  174 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  175 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  176 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  177 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  178 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  179 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  180 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  181 16:55:41    -3939.920898        0.044950\n",
      "FIRE:  182 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  183 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  184 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  185 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  186 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  187 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  188 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  189 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  190 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  191 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  192 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  193 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  194 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  195 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  196 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  197 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  198 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  199 16:55:42    -3939.920898        0.044950\n",
      "FIRE:  200 16:55:42    -3939.920898        0.044950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:55:42    -3941.007324        0.692459\n",
      "FIRE:    1 16:55:43    -3941.028320        0.678390\n",
      "FIRE:    2 16:55:43    -3941.067383        0.651008\n",
      "FIRE:    3 16:55:44    -3941.120361        0.611337\n",
      "FIRE:    4 16:55:44    -3941.175049        0.559792\n",
      "FIRE:    5 16:55:45    -3941.228271        0.495359\n",
      "FIRE:    6 16:55:45    -3941.273682        0.415650\n",
      "FIRE:    7 16:55:46    -3941.307861        0.318694\n",
      "FIRE:    8 16:55:47    -3941.328125        0.194060\n",
      "FIRE:    9 16:55:47    -3941.337646        0.197563\n",
      "FIRE:   10 16:55:48    -3941.337646        0.194197\n",
      "FIRE:   11 16:55:48    -3941.340576        0.187570\n",
      "FIRE:   12 16:55:49    -3941.342285        0.177868\n",
      "FIRE:   13 16:55:49    -3941.347168        0.165364\n",
      "FIRE:   14 16:55:50    -3941.351562        0.150441\n",
      "FIRE:   15 16:55:50    -3941.353271        0.133519\n",
      "FIRE:   16 16:55:51    -3941.359131        0.115131\n",
      "FIRE:   17 16:55:52    -3941.362793        0.107068\n",
      "FIRE:   18 16:55:52    -3941.366455        0.118154\n",
      "FIRE:   19 16:55:53    -3941.368652        0.127923\n",
      "FIRE:   20 16:55:53    -3941.370361        0.134559\n",
      "FIRE:   21 16:55:54    -3941.373779        0.136187\n",
      "FIRE:   22 16:55:54    -3941.376709        0.130991\n",
      "FIRE:   23 16:55:55    -3941.379639        0.117262\n",
      "FIRE:   24 16:55:55    -3941.382324        0.093718\n",
      "FIRE:   25 16:55:56    -3941.384766        0.060332\n",
      "FIRE:   26 16:55:56    -3941.389160        0.032274\n",
      "FIRE:   27 16:55:57    -3941.388184        0.030073\n",
      "FIRE:   28 16:55:57    -3941.388916        0.028138\n",
      "FIRE:   29 16:55:58    -3941.389160        0.024663\n",
      "FIRE:   30 16:55:58    -3941.389404        0.021831\n",
      "FIRE:   31 16:55:59    -3941.389648        0.018283\n",
      "FIRE:   32 16:55:59    -3941.390381        0.015163\n",
      "FIRE:   33 16:56:00    -3941.390625        0.014935\n",
      "FIRE:   34 16:56:00    -3941.391357        0.014339\n",
      "FIRE:   35 16:56:01    -3941.390381        0.013139\n",
      "FIRE:   36 16:56:01    -3941.393066        0.016066\n",
      "FIRE:   37 16:56:02    -3941.392090        0.016563\n",
      "FIRE:   38 16:56:02    -3941.393555        0.014317\n",
      "FIRE:   39 16:56:03    -3941.393799        0.010548\n",
      "FIRE:   40 16:56:03    -3941.394531        0.010777\n",
      "FIRE:   41 16:56:04    -3941.394775        0.010708\n",
      "FIRE:   42 16:56:04    -3941.395508        0.012109\n",
      "FIRE:   43 16:56:05    -3941.395752        0.013524\n",
      "FIRE:   44 16:56:06    -3941.396973        0.020387\n",
      "FIRE:   45 16:56:06    -3941.396973        0.023888\n",
      "FIRE:   46 16:56:07    -3941.397461        0.020439\n",
      "FIRE:   47 16:56:07    -3941.398682        0.009572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:56:08    -3940.800537        0.385077\n",
      "FIRE:    1 16:56:08    -3940.819824        0.378590\n",
      "FIRE:    2 16:56:09    -3940.853760        0.364997\n",
      "FIRE:    3 16:56:09    -3940.899414        0.342966\n",
      "FIRE:    4 16:56:10    -3940.948730        0.310286\n",
      "FIRE:    5 16:56:10    -3940.996338        0.264009\n",
      "FIRE:    6 16:56:11    -3941.037354        0.209187\n",
      "FIRE:    7 16:56:11    -3941.064453        0.147546\n",
      "FIRE:    8 16:56:12    -3941.076904        0.097217\n",
      "FIRE:    9 16:56:13    -3941.069580        0.127271\n",
      "FIRE:   10 16:56:13    -3941.069824        0.124478\n",
      "FIRE:   11 16:56:14    -3941.072510        0.118990\n",
      "FIRE:   12 16:56:14    -3941.073975        0.110988\n",
      "FIRE:   13 16:56:15    -3941.076416        0.100744\n",
      "FIRE:   14 16:56:15    -3941.080322        0.088605\n",
      "FIRE:   15 16:56:16    -3941.081543        0.074991\n",
      "FIRE:   16 16:56:16    -3941.084717        0.060673\n",
      "FIRE:   17 16:56:17    -3941.087158        0.045638\n",
      "FIRE:   18 16:56:17    -3941.089355        0.029284\n",
      "FIRE:   19 16:56:18    -3941.087646        0.021328\n",
      "FIRE:   20 16:56:18    -3941.087646        0.021049\n",
      "FIRE:   21 16:56:19    -3941.088135        0.020492\n",
      "FIRE:   22 16:56:19    -3941.087646        0.019664\n",
      "FIRE:   23 16:56:20    -3941.088135        0.018577\n",
      "FIRE:   24 16:56:20    -3941.087646        0.017256\n",
      "FIRE:   25 16:56:21    -3941.088379        0.015710\n",
      "FIRE:   26 16:56:21    -3941.088379        0.013966\n",
      "FIRE:   27 16:56:22    -3941.088623        0.011877\n",
      "FIRE:   28 16:56:22    -3941.088867        0.010479\n",
      "FIRE:   29 16:56:23    -3941.089355        0.009710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:56:27    -3937.385986        4.347805\n",
      "FIRE:    1 16:56:30    -3937.974609        3.544226\n",
      "FIRE:    2 16:56:33    -3938.704834        2.344191\n",
      "FIRE:    3 16:56:36    -3939.165039        1.364069\n",
      "FIRE:    4 16:56:38    -3939.394775        0.834582\n",
      "FIRE:    5 16:56:43    -3939.454590        0.532155\n",
      "FIRE:    6 16:56:47    -3939.423340        0.985540\n",
      "FIRE:    7 16:56:51    -3939.435059        0.943363\n",
      "FIRE:    8 16:56:54    -3939.454590        0.861290\n",
      "FIRE:    9 16:56:57    -3939.479004        0.743761\n",
      "FIRE:   10 16:56:59    -3939.507324        0.597054\n",
      "FIRE:   11 16:57:02    -3939.531738        0.429071\n",
      "FIRE:   12 16:57:04    -3939.552979        0.249561\n",
      "FIRE:   13 16:57:07    -3939.567139        0.181431\n",
      "FIRE:   14 16:57:09    -3939.573975        0.187042\n",
      "FIRE:   15 16:57:12    -3939.573975        0.288401\n",
      "FIRE:   16 16:57:15    -3939.570068        0.408830\n",
      "FIRE:   17 16:57:17    -3939.568359        0.459496\n",
      "FIRE:   18 16:57:20    -3939.570557        0.424720\n",
      "FIRE:   19 16:57:22    -3939.574707        0.302962\n",
      "FIRE:   20 16:57:25    -3939.582275        0.206098\n",
      "FIRE:   21 16:57:27    -3939.581787        0.238142\n",
      "FIRE:   22 16:57:30    -3939.573975        0.256820\n",
      "FIRE:   23 16:57:32    -3939.574951        0.242185\n",
      "FIRE:   24 16:57:35    -3939.576416        0.213954\n",
      "FIRE:   25 16:57:38    -3939.580322        0.174189\n",
      "FIRE:   26 16:57:41    -3939.581543        0.146521\n",
      "FIRE:   27 16:57:43    -3939.583008        0.135102\n",
      "FIRE:   28 16:57:46    -3939.584229        0.122869\n",
      "FIRE:   29 16:57:46    -3939.585449        0.107791\n",
      "FIRE:   30 16:57:47    -3939.586182        0.094944\n",
      "FIRE:   31 16:57:48    -3939.586182        0.094959\n",
      "FIRE:   32 16:57:51    -3939.585693        0.097709\n",
      "FIRE:   33 16:57:52    -3939.585938        0.090050\n",
      "FIRE:   34 16:57:54    -3939.587646        0.075840\n",
      "FIRE:   35 16:57:55    -3939.587646        0.062820\n",
      "FIRE:   36 16:57:57    -3939.587646        0.058601\n",
      "FIRE:   37 16:57:57    -3939.587646        0.041081\n",
      "FIRE:   38 16:57:57    -3939.587646        0.041081\n",
      "FIRE:   39 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   40 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   41 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   42 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   43 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   44 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   45 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   46 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   47 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   48 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   49 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   50 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   51 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   52 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   53 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   54 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   55 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   56 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   57 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   58 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   59 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   60 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   61 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   62 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   63 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   64 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   65 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   66 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   67 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   68 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   69 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   70 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   71 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   72 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   73 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   74 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   75 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   76 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   77 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   78 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   79 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   80 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   81 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   82 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   83 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   84 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   85 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   86 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   87 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   88 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   89 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   90 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   91 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   92 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   93 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   94 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   95 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   96 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   97 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   98 16:57:58    -3939.587646        0.041081\n",
      "FIRE:   99 16:57:58    -3939.587646        0.041081\n",
      "FIRE:  100 16:57:58    -3939.587646        0.041081\n",
      "FIRE:  101 16:57:58    -3939.587646        0.041081\n",
      "FIRE:  102 16:57:58    -3939.587646        0.041081\n",
      "FIRE:  103 16:57:58    -3939.587646        0.041081\n",
      "FIRE:  104 16:57:58    -3939.587646        0.041081\n",
      "FIRE:  105 16:57:58    -3939.587646        0.041081\n",
      "FIRE:  106 16:57:58    -3939.587646        0.041081\n",
      "FIRE:  107 16:57:58    -3939.587646        0.041081\n",
      "FIRE:  108 16:57:58    -3939.587646        0.041081\n",
      "FIRE:  109 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  110 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  111 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  112 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  113 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  114 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  115 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  116 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  117 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  118 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  119 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  120 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  121 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  122 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  123 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  124 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  125 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  126 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  127 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  128 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  129 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  130 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  131 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  132 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  133 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  134 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  135 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  136 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  137 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  138 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  139 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  140 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  141 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  142 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  143 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  144 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  145 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  146 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  147 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  148 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  149 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  150 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  151 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  152 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  153 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  154 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  155 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  156 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  157 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  158 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  159 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  160 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  161 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  162 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  163 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  164 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  165 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  166 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  167 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  168 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  169 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  170 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  171 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  172 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  173 16:57:59    -3939.587646        0.041081\n",
      "FIRE:  174 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  175 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  176 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  177 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  178 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  179 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  180 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  181 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  182 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  183 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  184 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  185 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  186 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  187 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  188 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  189 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  190 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  191 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  192 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  193 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  194 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  195 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  196 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  197 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  198 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  199 16:58:00    -3939.587646        0.041081\n",
      "FIRE:  200 16:58:00    -3939.587646        0.041081\n",
      "Progress: 12/42 calculations completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:58:01    -3941.007324        0.692459\n",
      "FIRE:    1 16:58:01    -3941.028320        0.678389\n",
      "FIRE:    2 16:58:02    -3941.067627        0.651009\n",
      "FIRE:    3 16:58:02    -3941.120361        0.611337\n",
      "FIRE:    4 16:58:03    -3941.175537        0.559793\n",
      "FIRE:    5 16:58:03    -3941.228271        0.495360\n",
      "FIRE:    6 16:58:04    -3941.273682        0.415652\n",
      "FIRE:    7 16:58:04    -3941.307861        0.318693\n",
      "FIRE:    8 16:58:05    -3941.328125        0.194058\n",
      "FIRE:    9 16:58:05    -3941.337402        0.197562\n",
      "FIRE:   10 16:58:06    -3941.337402        0.194199\n",
      "FIRE:   11 16:58:06    -3941.340576        0.187570\n",
      "FIRE:   12 16:58:07    -3941.342529        0.177868\n",
      "FIRE:   13 16:58:07    -3941.346924        0.165367\n",
      "FIRE:   14 16:58:08    -3941.351318        0.150442\n",
      "FIRE:   15 16:58:08    -3941.353271        0.133520\n",
      "FIRE:   16 16:58:09    -3941.359131        0.115130\n",
      "FIRE:   17 16:58:09    -3941.362793        0.107066\n",
      "FIRE:   18 16:58:10    -3941.366455        0.118153\n",
      "FIRE:   19 16:58:11    -3941.368896        0.127923\n",
      "FIRE:   20 16:58:11    -3941.370361        0.134558\n",
      "FIRE:   21 16:58:12    -3941.373535        0.136186\n",
      "FIRE:   22 16:58:12    -3941.376953        0.130991\n",
      "FIRE:   23 16:58:13    -3941.379639        0.117264\n",
      "FIRE:   24 16:58:13    -3941.382080        0.093716\n",
      "FIRE:   25 16:58:14    -3941.384766        0.060332\n",
      "FIRE:   26 16:58:14    -3941.389160        0.032273\n",
      "FIRE:   27 16:58:15    -3941.388428        0.030084\n",
      "FIRE:   28 16:58:16    -3941.388916        0.028146\n",
      "FIRE:   29 16:58:16    -3941.389160        0.024669\n",
      "FIRE:   30 16:58:17    -3941.389404        0.021829\n",
      "FIRE:   31 16:58:17    -3941.390137        0.018285\n",
      "FIRE:   32 16:58:18    -3941.390381        0.015162\n",
      "FIRE:   33 16:58:18    -3941.390869        0.014932\n",
      "FIRE:   34 16:58:19    -3941.391113        0.014344\n",
      "FIRE:   35 16:58:19    -3941.390137        0.013139\n",
      "FIRE:   36 16:58:20    -3941.392822        0.016066\n",
      "FIRE:   37 16:58:20    -3941.392090        0.016566\n",
      "FIRE:   38 16:58:21    -3941.393311        0.014316\n",
      "FIRE:   39 16:58:21    -3941.393555        0.010542\n",
      "FIRE:   40 16:58:22    -3941.394531        0.010779\n",
      "FIRE:   41 16:58:23    -3941.394775        0.010711\n",
      "FIRE:   42 16:58:23    -3941.395752        0.012107\n",
      "FIRE:   43 16:58:24    -3941.396240        0.013522\n",
      "FIRE:   44 16:58:24    -3941.396729        0.020386\n",
      "FIRE:   45 16:58:25    -3941.397217        0.023888\n",
      "FIRE:   46 16:58:25    -3941.397705        0.020446\n",
      "FIRE:   47 16:58:26    -3941.398682        0.009572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:58:27    -3941.001465        0.661112\n",
      "FIRE:    1 16:58:27    -3941.025391        0.650066\n",
      "FIRE:    2 16:58:28    -3941.071777        0.628402\n",
      "FIRE:    3 16:58:28    -3941.133301        0.596461\n",
      "FIRE:    4 16:58:29    -3941.200439        0.553663\n",
      "FIRE:    5 16:58:29    -3941.265625        0.497747\n",
      "FIRE:    6 16:58:30    -3941.320801        0.424924\n",
      "FIRE:    7 16:58:30    -3941.363281        0.331925\n",
      "FIRE:    8 16:58:31    -3941.388672        0.207651\n",
      "FIRE:    9 16:58:32    -3941.394531        0.229925\n",
      "FIRE:   10 16:58:32    -3941.395996        0.225791\n",
      "FIRE:   11 16:58:33    -3941.398926        0.217664\n",
      "FIRE:   12 16:58:33    -3941.401367        0.205815\n",
      "FIRE:   13 16:58:34    -3941.406250        0.190659\n",
      "FIRE:   14 16:58:35    -3941.412109        0.172712\n",
      "FIRE:   15 16:58:35    -3941.417236        0.152599\n",
      "FIRE:   16 16:58:36    -3941.422119        0.131041\n",
      "FIRE:   17 16:58:36    -3941.426270        0.123293\n",
      "FIRE:   18 16:58:37    -3941.428955        0.134562\n",
      "FIRE:   19 16:58:37    -3941.433350        0.143739\n",
      "FIRE:   20 16:58:38    -3941.435303        0.148684\n",
      "FIRE:   21 16:58:39    -3941.438965        0.147359\n",
      "FIRE:   22 16:58:40    -3941.440674        0.137978\n",
      "FIRE:   23 16:58:41    -3941.443604        0.119028\n",
      "FIRE:   24 16:58:42    -3941.448486        0.089679\n",
      "FIRE:   25 16:58:43    -3941.448730        0.050747\n",
      "FIRE:   26 16:58:44    -3941.452393        0.038739\n",
      "FIRE:   27 16:58:45    -3941.452393        0.041274\n",
      "FIRE:   28 16:58:45    -3941.453369        0.040033\n",
      "FIRE:   29 16:58:47    -3941.453857        0.037624\n",
      "FIRE:   30 16:58:47    -3941.454346        0.034182\n",
      "FIRE:   31 16:58:48    -3941.454834        0.029912\n",
      "FIRE:   32 16:58:49    -3941.454834        0.025086\n",
      "FIRE:   33 16:58:49    -3941.455566        0.020026\n",
      "FIRE:   34 16:58:50    -3941.456055        0.015170\n",
      "FIRE:   35 16:58:50    -3941.455566        0.013835\n",
      "FIRE:   36 16:58:51    -3941.455322        0.016351\n",
      "FIRE:   37 16:58:51    -3941.454834        0.016514\n",
      "FIRE:   38 16:58:52    -3941.455078        0.016096\n",
      "FIRE:   39 16:58:53    -3941.456055        0.013161\n",
      "FIRE:   40 16:58:53    -3941.457275        0.009704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 16:58:58    -3935.852295        9.526466\n",
      "FIRE:    1 16:59:01    -3937.120605        5.848660\n",
      "FIRE:    2 16:59:04    -3938.088867        3.562032\n",
      "FIRE:    3 16:59:06    -3938.696777        2.371972\n",
      "FIRE:    4 16:59:09    -3939.032715        1.638613\n",
      "FIRE:    5 16:59:12    -3939.166992        1.063407\n",
      "FIRE:    6 16:59:14    -3939.175537        0.804035\n",
      "FIRE:    7 16:59:17    -3939.131104        1.145423\n",
      "FIRE:    8 16:59:20    -3939.097900        1.354128\n",
      "FIRE:    9 16:59:23    -3939.118164        1.299599\n",
      "FIRE:   10 16:59:25    -3939.157471        1.193180\n",
      "FIRE:   11 16:59:28    -3939.205811        1.039929\n",
      "FIRE:   12 16:59:31    -3939.258545        0.851372\n",
      "FIRE:   13 16:59:33    -3939.307861        0.635463\n",
      "FIRE:   14 16:59:36    -3939.345947        0.398823\n",
      "FIRE:   15 16:59:39    -3939.367188        0.232446\n",
      "FIRE:   16 16:59:42    -3939.374512        0.173044\n",
      "FIRE:   17 16:59:44    -3939.366943        0.374818\n",
      "FIRE:   18 16:59:47    -3939.353516        0.592280\n",
      "FIRE:   19 16:59:50    -3939.354736        0.583280\n",
      "FIRE:   20 16:59:52    -3939.358398        0.565429\n",
      "FIRE:   21 16:59:55    -3939.362793        0.539029\n",
      "FIRE:   22 16:59:58    -3939.366699        0.504533\n",
      "FIRE:   23 17:00:00    -3939.373291        0.462547\n",
      "FIRE:   24 17:00:04    -3939.379395        0.413816\n",
      "FIRE:   25 17:00:08    -3939.385254        0.359222\n",
      "FIRE:   26 17:00:11    -3939.390625        0.293329\n",
      "FIRE:   27 17:00:14    -3939.395020        0.215723\n",
      "FIRE:   28 17:00:16    -3939.398926        0.131272\n",
      "FIRE:   29 17:00:19    -3939.400635        0.089286\n",
      "FIRE:   30 17:00:21    -3939.398926        0.152222\n",
      "FIRE:   31 17:00:24    -3939.398926        0.203287\n",
      "FIRE:   32 17:00:27    -3939.398193        0.226155\n",
      "FIRE:   33 17:00:29    -3939.398682        0.240715\n",
      "FIRE:   34 17:00:32    -3939.402344        0.231378\n",
      "FIRE:   35 17:00:35    -3939.406738        0.177133\n",
      "FIRE:   36 17:00:37    -3939.410156        0.093594\n",
      "FIRE:   37 17:00:41    -3939.410645        0.085663\n",
      "FIRE:   38 17:00:43    -3939.410645        0.081211\n",
      "FIRE:   39 17:00:45    -3939.411133        0.072546\n",
      "FIRE:   40 17:00:45    -3939.410645        0.060007\n",
      "FIRE:   41 17:00:46    -3939.412109        0.044436\n",
      "FIRE:   42 17:00:46    -3939.412109        0.044436\n",
      "FIRE:   43 17:00:46    -3939.412109        0.044436\n",
      "FIRE:   44 17:00:46    -3939.412109        0.044436\n",
      "FIRE:   45 17:00:46    -3939.412109        0.044436\n",
      "FIRE:   46 17:00:46    -3939.412109        0.044436\n",
      "FIRE:   47 17:00:46    -3939.412109        0.044436\n",
      "FIRE:   48 17:00:46    -3939.412109        0.044436\n",
      "FIRE:   49 17:00:46    -3939.412109        0.044436\n",
      "FIRE:   50 17:00:46    -3939.412109        0.044436\n",
      "FIRE:   51 17:00:46    -3939.412109        0.044436\n",
      "FIRE:   52 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   53 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   54 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   55 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   56 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   57 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   58 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   59 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   60 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   61 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   62 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   63 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   64 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   65 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   66 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   67 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   68 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   69 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   70 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   71 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   72 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   73 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   74 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   75 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   76 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   77 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   78 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   79 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   80 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   81 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   82 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   83 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   84 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   85 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   86 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   87 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   88 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   89 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   90 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   91 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   92 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   93 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   94 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   95 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   96 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   97 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   98 17:00:47    -3939.412109        0.044436\n",
      "FIRE:   99 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  100 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  101 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  102 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  103 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  104 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  105 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  106 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  107 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  108 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  109 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  110 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  111 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  112 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  113 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  114 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  115 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  116 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  117 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  118 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  119 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  120 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  121 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  122 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  123 17:00:47    -3939.412109        0.044436\n",
      "FIRE:  124 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  125 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  126 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  127 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  128 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  129 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  130 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  131 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  132 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  133 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  134 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  135 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  136 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  137 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  138 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  139 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  140 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  141 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  142 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  143 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  144 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  145 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  146 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  147 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  148 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  149 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  150 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  151 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  152 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  153 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  154 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  155 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  156 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  157 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  158 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  159 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  160 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  161 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  162 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  163 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  164 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  165 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  166 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  167 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  168 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  169 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  170 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  171 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  172 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  173 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  174 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  175 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  176 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  177 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  178 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  179 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  180 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  181 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  182 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  183 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  184 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  185 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  186 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  187 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  188 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  189 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  190 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  191 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  192 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  193 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  194 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  195 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  196 17:00:48    -3939.412109        0.044436\n",
      "FIRE:  197 17:00:49    -3939.412109        0.044436\n",
      "FIRE:  198 17:00:49    -3939.412109        0.044436\n",
      "FIRE:  199 17:00:49    -3939.412109        0.044436\n",
      "FIRE:  200 17:00:49    -3939.412109        0.044436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:00:50    -3941.007324        0.692459\n",
      "FIRE:    1 17:00:51    -3941.028320        0.678389\n",
      "FIRE:    2 17:00:52    -3941.067627        0.651009\n",
      "FIRE:    3 17:00:52    -3941.120361        0.611337\n",
      "FIRE:    4 17:00:53    -3941.175049        0.559792\n",
      "FIRE:    5 17:00:54    -3941.228271        0.495361\n",
      "FIRE:    6 17:00:54    -3941.273926        0.415652\n",
      "FIRE:    7 17:00:55    -3941.307861        0.318695\n",
      "FIRE:    8 17:00:55    -3941.327881        0.194058\n",
      "FIRE:    9 17:00:56    -3941.337402        0.197563\n",
      "FIRE:   10 17:00:57    -3941.337646        0.194199\n",
      "FIRE:   11 17:00:57    -3941.340576        0.187572\n",
      "FIRE:   12 17:00:58    -3941.342285        0.177870\n",
      "FIRE:   13 17:00:58    -3941.347168        0.165365\n",
      "FIRE:   14 17:00:59    -3941.351318        0.150441\n",
      "FIRE:   15 17:00:59    -3941.353271        0.133518\n",
      "FIRE:   16 17:01:00    -3941.359375        0.115129\n",
      "FIRE:   17 17:01:01    -3941.363281        0.107068\n",
      "FIRE:   18 17:01:01    -3941.366211        0.118153\n",
      "FIRE:   19 17:01:02    -3941.368652        0.127922\n",
      "FIRE:   20 17:01:02    -3941.370605        0.134559\n",
      "FIRE:   21 17:01:03    -3941.373535        0.136187\n",
      "FIRE:   22 17:01:03    -3941.376709        0.130989\n",
      "FIRE:   23 17:01:04    -3941.379150        0.117264\n",
      "FIRE:   24 17:01:05    -3941.382080        0.093717\n",
      "FIRE:   25 17:01:05    -3941.384766        0.060334\n",
      "FIRE:   26 17:01:06    -3941.389160        0.032277\n",
      "FIRE:   27 17:01:07    -3941.388428        0.030075\n",
      "FIRE:   28 17:01:07    -3941.389160        0.028146\n",
      "FIRE:   29 17:01:08    -3941.389160        0.024667\n",
      "FIRE:   30 17:01:08    -3941.389404        0.021827\n",
      "FIRE:   31 17:01:09    -3941.390137        0.018287\n",
      "FIRE:   32 17:01:09    -3941.390381        0.015165\n",
      "FIRE:   33 17:01:10    -3941.390625        0.014934\n",
      "FIRE:   34 17:01:10    -3941.391357        0.014341\n",
      "FIRE:   35 17:01:11    -3941.390381        0.013139\n",
      "FIRE:   36 17:01:11    -3941.393066        0.016069\n",
      "FIRE:   37 17:01:12    -3941.391846        0.016566\n",
      "FIRE:   38 17:01:12    -3941.393311        0.014315\n",
      "FIRE:   39 17:01:13    -3941.394043        0.010543\n",
      "FIRE:   40 17:01:13    -3941.394531        0.010779\n",
      "FIRE:   41 17:01:14    -3941.394531        0.010711\n",
      "FIRE:   42 17:01:15    -3941.395264        0.012110\n",
      "FIRE:   43 17:01:15    -3941.395996        0.013519\n",
      "FIRE:   44 17:01:16    -3941.396729        0.020389\n",
      "FIRE:   45 17:01:16    -3941.396973        0.023889\n",
      "FIRE:   46 17:01:17    -3941.397461        0.020446\n",
      "FIRE:   47 17:01:17    -3941.398682        0.009571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:01:18    -3940.701416        0.514415\n",
      "FIRE:    1 17:01:18    -3940.722900        0.504976\n",
      "FIRE:    2 17:01:19    -3940.765869        0.486484\n",
      "FIRE:    3 17:01:20    -3940.816406        0.459193\n",
      "FIRE:    4 17:01:20    -3940.873047        0.422332\n",
      "FIRE:    5 17:01:21    -3940.923340        0.373359\n",
      "FIRE:    6 17:01:21    -3940.966797        0.308466\n",
      "FIRE:    7 17:01:22    -3941.002441        0.227189\n",
      "FIRE:    8 17:01:22    -3941.025391        0.130467\n",
      "FIRE:    9 17:01:23    -3941.025146        0.166275\n",
      "FIRE:   10 17:01:23    -3941.027588        0.161237\n",
      "FIRE:   11 17:01:24    -3941.030762        0.151401\n",
      "FIRE:   12 17:01:24    -3941.034424        0.137267\n",
      "FIRE:   13 17:01:25    -3941.036133        0.119585\n",
      "FIRE:   14 17:01:25    -3941.040039        0.099364\n",
      "FIRE:   15 17:01:26    -3941.042480        0.077961\n",
      "FIRE:   16 17:01:26    -3941.043945        0.057280\n",
      "FIRE:   17 17:01:27    -3941.046143        0.039022\n",
      "FIRE:   18 17:01:28    -3941.046387        0.031671\n",
      "FIRE:   19 17:01:28    -3941.046387        0.037811\n",
      "FIRE:   20 17:01:29    -3941.045410        0.037134\n",
      "FIRE:   21 17:01:29    -3941.045654        0.035794\n",
      "FIRE:   22 17:01:30    -3941.046143        0.033814\n",
      "FIRE:   23 17:01:30    -3941.046875        0.031656\n",
      "FIRE:   24 17:01:31    -3941.047119        0.029720\n",
      "FIRE:   25 17:01:31    -3941.048340        0.027470\n",
      "FIRE:   26 17:01:32    -3941.048828        0.024959\n",
      "FIRE:   27 17:01:32    -3941.048340        0.021938\n",
      "FIRE:   28 17:01:33    -3941.048096        0.018981\n",
      "FIRE:   29 17:01:34    -3941.048340        0.017255\n",
      "FIRE:   30 17:01:34    -3941.048584        0.015456\n",
      "FIRE:   31 17:01:35    -3941.048096        0.013720\n",
      "FIRE:   32 17:01:36    -3941.048096        0.014473\n",
      "FIRE:   33 17:01:37    -3941.048828        0.016722\n",
      "FIRE:   34 17:01:37    -3941.048340        0.017529\n",
      "FIRE:   35 17:01:38    -3941.049561        0.016393\n",
      "FIRE:   36 17:01:39    -3941.049072        0.013143\n",
      "FIRE:   37 17:01:39    -3941.048584        0.009130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:01:44    -3935.323975        6.976619\n",
      "FIRE:    1 17:01:48    -3936.793701        4.647290\n",
      "FIRE:    2 17:01:51    -3937.783691        3.188551\n",
      "FIRE:    3 17:01:55    -3938.397217        2.094179\n",
      "FIRE:    4 17:01:57    -3938.724121        1.339868\n",
      "FIRE:    5 17:02:00    -3938.845947        0.862605\n",
      "FIRE:    6 17:02:03    -3938.843018        0.939451\n",
      "FIRE:    7 17:02:06    -3938.795166        1.251174\n",
      "FIRE:    8 17:02:09    -3938.813721        1.191569\n",
      "FIRE:    9 17:02:12    -3938.847900        1.075936\n",
      "FIRE:   10 17:02:14    -3938.893311        0.911202\n",
      "FIRE:   11 17:02:17    -3938.941162        0.707174\n",
      "FIRE:   12 17:02:21    -3938.984375        0.476234\n",
      "FIRE:   13 17:02:23    -3939.018311        0.274813\n",
      "FIRE:   14 17:02:26    -3939.039795        0.221742\n",
      "FIRE:   15 17:02:29    -3939.045166        0.237686\n",
      "FIRE:   16 17:02:32    -3939.039551        0.423887\n",
      "FIRE:   17 17:02:34    -3939.027100        0.572173\n",
      "FIRE:   18 17:02:39    -3939.029297        0.561809\n",
      "FIRE:   19 17:02:42    -3939.031982        0.541316\n",
      "FIRE:   20 17:02:46    -3939.034668        0.511175\n",
      "FIRE:   21 17:02:50    -3939.041260        0.472094\n",
      "FIRE:   22 17:02:54    -3939.045654        0.425006\n",
      "FIRE:   23 17:02:59    -3939.050049        0.371072\n",
      "FIRE:   24 17:03:02    -3939.056396        0.311663\n",
      "FIRE:   25 17:03:05    -3939.061279        0.285476\n",
      "FIRE:   26 17:03:09    -3939.064453        0.295580\n",
      "FIRE:   27 17:03:14    -3939.067383        0.308268\n",
      "FIRE:   28 17:03:19    -3939.066895        0.323678\n",
      "FIRE:   29 17:03:23    -3939.064209        0.341435\n",
      "FIRE:   30 17:03:26    -3939.061768        0.360421\n",
      "FIRE:   31 17:03:28    -3939.055908        0.378665\n",
      "FIRE:   32 17:03:31    -3939.050781        0.392894\n",
      "FIRE:   33 17:03:33    -3939.046143        0.397293\n",
      "FIRE:   34 17:03:36    -3939.041260        0.381731\n",
      "FIRE:   35 17:03:40    -3939.033691        0.330638\n",
      "FIRE:   36 17:03:43    -3939.025879        0.227234\n",
      "FIRE:   37 17:03:45    -3939.016602        0.159304\n",
      "FIRE:   38 17:03:48    -3939.018311        0.153308\n",
      "FIRE:   39 17:03:51    -3939.018311        0.143347\n",
      "FIRE:   40 17:03:53    -3939.021484        0.124095\n",
      "FIRE:   41 17:03:55    -3939.022461        0.106467\n",
      "FIRE:   42 17:03:57    -3939.022949        0.096091\n",
      "FIRE:   43 17:03:59    -3939.023193        0.086769\n",
      "FIRE:   44 17:04:00    -3939.024414        0.078923\n",
      "FIRE:   45 17:04:01    -3939.024902        0.070299\n",
      "FIRE:   46 17:04:01    -3939.023926        0.057819\n",
      "FIRE:   47 17:04:02    -3939.025146        0.052680\n",
      "FIRE:   48 17:04:03    -3939.025146        0.049334\n",
      "FIRE:   49 17:04:03    -3939.025146        0.049334\n",
      "FIRE:   50 17:04:03    -3939.025146        0.049334\n",
      "FIRE:   51 17:04:03    -3939.025146        0.049334\n",
      "FIRE:   52 17:04:03    -3939.025146        0.049334\n",
      "FIRE:   53 17:04:03    -3939.025146        0.049334\n",
      "FIRE:   54 17:04:03    -3939.025146        0.049334\n",
      "FIRE:   55 17:04:03    -3939.025146        0.049334\n",
      "FIRE:   56 17:04:03    -3939.025146        0.049334\n",
      "FIRE:   57 17:04:03    -3939.025146        0.049334\n",
      "FIRE:   58 17:04:03    -3939.025146        0.049334\n",
      "FIRE:   59 17:04:03    -3939.025146        0.049334\n",
      "FIRE:   60 17:04:03    -3939.025146        0.049334\n",
      "FIRE:   61 17:04:03    -3939.025146        0.049334\n",
      "FIRE:   62 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   63 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   64 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   65 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   66 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   67 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   68 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   69 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   70 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   71 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   72 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   73 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   74 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   75 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   76 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   77 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   78 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   79 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   80 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   81 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   82 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   83 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   84 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   85 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   86 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   87 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   88 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   89 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   90 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   91 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   92 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   93 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   94 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   95 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   96 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   97 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   98 17:04:04    -3939.025146        0.049334\n",
      "FIRE:   99 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  100 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  101 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  102 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  103 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  104 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  105 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  106 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  107 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  108 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  109 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  110 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  111 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  112 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  113 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  114 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  115 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  116 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  117 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  118 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  119 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  120 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  121 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  122 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  123 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  124 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  125 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  126 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  127 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  128 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  129 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  130 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  131 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  132 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  133 17:04:04    -3939.025146        0.049334\n",
      "FIRE:  134 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  135 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  136 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  137 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  138 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  139 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  140 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  141 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  142 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  143 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  144 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  145 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  146 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  147 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  148 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  149 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  150 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  151 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  152 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  153 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  154 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  155 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  156 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  157 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  158 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  159 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  160 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  161 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  162 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  163 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  164 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  165 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  166 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  167 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  168 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  169 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  170 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  171 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  172 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  173 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  174 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  175 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  176 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  177 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  178 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  179 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  180 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  181 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  182 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  183 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  184 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  185 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  186 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  187 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  188 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  189 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  190 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  191 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  192 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  193 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  194 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  195 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  196 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  197 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  198 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  199 17:04:05    -3939.025146        0.049334\n",
      "FIRE:  200 17:04:05    -3939.025146        0.049334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:04:06    -3940.666504        0.521325\n",
      "FIRE:    1 17:04:07    -3940.683350        0.490063\n",
      "FIRE:    2 17:04:07    -3940.716553        0.432639\n",
      "FIRE:    3 17:04:08    -3940.758545        0.361905\n",
      "FIRE:    4 17:04:08    -3940.799805        0.325827\n",
      "FIRE:    5 17:04:09    -3940.837402        0.274776\n",
      "FIRE:    6 17:04:09    -3940.869141        0.205750\n",
      "FIRE:    7 17:04:10    -3940.892822        0.172088\n",
      "FIRE:    8 17:04:11    -3940.906006        0.158779\n",
      "FIRE:    9 17:04:11    -3940.905518        0.183579\n",
      "FIRE:   10 17:04:12    -3940.906738        0.179349\n",
      "FIRE:   11 17:04:12    -3940.909912        0.171012\n",
      "FIRE:   12 17:04:13    -3940.911865        0.158816\n",
      "FIRE:   13 17:04:13    -3940.915283        0.143133\n",
      "FIRE:   14 17:04:14    -3940.916260        0.124403\n",
      "FIRE:   15 17:04:14    -3940.919922        0.103209\n",
      "FIRE:   16 17:04:15    -3940.922119        0.080161\n",
      "FIRE:   17 17:04:15    -3940.923584        0.053497\n",
      "FIRE:   18 17:04:16    -3940.925781        0.041347\n",
      "FIRE:   19 17:04:16    -3940.927734        0.050364\n",
      "FIRE:   20 17:04:17    -3940.927002        0.049846\n",
      "FIRE:   21 17:04:17    -3940.928223        0.048813\n",
      "FIRE:   22 17:04:18    -3940.928223        0.047278\n",
      "FIRE:   23 17:04:18    -3940.927490        0.045278\n",
      "FIRE:   24 17:04:19    -3940.927734        0.042842\n",
      "FIRE:   25 17:04:19    -3940.927246        0.040014\n",
      "FIRE:   26 17:04:20    -3940.927979        0.036856\n",
      "FIRE:   27 17:04:21    -3940.928711        0.033059\n",
      "FIRE:   28 17:04:21    -3940.927734        0.031113\n",
      "FIRE:   29 17:04:22    -3940.928223        0.030502\n",
      "FIRE:   30 17:04:22    -3940.927979        0.029711\n",
      "FIRE:   31 17:04:23    -3940.929443        0.028605\n",
      "FIRE:   32 17:04:23    -3940.929199        0.026984\n",
      "FIRE:   33 17:04:24    -3940.929199        0.024591\n",
      "FIRE:   34 17:04:25    -3940.927246        0.021167\n",
      "FIRE:   35 17:04:26    -3940.927979        0.016507\n",
      "FIRE:   36 17:04:27    -3940.929932        0.010552\n",
      "FIRE:   37 17:04:27    -3940.929688        0.007594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:04:28    -3940.976074        0.610202\n",
      "FIRE:    1 17:04:29    -3940.998535        0.598212\n",
      "FIRE:    2 17:04:29    -3941.037354        0.574392\n",
      "FIRE:    3 17:04:30    -3941.089600        0.538822\n",
      "FIRE:    4 17:04:30    -3941.144531        0.491161\n",
      "FIRE:    5 17:04:31    -3941.196289        0.430555\n",
      "FIRE:    6 17:04:31    -3941.241211        0.356208\n",
      "FIRE:    7 17:04:32    -3941.271973        0.269055\n",
      "FIRE:    8 17:04:32    -3941.288574        0.164083\n",
      "FIRE:    9 17:04:33    -3941.290527        0.187923\n",
      "FIRE:   10 17:04:33    -3941.291992        0.184869\n",
      "FIRE:   11 17:04:34    -3941.293457        0.178853\n",
      "FIRE:   12 17:04:34    -3941.297119        0.170047\n",
      "FIRE:   13 17:04:35    -3941.299072        0.158708\n",
      "FIRE:   14 17:04:36    -3941.303711        0.145163\n",
      "FIRE:   15 17:04:36    -3941.308105        0.129807\n",
      "FIRE:   16 17:04:37    -3941.309326        0.113131\n",
      "FIRE:   17 17:04:38    -3941.315186        0.093872\n",
      "FIRE:   18 17:04:38    -3941.318115        0.073895\n",
      "FIRE:   19 17:04:39    -3941.319336        0.081457\n",
      "FIRE:   20 17:04:40    -3941.321289        0.087003\n",
      "FIRE:   21 17:04:40    -3941.322021        0.089223\n",
      "FIRE:   22 17:04:41    -3941.322510        0.087150\n",
      "FIRE:   23 17:04:41    -3941.322021        0.079837\n",
      "FIRE:   24 17:04:42    -3941.324707        0.066226\n",
      "FIRE:   25 17:04:42    -3941.327637        0.050729\n",
      "FIRE:   26 17:04:43    -3941.328369        0.040150\n",
      "FIRE:   27 17:04:43    -3941.330566        0.030332\n",
      "FIRE:   28 17:04:44    -3941.330322        0.043796\n",
      "FIRE:   29 17:04:44    -3941.330811        0.042535\n",
      "FIRE:   30 17:04:45    -3941.332520        0.040119\n",
      "FIRE:   31 17:04:45    -3941.332031        0.036761\n",
      "FIRE:   32 17:04:46    -3941.332520        0.032716\n",
      "FIRE:   33 17:04:47    -3941.332764        0.028271\n",
      "FIRE:   34 17:04:47    -3941.332031        0.023667\n",
      "FIRE:   35 17:04:48    -3941.333008        0.019081\n",
      "FIRE:   36 17:04:48    -3941.334961        0.016516\n",
      "FIRE:   37 17:04:49    -3941.333984        0.017082\n",
      "FIRE:   38 17:04:49    -3941.333252        0.015815\n",
      "FIRE:   39 17:04:50    -3941.334717        0.013449\n",
      "FIRE:   40 17:04:50    -3941.333984        0.008566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:04:55    -3781.367188      394.825752\n",
      "FIRE:    1 17:04:57    -3795.197266      357.527327\n",
      "FIRE:    2 17:05:00    -3842.569092      416.316148\n",
      "FIRE:    3 17:05:03    -3896.318848      199.965151\n",
      "FIRE:    4 17:05:05    -3915.232178       60.462933\n",
      "FIRE:    5 17:05:08    -3919.427734       49.479576\n",
      "FIRE:    6 17:05:11    -3923.328369       45.208624\n",
      "FIRE:    7 17:05:13    -3924.425781       38.181897\n",
      "FIRE:    8 17:05:17    -3925.064941       31.295984\n",
      "FIRE:    9 17:05:20    -3925.634766       23.087416\n",
      "FIRE:   10 17:05:23    -3925.355469       24.446231\n",
      "FIRE:   11 17:05:26    -3924.764404       32.870201\n",
      "FIRE:   12 17:05:28    -3925.564941       15.825249\n",
      "FIRE:   13 17:05:31    -3925.107910       15.550425\n",
      "FIRE:   14 17:05:34    -3924.138184       27.274155\n",
      "FIRE:   15 17:05:37    -3922.046875       47.823802\n",
      "FIRE:   16 17:05:40    -3919.706055       64.095878\n",
      "FIRE:   17 17:05:43    -3917.225098       75.725895\n",
      "FIRE:   18 17:05:46    -3914.491455       86.503224\n",
      "FIRE:   19 17:05:48    -3911.843262      100.717132\n",
      "FIRE:   20 17:05:51    -3909.536133      117.806803\n",
      "FIRE:   21 17:05:54    -3907.693115      130.262501\n",
      "FIRE:   22 17:05:56    -3906.441650      133.426031\n",
      "FIRE:   23 17:05:59    -3905.783203      130.657891\n",
      "FIRE:   24 17:06:02    -3905.574951      127.882094\n",
      "FIRE:   25 17:06:05    -3905.644775      128.061371\n",
      "FIRE:   26 17:06:08    -3905.901367      129.306178\n",
      "FIRE:   27 17:06:10    -3906.347412      128.741383\n",
      "FIRE:   28 17:06:13    -3906.997559      124.842109\n",
      "FIRE:   29 17:06:16    -3907.781738      118.542023\n",
      "FIRE:   30 17:06:19    -3908.629883      112.131423\n",
      "FIRE:   31 17:06:21    -3909.507568      106.512452\n",
      "FIRE:   32 17:06:24    -3910.340820      101.308443\n",
      "FIRE:   33 17:06:27    -3910.985840       99.046833\n",
      "FIRE:   34 17:06:29    -3911.225586       98.639941\n",
      "FIRE:   35 17:06:32    -3910.858887      100.254980\n",
      "FIRE:   36 17:06:35    -3909.713135      105.365899\n",
      "FIRE:   37 17:06:38    -3907.642578      115.642806\n",
      "FIRE:   38 17:06:41    -3904.545654      132.489875\n",
      "FIRE:   39 17:06:44    -3900.309570      157.040457\n",
      "FIRE:   40 17:06:46    -3894.823242      188.520965\n",
      "FIRE:   41 17:06:49    -3888.093506      223.675464\n",
      "FIRE:   42 17:06:52    -3880.064209      261.841109\n",
      "FIRE:   43 17:06:55    -3870.326904      301.380116\n",
      "FIRE:   44 17:06:59    -3858.351807      340.502819\n",
      "FIRE:   45 17:07:02    -3842.981445      396.251417\n",
      "FIRE:   46 17:07:05    -3821.436523      502.723700\n",
      "FIRE:   47 17:07:08    -3793.450684      586.110380\n",
      "FIRE:   48 17:07:11    -3759.887207      956.308359\n",
      "FIRE:   49 17:07:13    -3727.752441     1588.859232\n",
      "FIRE:   50 17:07:16    -3711.442139     1925.088938\n",
      "FIRE:   51 17:07:19    -3711.706787     1648.824348\n",
      "FIRE:   52 17:07:22    -3702.649902     1386.953966\n",
      "FIRE:   53 17:07:24    -3684.454102     1020.138225\n",
      "FIRE:   54 17:07:27    -3670.904785      517.599856\n",
      "FIRE:   55 17:07:30    -3669.657471      309.253038\n",
      "FIRE:   56 17:07:33    -3765.404053      574.851975\n",
      "FIRE:   57 17:07:36    -3813.308838      440.490652\n",
      "FIRE:   58 17:07:38    -3842.619141      361.215843\n",
      "FIRE:   59 17:07:41    -3863.940186      273.952722\n",
      "FIRE:   60 17:07:44    -3878.803223      193.553484\n",
      "FIRE:   61 17:07:47    -3888.850342      140.936001\n",
      "FIRE:   62 17:07:50    -3895.336182      104.836006\n",
      "FIRE:   63 17:07:52    -3899.138428       78.953520\n",
      "FIRE:   64 17:07:55    -3901.542725       77.488564\n",
      "FIRE:   65 17:07:58    -3904.385742       82.086813\n",
      "FIRE:   66 17:08:00    -3908.850830       70.553014\n",
      "FIRE:   67 17:08:03    -3914.221191       49.194109\n",
      "FIRE:   68 17:08:06    -3919.012939       29.325436\n",
      "FIRE:   69 17:08:08    -3922.588623       23.281906\n",
      "FIRE:   70 17:08:11    -3925.080811       20.032059\n",
      "FIRE:   71 17:08:14    -3926.768066       16.518999\n",
      "FIRE:   72 17:08:17    -3927.882568       12.852746\n",
      "FIRE:   73 17:08:20    -3928.469482       10.335544\n",
      "FIRE:   74 17:08:22    -3928.599365        9.332889\n",
      "FIRE:   75 17:08:25    -3928.724609        9.167030\n",
      "FIRE:   76 17:08:28    -3928.814941        8.529151\n",
      "FIRE:   77 17:08:31    -3928.836426       10.915139\n",
      "FIRE:   78 17:08:33    -3928.746338       14.831780\n",
      "FIRE:   79 17:08:36    -3928.484375       18.517469\n",
      "FIRE:   80 17:08:40    -3928.016846       21.030230\n",
      "FIRE:   81 17:08:43    -3927.243652       25.449792\n",
      "FIRE:   82 17:08:45    -3927.805908       20.546968\n",
      "FIRE:   83 17:08:48    -3924.704590       91.885834\n",
      "FIRE:   84 17:08:51    -3925.240479       37.381281\n",
      "FIRE:   85 17:08:53    -3924.317871      103.250449\n",
      "FIRE:   86 17:08:56    -3926.690918       25.060217\n",
      "FIRE:   87 17:08:58    -3927.760010       20.910164\n",
      "FIRE:   88 17:09:01    -3929.244385       13.178677\n",
      "FIRE:   89 17:09:04    -3926.256348       68.122006\n",
      "FIRE:   90 17:09:07    -3928.287598       51.507242\n",
      "FIRE:   91 17:09:09    -3929.408691       11.798633\n",
      "FIRE:   92 17:09:12    -3929.416260       11.508500\n",
      "FIRE:   93 17:09:15    -3929.430908       10.961849\n",
      "FIRE:   94 17:09:18    -3929.448242       10.245014\n",
      "FIRE:   95 17:09:21    -3929.466309        9.519655\n",
      "FIRE:   96 17:09:23    -3929.480469        8.998938\n",
      "FIRE:   97 17:09:26    -3929.491943        8.799966\n",
      "FIRE:   98 17:09:29    -3929.502441        8.772603\n",
      "FIRE:   99 17:09:32    -3929.516602        8.531683\n",
      "FIRE:  100 17:09:35    -3929.535400        7.654459\n",
      "FIRE:  101 17:09:38    -3929.561768        6.079305\n",
      "FIRE:  102 17:09:43    -3929.583984        4.588804\n",
      "FIRE:  103 17:09:47    -3929.601318        4.228545\n",
      "FIRE:  104 17:09:50    -3929.618164        3.950575\n",
      "FIRE:  105 17:09:53    -3929.642334        3.010937\n",
      "FIRE:  106 17:09:56    -3929.660645        2.936475\n",
      "FIRE:  107 17:09:59    -3929.670166        3.132457\n",
      "FIRE:  108 17:10:01    -3929.687256        2.737513\n",
      "FIRE:  109 17:10:04    -3929.696533        4.275096\n",
      "FIRE:  110 17:10:07    -3929.722900        3.975918\n",
      "FIRE:  111 17:10:10    -3929.753174        4.477517\n",
      "FIRE:  112 17:10:12    -3929.795166        2.871530\n",
      "FIRE:  113 17:10:15    -3929.830811        3.454543\n",
      "FIRE:  114 17:10:18    -3929.859131        5.203475\n",
      "FIRE:  115 17:10:21    -3929.858643        7.355797\n",
      "FIRE:  116 17:10:24    -3929.851562       12.314404\n",
      "FIRE:  117 17:10:26    -3929.653564       17.651734\n",
      "FIRE:  118 17:10:29    -3929.913330        8.136097\n",
      "FIRE:  119 17:10:32    -3929.787598       19.794863\n",
      "FIRE:  120 17:10:35    -3929.877197       13.463653\n",
      "FIRE:  121 17:10:37    -3929.952148        4.612311\n",
      "FIRE:  122 17:10:41    -3929.944824        5.699350\n",
      "FIRE:  123 17:10:43    -3929.946533        5.468556\n",
      "FIRE:  124 17:10:46    -3929.949707        5.023656\n",
      "FIRE:  125 17:10:49    -3929.954590        4.400645\n",
      "FIRE:  126 17:10:52    -3929.958496        3.680755\n",
      "FIRE:  127 17:10:55    -3929.962158        3.017054\n",
      "FIRE:  128 17:10:58    -3929.964111        2.653401\n",
      "FIRE:  129 17:11:00    -3929.963379        2.744190\n",
      "FIRE:  130 17:11:03    -3929.962402        3.147311\n",
      "FIRE:  131 17:11:06    -3929.962646        3.528941\n",
      "FIRE:  132 17:11:09    -3929.963135        3.578717\n",
      "FIRE:  133 17:11:12    -3929.967041        3.125430\n",
      "FIRE:  134 17:11:14    -3929.971436        2.251371\n",
      "FIRE:  135 17:11:17    -3929.972656        1.678895\n",
      "FIRE:  136 17:11:20    -3929.973633        2.139947\n",
      "FIRE:  137 17:11:23    -3929.976318        2.435382\n",
      "FIRE:  138 17:11:26    -3929.980225        1.755463\n",
      "FIRE:  139 17:11:29    -3929.984863        1.275445\n",
      "FIRE:  140 17:11:32    -3929.986572        2.151615\n",
      "FIRE:  141 17:11:35    -3929.991699        1.773658\n",
      "FIRE:  142 17:11:38    -3929.996582        1.586269\n",
      "FIRE:  143 17:11:41    -3929.998291        2.421468\n",
      "FIRE:  144 17:11:44    -3930.000244        2.687224\n",
      "FIRE:  145 17:11:48    -3929.995850        4.201209\n",
      "FIRE:  146 17:11:51    -3929.979248        5.347791\n",
      "FIRE:  147 17:11:53    -3929.939697        7.271101\n",
      "FIRE:  148 17:11:56    -3929.846680       10.057606\n",
      "FIRE:  149 17:11:59    -3929.654541       12.572006\n",
      "FIRE:  150 17:12:02    -3929.261719       16.004292\n",
      "FIRE:  151 17:12:04    -3928.466309       21.432649\n",
      "FIRE:  152 17:12:07    -3927.278320       32.265231\n",
      "FIRE:  153 17:12:10    -3925.835449       30.815247\n",
      "FIRE:  154 17:12:13    -3924.318115       22.785076\n",
      "FIRE:  155 17:12:16    -3922.793945       17.114645\n",
      "FIRE:  156 17:12:18    -3921.159912       17.818167\n",
      "FIRE:  157 17:12:21    -3919.212891       25.645882\n",
      "FIRE:  158 17:12:24    -3917.148682       31.123790\n",
      "FIRE:  159 17:12:28    -3915.171875       33.082435\n",
      "FIRE:  160 17:12:31    -3913.362061       36.928283\n",
      "FIRE:  161 17:12:34    -3911.759766       45.117682\n",
      "FIRE:  162 17:12:37    -3910.294678       56.450473\n",
      "FIRE:  163 17:12:40    -3908.872803       72.969750\n",
      "FIRE:  164 17:12:43    -3907.536865       89.329899\n",
      "FIRE:  165 17:12:46    -3906.500488      101.410228\n",
      "FIRE:  166 17:12:48    -3905.956299      105.930460\n",
      "FIRE:  167 17:12:51    -3905.898926      102.659576\n",
      "FIRE:  168 17:12:54    -3906.154053       94.187255\n",
      "FIRE:  169 17:12:56    -3906.534180       83.394110\n",
      "FIRE:  170 17:12:59    -3906.897705       71.855206\n",
      "FIRE:  171 17:13:02    -3907.111572       60.386353\n",
      "FIRE:  172 17:13:05    -3907.047607       49.930190\n",
      "FIRE:  173 17:13:07    -3906.634521       43.556178\n",
      "FIRE:  174 17:13:10    -3905.859863       40.616326\n",
      "FIRE:  175 17:13:13    -3904.733154       40.963435\n",
      "FIRE:  176 17:13:16    -3903.189453       44.884800\n",
      "FIRE:  177 17:13:19    -3901.116943       51.456437\n",
      "FIRE:  178 17:13:21    -3898.484619       60.300225\n",
      "FIRE:  179 17:13:24    -3895.409912       72.832589\n",
      "FIRE:  180 17:13:27    -3892.126465       90.461134\n",
      "FIRE:  181 17:13:30    -3888.804443      112.974514\n",
      "FIRE:  182 17:13:32    -3885.560791      140.788307\n",
      "FIRE:  183 17:13:35    -3882.547852      165.897316\n",
      "FIRE:  184 17:13:38    -3879.704346      175.721506\n",
      "FIRE:  185 17:13:41    -3876.662354      172.785361\n",
      "FIRE:  186 17:13:45    -3873.091797      173.986239\n",
      "FIRE:  187 17:13:48    -3868.986816      175.142600\n",
      "FIRE:  188 17:13:52    -3864.326660      185.045411\n",
      "FIRE:  189 17:13:54    -3859.037354      215.858365\n",
      "FIRE:  190 17:13:57    -3853.876465      233.427893\n",
      "FIRE:  191 17:14:00    -3850.472900      237.020033\n",
      "FIRE:  192 17:14:03    -3849.020508      248.638539\n",
      "FIRE:  193 17:14:06    -3848.622559      266.525747\n",
      "FIRE:  194 17:14:09    -3848.299316      285.159834\n",
      "FIRE:  195 17:14:13    -3848.248535      303.776109\n",
      "FIRE:  196 17:14:16    -3848.005615      316.618575\n",
      "FIRE:  197 17:14:20    -3845.328125      331.648485\n",
      "FIRE:  198 17:14:22    -3838.758301      357.744101\n",
      "FIRE:  199 17:14:25    -3825.914795      466.559491\n",
      "FIRE:  200 17:14:28    -3796.745850      871.275280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:14:29    -3940.666504        0.521325\n",
      "FIRE:    1 17:14:29    -3940.683350        0.490062\n",
      "FIRE:    2 17:14:30    -3940.716553        0.432640\n",
      "FIRE:    3 17:14:30    -3940.758545        0.361904\n",
      "FIRE:    4 17:14:31    -3940.799805        0.325827\n",
      "FIRE:    5 17:14:31    -3940.837402        0.274775\n",
      "FIRE:    6 17:14:32    -3940.869629        0.205748\n",
      "FIRE:    7 17:14:32    -3940.892090        0.172086\n",
      "FIRE:    8 17:14:33    -3940.905762        0.158780\n",
      "FIRE:    9 17:14:34    -3940.905762        0.183579\n",
      "FIRE:   10 17:14:34    -3940.906982        0.179350\n",
      "FIRE:   11 17:14:35    -3940.909912        0.171013\n",
      "FIRE:   12 17:14:35    -3940.912109        0.158816\n",
      "FIRE:   13 17:14:36    -3940.915771        0.143132\n",
      "FIRE:   14 17:14:36    -3940.916260        0.124401\n",
      "FIRE:   15 17:14:37    -3940.919922        0.103207\n",
      "FIRE:   16 17:14:37    -3940.922363        0.080160\n",
      "FIRE:   17 17:14:38    -3940.923584        0.053497\n",
      "FIRE:   18 17:14:39    -3940.925781        0.041347\n",
      "FIRE:   19 17:14:39    -3940.927734        0.050360\n",
      "FIRE:   20 17:14:40    -3940.927490        0.049843\n",
      "FIRE:   21 17:14:40    -3940.927979        0.048811\n",
      "FIRE:   22 17:14:41    -3940.928223        0.047279\n",
      "FIRE:   23 17:14:42    -3940.927246        0.045277\n",
      "FIRE:   24 17:14:42    -3940.927490        0.042840\n",
      "FIRE:   25 17:14:43    -3940.927246        0.040016\n",
      "FIRE:   26 17:14:43    -3940.927734        0.036853\n",
      "FIRE:   27 17:14:44    -3940.928711        0.033050\n",
      "FIRE:   28 17:14:44    -3940.927734        0.031113\n",
      "FIRE:   29 17:14:45    -3940.928223        0.030500\n",
      "FIRE:   30 17:14:45    -3940.927490        0.029711\n",
      "FIRE:   31 17:14:46    -3940.929688        0.028606\n",
      "FIRE:   32 17:14:46    -3940.929443        0.026984\n",
      "FIRE:   33 17:14:47    -3940.928711        0.024590\n",
      "FIRE:   34 17:14:47    -3940.927246        0.021167\n",
      "FIRE:   35 17:14:48    -3940.928223        0.016511\n",
      "FIRE:   36 17:14:48    -3940.930176        0.010552\n",
      "FIRE:   37 17:14:49    -3940.929688        0.007595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:14:50    -3941.140625        0.561665\n",
      "FIRE:    1 17:14:50    -3941.165039        0.548707\n",
      "FIRE:    2 17:14:51    -3941.209229        0.522561\n",
      "FIRE:    3 17:14:51    -3941.264160        0.482627\n",
      "FIRE:    4 17:14:52    -3941.325195        0.427797\n",
      "FIRE:    5 17:14:52    -3941.379150        0.356896\n",
      "FIRE:    6 17:14:53    -3941.423584        0.270329\n",
      "FIRE:    7 17:14:53    -3941.455566        0.192029\n",
      "FIRE:    8 17:14:54    -3941.471924        0.169328\n",
      "FIRE:    9 17:14:55    -3941.472168        0.175320\n",
      "FIRE:   10 17:14:55    -3941.473633        0.170408\n",
      "FIRE:   11 17:14:56    -3941.475586        0.160773\n",
      "FIRE:   12 17:14:56    -3941.479004        0.146812\n",
      "FIRE:   13 17:14:57    -3941.481689        0.129157\n",
      "FIRE:   14 17:14:57    -3941.486816        0.108618\n",
      "FIRE:   15 17:14:58    -3941.490479        0.092973\n",
      "FIRE:   16 17:14:58    -3941.492676        0.076744\n",
      "FIRE:   17 17:14:59    -3941.495850        0.059645\n",
      "FIRE:   18 17:14:59    -3941.497314        0.049295\n",
      "FIRE:   19 17:15:00    -3941.498291        0.045655\n",
      "FIRE:   20 17:15:00    -3941.499023        0.058115\n",
      "FIRE:   21 17:15:01    -3941.499512        0.057176\n",
      "FIRE:   22 17:15:01    -3941.499023        0.055320\n",
      "FIRE:   23 17:15:02    -3941.499023        0.052571\n",
      "FIRE:   24 17:15:03    -3941.499756        0.048996\n",
      "FIRE:   25 17:15:03    -3941.499023        0.044665\n",
      "FIRE:   26 17:15:04    -3941.501709        0.039679\n",
      "FIRE:   27 17:15:04    -3941.502930        0.034163\n",
      "FIRE:   28 17:15:05    -3941.503662        0.027915\n",
      "FIRE:   29 17:15:05    -3941.501953        0.025291\n",
      "FIRE:   30 17:15:06    -3941.503174        0.022704\n",
      "FIRE:   31 17:15:06    -3941.502441        0.022710\n",
      "FIRE:   32 17:15:07    -3941.503662        0.022988\n",
      "FIRE:   33 17:15:07    -3941.502686        0.022553\n",
      "FIRE:   34 17:15:08    -3941.503418        0.020805\n",
      "FIRE:   35 17:15:09    -3941.505615        0.021852\n",
      "FIRE:   36 17:15:09    -3941.506348        0.019955\n",
      "FIRE:   37 17:15:10    -3941.506104        0.017687\n",
      "FIRE:   38 17:15:10    -3941.506592        0.013480\n",
      "FIRE:   39 17:15:11    -3941.507812        0.010472\n",
      "FIRE:   40 17:15:11    -3941.508057        0.013021\n",
      "FIRE:   41 17:15:12    -3941.508545        0.011412\n",
      "FIRE:   42 17:15:12    -3941.509277        0.011148\n",
      "FIRE:   43 17:15:13    -3941.510010        0.009821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:15:17    -3940.270996        1.062647\n",
      "FIRE:    1 17:15:20    -3940.323486        0.942787\n",
      "FIRE:    2 17:15:23    -3940.404297        0.723110\n",
      "FIRE:    3 17:15:26    -3940.479004        0.458729\n",
      "FIRE:    4 17:15:29    -3940.523926        0.233730\n",
      "FIRE:    5 17:15:31    -3940.539062        0.392456\n",
      "FIRE:    6 17:15:34    -3940.537109        0.463092\n",
      "FIRE:    7 17:15:37    -3940.539795        0.446679\n",
      "FIRE:    8 17:15:40    -3940.543945        0.414449\n",
      "FIRE:    9 17:15:43    -3940.548340        0.367666\n",
      "FIRE:   10 17:15:46    -3940.556396        0.308205\n",
      "FIRE:   11 17:15:50    -3940.561768        0.238705\n",
      "FIRE:   12 17:15:53    -3940.567139        0.170400\n",
      "FIRE:   13 17:15:55    -3940.573730        0.165370\n",
      "FIRE:   14 17:15:58    -3940.575928        0.158902\n",
      "FIRE:   15 17:16:01    -3940.578125        0.150659\n",
      "FIRE:   16 17:16:03    -3940.578857        0.172545\n",
      "FIRE:   17 17:16:06    -3940.577881        0.208123\n",
      "FIRE:   18 17:16:09    -3940.578613        0.206751\n",
      "FIRE:   19 17:16:12    -3940.579102        0.184962\n",
      "FIRE:   20 17:16:14    -3940.584473        0.141975\n",
      "FIRE:   21 17:16:17    -3940.588867        0.093437\n",
      "FIRE:   22 17:16:19    -3940.591553        0.102905\n",
      "FIRE:   23 17:16:22    -3940.591064        0.132000\n",
      "FIRE:   24 17:16:24    -3940.591309        0.143367\n",
      "FIRE:   25 17:16:27    -3940.595215        0.133235\n",
      "FIRE:   26 17:16:29    -3940.599365        0.089062\n",
      "FIRE:   27 17:16:31    -3940.604248        0.091647\n",
      "FIRE:   28 17:16:34    -3940.607178        0.090590\n",
      "FIRE:   29 17:16:37    -3940.610596        0.091556\n",
      "FIRE:   30 17:16:40    -3940.613037        0.101264\n",
      "FIRE:   31 17:16:43    -3940.615234        0.100416\n",
      "FIRE:   32 17:16:45    -3940.616211        0.072833\n",
      "FIRE:   33 17:16:47    -3940.615967        0.066160\n",
      "FIRE:   34 17:16:49    -3940.619873        0.059275\n",
      "FIRE:   35 17:16:51    -3940.619141        0.057569\n",
      "FIRE:   36 17:16:52    -3940.620117        0.053176\n",
      "FIRE:   37 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   38 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   39 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   40 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   41 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   42 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   43 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   44 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   45 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   46 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   47 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   48 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   49 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   50 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   51 17:16:52    -3940.619873        0.044399\n",
      "FIRE:   52 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   53 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   54 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   55 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   56 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   57 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   58 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   59 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   60 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   61 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   62 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   63 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   64 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   65 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   66 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   67 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   68 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   69 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   70 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   71 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   72 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   73 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   74 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   75 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   76 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   77 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   78 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   79 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   80 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   81 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   82 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   83 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   84 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   85 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   86 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   87 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   88 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   89 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   90 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   91 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   92 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   93 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   94 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   95 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   96 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   97 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   98 17:16:53    -3940.619873        0.044399\n",
      "FIRE:   99 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  100 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  101 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  102 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  103 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  104 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  105 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  106 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  107 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  108 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  109 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  110 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  111 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  112 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  113 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  114 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  115 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  116 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  117 17:16:53    -3940.619873        0.044399\n",
      "FIRE:  118 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  119 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  120 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  121 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  122 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  123 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  124 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  125 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  126 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  127 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  128 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  129 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  130 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  131 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  132 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  133 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  134 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  135 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  136 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  137 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  138 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  139 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  140 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  141 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  142 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  143 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  144 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  145 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  146 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  147 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  148 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  149 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  150 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  151 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  152 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  153 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  154 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  155 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  156 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  157 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  158 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  159 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  160 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  161 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  162 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  163 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  164 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  165 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  166 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  167 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  168 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  169 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  170 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  171 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  172 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  173 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  174 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  175 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  176 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  177 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  178 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  179 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  180 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  181 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  182 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  183 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  184 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  185 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  186 17:16:54    -3940.619873        0.044399\n",
      "FIRE:  187 17:16:55    -3940.619873        0.044399\n",
      "FIRE:  188 17:16:55    -3940.619873        0.044399\n",
      "FIRE:  189 17:16:55    -3940.619873        0.044399\n",
      "FIRE:  190 17:16:55    -3940.619873        0.044399\n",
      "FIRE:  191 17:16:55    -3940.619873        0.044399\n",
      "FIRE:  192 17:16:55    -3940.619873        0.044399\n",
      "FIRE:  193 17:16:55    -3940.619873        0.044399\n",
      "FIRE:  194 17:16:55    -3940.619873        0.044399\n",
      "FIRE:  195 17:16:55    -3940.619873        0.044399\n",
      "FIRE:  196 17:16:55    -3940.619873        0.044399\n",
      "FIRE:  197 17:16:55    -3940.619873        0.044399\n",
      "FIRE:  198 17:16:55    -3940.619873        0.044399\n",
      "FIRE:  199 17:16:55    -3940.619873        0.044399\n",
      "FIRE:  200 17:16:55    -3940.619873        0.044399\n",
      "Progress: 16/42 calculations completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:16:55    -3940.666504        0.521325\n",
      "FIRE:    1 17:16:56    -3940.683350        0.490062\n",
      "FIRE:    2 17:16:57    -3940.716553        0.432640\n",
      "FIRE:    3 17:16:57    -3940.758301        0.361904\n",
      "FIRE:    4 17:16:58    -3940.799805        0.325827\n",
      "FIRE:    5 17:16:58    -3940.837402        0.274775\n",
      "FIRE:    6 17:16:59    -3940.869141        0.205748\n",
      "FIRE:    7 17:16:59    -3940.892334        0.172088\n",
      "FIRE:    8 17:17:00    -3940.906006        0.158782\n",
      "FIRE:    9 17:17:01    -3940.905518        0.183581\n",
      "FIRE:   10 17:17:01    -3940.906982        0.179350\n",
      "FIRE:   11 17:17:02    -3940.909912        0.171013\n",
      "FIRE:   12 17:17:02    -3940.911865        0.158815\n",
      "FIRE:   13 17:17:03    -3940.915283        0.143133\n",
      "FIRE:   14 17:17:03    -3940.916260        0.124402\n",
      "FIRE:   15 17:17:04    -3940.919922        0.103202\n",
      "FIRE:   16 17:17:04    -3940.922119        0.080161\n",
      "FIRE:   17 17:17:05    -3940.923828        0.053500\n",
      "FIRE:   18 17:17:05    -3940.925781        0.041349\n",
      "FIRE:   19 17:17:06    -3940.927490        0.050360\n",
      "FIRE:   20 17:17:06    -3940.927246        0.049846\n",
      "FIRE:   21 17:17:07    -3940.927979        0.048809\n",
      "FIRE:   22 17:17:07    -3940.928467        0.047279\n",
      "FIRE:   23 17:17:08    -3940.927490        0.045278\n",
      "FIRE:   24 17:17:09    -3940.927490        0.042841\n",
      "FIRE:   25 17:17:09    -3940.927246        0.040014\n",
      "FIRE:   26 17:17:10    -3940.927490        0.036857\n",
      "FIRE:   27 17:17:10    -3940.928711        0.033059\n",
      "FIRE:   28 17:17:11    -3940.927734        0.031114\n",
      "FIRE:   29 17:17:11    -3940.928467        0.030500\n",
      "FIRE:   30 17:17:12    -3940.927490        0.029710\n",
      "FIRE:   31 17:17:12    -3940.929688        0.028607\n",
      "FIRE:   32 17:17:13    -3940.929443        0.026984\n",
      "FIRE:   33 17:17:13    -3940.928955        0.024589\n",
      "FIRE:   34 17:17:14    -3940.927246        0.021166\n",
      "FIRE:   35 17:17:15    -3940.927979        0.016505\n",
      "FIRE:   36 17:17:15    -3940.930176        0.010554\n",
      "FIRE:   37 17:17:16    -3940.929199        0.007598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:17:16    -3940.598877        0.391174\n",
      "FIRE:    1 17:17:17    -3940.614746        0.383311\n",
      "FIRE:    2 17:17:18    -3940.643555        0.367268\n",
      "FIRE:    3 17:17:18    -3940.678467        0.342233\n",
      "FIRE:    4 17:17:19    -3940.716309        0.306779\n",
      "FIRE:    5 17:17:19    -3940.751709        0.259261\n",
      "FIRE:    6 17:17:20    -3940.781738        0.199286\n",
      "FIRE:    7 17:17:20    -3940.804199        0.179784\n",
      "FIRE:    8 17:17:21    -3940.816406        0.151781\n",
      "FIRE:    9 17:17:21    -3940.822021        0.113049\n",
      "FIRE:   10 17:17:22    -3940.823730        0.111031\n",
      "FIRE:   11 17:17:22    -3940.823730        0.107348\n",
      "FIRE:   12 17:17:23    -3940.826904        0.102398\n",
      "FIRE:   13 17:17:23    -3940.829346        0.096246\n",
      "FIRE:   14 17:17:24    -3940.832031        0.089259\n",
      "FIRE:   15 17:17:25    -3940.833740        0.081867\n",
      "FIRE:   16 17:17:25    -3940.836426        0.074521\n",
      "FIRE:   17 17:17:26    -3940.837158        0.066978\n",
      "FIRE:   18 17:17:26    -3940.839600        0.059770\n",
      "FIRE:   19 17:17:27    -3940.841797        0.053183\n",
      "FIRE:   20 17:17:27    -3940.840332        0.046874\n",
      "FIRE:   21 17:17:28    -3940.841553        0.046489\n",
      "FIRE:   22 17:17:28    -3940.843506        0.054278\n",
      "FIRE:   23 17:17:29    -3940.842285        0.056368\n",
      "FIRE:   24 17:17:29    -3940.845459        0.052072\n",
      "FIRE:   25 17:17:30    -3940.845459        0.041311\n",
      "FIRE:   26 17:17:31    -3940.845459        0.040094\n",
      "FIRE:   27 17:17:31    -3940.846680        0.034482\n",
      "FIRE:   28 17:17:32    -3940.846924        0.032552\n",
      "FIRE:   29 17:17:32    -3940.846436        0.028862\n",
      "FIRE:   30 17:17:33    -3940.847168        0.023753\n",
      "FIRE:   31 17:17:33    -3940.846924        0.019680\n",
      "FIRE:   32 17:17:34    -3940.846924        0.018318\n",
      "FIRE:   33 17:17:34    -3940.848389        0.016802\n",
      "FIRE:   34 17:17:35    -3940.847900        0.015011\n",
      "FIRE:   35 17:17:37    -3940.847656        0.012637\n",
      "FIRE:   36 17:17:38    -3940.847900        0.009563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:17:44    -3786.025635      358.973793\n",
      "FIRE:    1 17:17:47    -3823.912598      415.273604\n",
      "FIRE:    2 17:17:51    -3879.757812      294.163964\n",
      "FIRE:    3 17:17:56    -3910.035645      101.243524\n",
      "FIRE:    4 17:18:01    -3914.506836       54.338704\n",
      "FIRE:    5 17:18:05    -3919.121582       49.495971\n",
      "FIRE:    6 17:18:08    -3922.916016       49.869064\n",
      "FIRE:    7 17:18:10    -3923.271729       43.782964\n",
      "FIRE:    8 17:18:13    -3923.943848       33.704464\n",
      "FIRE:    9 17:18:16    -3924.714844       23.430332\n",
      "FIRE:   10 17:18:19    -3925.119385       14.438782\n",
      "FIRE:   11 17:18:21    -3924.530762       26.837036\n",
      "FIRE:   12 17:18:24    -3923.818604       30.246100\n",
      "FIRE:   13 17:18:27    -3922.462891       37.220522\n",
      "FIRE:   14 17:18:30    -3920.491699       48.642855\n",
      "FIRE:   15 17:18:32    -3917.868896       65.326405\n",
      "FIRE:   16 17:18:35    -3914.784912       82.863044\n",
      "FIRE:   17 17:18:39    -3911.703125      101.268885\n",
      "FIRE:   18 17:18:44    -3908.897461      121.545305\n",
      "FIRE:   19 17:18:48    -3906.473877      139.373155\n",
      "FIRE:   20 17:18:52    -3904.585449      149.810862\n",
      "FIRE:   21 17:18:54    -3903.396484      151.296337\n",
      "FIRE:   22 17:18:57    -3902.870361      148.408919\n",
      "FIRE:   23 17:19:00    -3902.827148      147.028198\n",
      "FIRE:   24 17:19:02    -3903.115723      147.700932\n",
      "FIRE:   25 17:19:05    -3903.667969      147.019381\n",
      "FIRE:   26 17:19:08    -3904.442139      142.327426\n",
      "FIRE:   27 17:19:10    -3905.347900      134.049502\n",
      "FIRE:   28 17:19:13    -3906.290283      125.158022\n",
      "FIRE:   29 17:19:16    -3907.228516      117.703109\n",
      "FIRE:   30 17:19:18    -3908.125488      111.661748\n",
      "FIRE:   31 17:19:21    -3908.896729      109.665751\n",
      "FIRE:   32 17:19:24    -3909.415771      109.636088\n",
      "FIRE:   33 17:19:27    -3909.511963      111.713424\n",
      "FIRE:   34 17:19:29    -3908.971436      116.795803\n",
      "FIRE:   35 17:19:32    -3907.580811      125.578369\n",
      "FIRE:   36 17:19:35    -3905.199463      138.840164\n",
      "FIRE:   37 17:19:37    -3901.792480      157.732214\n",
      "FIRE:   38 17:19:40    -3897.368896      181.998260\n",
      "FIRE:   39 17:19:43    -3892.052002      209.070378\n",
      "FIRE:   40 17:19:45    -3886.011475      238.042586\n",
      "FIRE:   41 17:19:48    -3879.168213      268.914489\n",
      "FIRE:   42 17:19:50    -3871.117676      302.482465\n",
      "FIRE:   43 17:19:53    -3860.894043      343.234666\n",
      "FIRE:   44 17:19:56    -3846.108643      420.612634\n",
      "FIRE:   45 17:19:58    -3823.204346      579.444005\n",
      "FIRE:   46 17:20:01    -3792.970947      749.037143\n",
      "FIRE:   47 17:20:03    -3760.117188     1141.337088\n",
      "FIRE:   48 17:20:06    -3737.564453     1230.869283\n",
      "FIRE:   49 17:20:09    -3734.776855      994.759773\n",
      "FIRE:   50 17:20:11    -3733.122070      892.807339\n",
      "FIRE:   51 17:20:14    -3726.386475      963.851694\n",
      "FIRE:   52 17:20:17    -3713.986328      819.987846\n",
      "FIRE:   53 17:20:20    -3703.358643      542.939808\n",
      "FIRE:   54 17:20:23    -3698.296875      313.277411\n",
      "FIRE:   55 17:20:26    -3702.007568      293.253969\n",
      "FIRE:   56 17:20:28    -3786.847412      515.553252\n",
      "FIRE:   57 17:20:31    -3827.773926      433.127456\n",
      "FIRE:   58 17:20:34    -3852.474365      348.375845\n",
      "FIRE:   59 17:20:37    -3869.537109      267.228914\n",
      "FIRE:   60 17:20:40    -3881.877930      202.776091\n",
      "FIRE:   61 17:20:43    -3891.198486      157.190731\n",
      "FIRE:   62 17:20:45    -3898.406982      123.733727\n",
      "FIRE:   63 17:20:48    -3903.943359       98.516043\n",
      "FIRE:   64 17:20:51    -3908.106689       78.968899\n",
      "FIRE:   65 17:20:53    -3911.221436       63.236200\n",
      "FIRE:   66 17:20:56    -3913.727295       50.209713\n",
      "FIRE:   67 17:20:59    -3916.187500       41.382461\n",
      "FIRE:   68 17:21:01    -3918.974854       35.241137\n",
      "FIRE:   69 17:21:04    -3921.950195       26.376324\n",
      "FIRE:   70 17:21:07    -3924.674561       20.420917\n",
      "FIRE:   71 17:21:09    -3926.836670       16.696344\n",
      "FIRE:   72 17:21:12    -3927.261475       15.050441\n",
      "FIRE:   73 17:21:15    -3927.434082       10.921841\n",
      "FIRE:   74 17:21:17    -3927.570801        8.076091\n",
      "FIRE:   75 17:21:20    -3927.669678        6.670879\n",
      "FIRE:   76 17:21:22    -3927.729248        5.914978\n",
      "FIRE:   77 17:21:25    -3927.781738        5.685130\n",
      "FIRE:   78 17:21:28    -3927.816162        6.685497\n",
      "FIRE:   79 17:21:30    -3927.786377       11.515876\n",
      "FIRE:   80 17:21:33    -3927.570801       21.086595\n",
      "FIRE:   81 17:21:36    -3927.024414       34.324909\n",
      "FIRE:   82 17:21:38    -3926.111816       44.870249\n",
      "FIRE:   83 17:21:41    -3924.931396       46.221529\n",
      "FIRE:   84 17:21:44    -3923.649902       38.655722\n",
      "FIRE:   85 17:21:48    -3922.383057       28.792917\n",
      "FIRE:   86 17:21:52    -3921.148926       24.191765\n",
      "FIRE:   87 17:21:55    -3919.877441       21.121991\n",
      "FIRE:   88 17:21:57    -3918.537842       24.751285\n",
      "FIRE:   89 17:22:00    -3917.331055       28.271682\n",
      "FIRE:   90 17:22:02    -3916.452881       28.256693\n",
      "FIRE:   91 17:22:05    -3915.856689       25.076178\n",
      "FIRE:   92 17:22:08    -3915.403076       21.254709\n",
      "FIRE:   93 17:22:10    -3915.060303       18.577319\n",
      "FIRE:   94 17:22:13    -3914.874023       17.977935\n",
      "FIRE:   95 17:22:16    -3914.873779       19.637846\n",
      "FIRE:   96 17:22:18    -3915.059082       23.508635\n",
      "FIRE:   97 17:22:21    -3915.373291       29.153366\n",
      "FIRE:   98 17:22:24    -3915.724854       35.882301\n",
      "FIRE:   99 17:22:27    -3915.996826       42.705640\n",
      "FIRE:  100 17:22:29    -3916.078857       48.200330\n",
      "FIRE:  101 17:22:32    -3915.861572       51.163052\n",
      "FIRE:  102 17:22:35    -3915.258545       54.739042\n",
      "FIRE:  103 17:22:37    -3914.229492       56.867534\n",
      "FIRE:  104 17:22:40    -3912.737549       59.162611\n",
      "FIRE:  105 17:22:43    -3910.657471       63.371419\n",
      "FIRE:  106 17:22:45    -3907.758057       70.582641\n",
      "FIRE:  107 17:22:48    -3903.873291       81.940492\n",
      "FIRE:  108 17:22:51    -3899.014160       99.233387\n",
      "FIRE:  109 17:22:53    -3893.388672      123.862466\n",
      "FIRE:  110 17:22:56    -3887.071045      157.296013\n",
      "FIRE:  111 17:22:59    -3879.700928      198.432572\n",
      "FIRE:  112 17:23:01    -3870.865967      243.456411\n",
      "FIRE:  113 17:23:04    -3860.010254      292.348172\n",
      "FIRE:  114 17:23:07    -3845.858154      355.800331\n",
      "FIRE:  115 17:23:09    -3826.235840      475.592442\n",
      "FIRE:  116 17:23:12    -3797.104248      724.872565\n",
      "FIRE:  117 17:23:14    -3759.820801     1087.767780\n",
      "FIRE:  118 17:23:17    -3743.044189     1324.724292\n",
      "FIRE:  119 17:23:20    -3743.241455     1021.523611\n",
      "FIRE:  120 17:23:22    -3741.917969      755.672232\n",
      "FIRE:  121 17:23:25    -3731.490723      675.634896\n",
      "FIRE:  122 17:23:28    -3715.121826      791.941068\n",
      "FIRE:  123 17:23:30    -3696.137939      747.963473\n",
      "FIRE:  124 17:23:33    -3681.423340      566.766572\n",
      "FIRE:  125 17:23:35    -3672.780029      338.627175\n",
      "FIRE:  126 17:23:38    -3671.581299      291.362110\n",
      "FIRE:  127 17:23:41    -3769.172119      534.055218\n",
      "FIRE:  128 17:23:43    -3820.715820      420.865730\n",
      "FIRE:  129 17:23:46    -3851.042969      307.857072\n",
      "FIRE:  130 17:23:49    -3870.695068      217.611991\n",
      "FIRE:  131 17:23:51    -3884.181396      155.280358\n",
      "FIRE:  132 17:23:54    -3893.869385      113.127251\n",
      "FIRE:  133 17:23:57    -3901.060791       84.076936\n",
      "FIRE:  134 17:24:00    -3906.481689       62.939561\n",
      "FIRE:  135 17:24:02    -3910.577393       47.434072\n",
      "FIRE:  136 17:24:05    -3913.746338       36.469626\n",
      "FIRE:  137 17:24:08    -3916.354492       28.511040\n",
      "FIRE:  138 17:24:11    -3918.645996       22.017577\n",
      "FIRE:  139 17:24:14    -3920.744385       16.568734\n",
      "FIRE:  140 17:24:16    -3922.704834       13.745937\n",
      "FIRE:  141 17:24:19    -3924.520508       10.561016\n",
      "FIRE:  142 17:24:21    -3926.111328        7.663429\n",
      "FIRE:  143 17:24:24    -3927.414307        6.024413\n",
      "FIRE:  144 17:24:27    -3928.427246        4.640155\n",
      "FIRE:  145 17:24:29    -3929.208984        3.910162\n",
      "FIRE:  146 17:24:32    -3929.839600        4.098083\n",
      "FIRE:  147 17:24:35    -3930.379395        4.005206\n",
      "FIRE:  148 17:24:38    -3930.846924        3.625455\n",
      "FIRE:  149 17:24:42    -3931.230713        3.316899\n",
      "FIRE:  150 17:24:45    -3931.495361        2.998156\n",
      "FIRE:  151 17:24:47    -3931.611084        3.048653\n",
      "FIRE:  152 17:24:50    -3931.559082        3.535693\n",
      "FIRE:  153 17:24:53    -3931.352539        5.094278\n",
      "FIRE:  154 17:24:56    -3931.050293        6.689645\n",
      "FIRE:  155 17:24:59    -3930.740967        7.873318\n",
      "FIRE:  156 17:25:01    -3930.501709        8.242650\n",
      "FIRE:  157 17:25:04    -3930.351318        7.737503\n",
      "FIRE:  158 17:25:07    -3930.245605        6.652673\n",
      "FIRE:  159 17:25:09    -3930.100098        6.731065\n",
      "FIRE:  160 17:25:12    -3929.845215        7.809877\n",
      "FIRE:  161 17:25:15    -3929.429688        9.343744\n",
      "FIRE:  162 17:25:18    -3928.822998       11.387651\n",
      "FIRE:  163 17:25:20    -3928.010254       13.970016\n",
      "FIRE:  164 17:25:23    -3926.971191       17.079776\n",
      "FIRE:  165 17:25:26    -3925.672363       20.611202\n",
      "FIRE:  166 17:25:28    -3924.085449       24.383361\n",
      "FIRE:  167 17:25:31    -3922.267090       27.977186\n",
      "FIRE:  168 17:25:33    -3920.446777       30.439884\n",
      "FIRE:  169 17:25:36    -3919.007812       30.600362\n",
      "FIRE:  170 17:25:39    -3918.286621       30.036173\n",
      "FIRE:  171 17:25:44    -3918.360352       29.718585\n",
      "FIRE:  172 17:25:47    -3919.053223       27.990535\n",
      "FIRE:  173 17:25:49    -3920.101562       24.593253\n",
      "FIRE:  174 17:25:52    -3921.288574       19.745823\n",
      "FIRE:  175 17:25:55    -3922.451904       14.176742\n",
      "FIRE:  176 17:25:57    -3923.437744        9.959961\n",
      "FIRE:  177 17:26:00    -3924.111816        7.973385\n",
      "FIRE:  178 17:26:03    -3924.420410        7.836984\n",
      "FIRE:  179 17:26:05    -3924.366943       11.389969\n",
      "FIRE:  180 17:26:09    -3924.018311       15.844565\n",
      "FIRE:  181 17:26:15    -3926.774902        4.762363\n",
      "FIRE:  182 17:26:19    -3928.142334        4.296672\n",
      "FIRE:  183 17:26:22    -3929.121826        3.307309\n",
      "FIRE:  184 17:26:26    -3929.919189        1.929977\n",
      "FIRE:  185 17:26:28    -3930.578613        2.043961\n",
      "FIRE:  186 17:26:31    -3931.159424        2.341758\n",
      "FIRE:  187 17:26:34    -3931.718750        2.309827\n",
      "FIRE:  188 17:26:36    -3932.261475        2.030370\n",
      "FIRE:  189 17:26:39    -3932.756104        1.611108\n",
      "FIRE:  190 17:26:42    -3933.159912        1.136106\n",
      "FIRE:  191 17:26:45    -3933.461182        1.111440\n",
      "FIRE:  192 17:26:48    -3933.661133        1.276222\n",
      "FIRE:  193 17:26:51    -3933.776855        1.244671\n",
      "FIRE:  194 17:26:56    -3933.835938        1.047120\n",
      "FIRE:  195 17:26:59    -3933.856201        1.142538\n",
      "FIRE:  196 17:27:02    -3934.226318        0.559536\n",
      "FIRE:  197 17:27:04    -3934.334473        0.741813\n",
      "FIRE:  198 17:27:07    -3934.340332        0.631992\n",
      "FIRE:  199 17:27:10    -3934.312988        0.638810\n",
      "FIRE:  200 17:27:12    -3934.287109        0.907245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:27:13    -3940.666504        0.521325\n",
      "FIRE:    1 17:27:14    -3940.683350        0.490062\n",
      "FIRE:    2 17:27:14    -3940.716553        0.432640\n",
      "FIRE:    3 17:27:15    -3940.758301        0.361904\n",
      "FIRE:    4 17:27:15    -3940.799805        0.325827\n",
      "FIRE:    5 17:27:16    -3940.837646        0.274776\n",
      "FIRE:    6 17:27:16    -3940.869385        0.205748\n",
      "FIRE:    7 17:27:17    -3940.892578        0.172088\n",
      "FIRE:    8 17:27:17    -3940.906006        0.158781\n",
      "FIRE:    9 17:27:18    -3940.905518        0.183580\n",
      "FIRE:   10 17:27:19    -3940.906982        0.179352\n",
      "FIRE:   11 17:27:19    -3940.909912        0.171013\n",
      "FIRE:   12 17:27:20    -3940.911621        0.158816\n",
      "FIRE:   13 17:27:20    -3940.915527        0.143132\n",
      "FIRE:   14 17:27:21    -3940.916260        0.124401\n",
      "FIRE:   15 17:27:21    -3940.919922        0.103202\n",
      "FIRE:   16 17:27:22    -3940.922363        0.080162\n",
      "FIRE:   17 17:27:23    -3940.923584        0.053499\n",
      "FIRE:   18 17:27:23    -3940.925781        0.041350\n",
      "FIRE:   19 17:27:24    -3940.927734        0.050360\n",
      "FIRE:   20 17:27:24    -3940.927002        0.049846\n",
      "FIRE:   21 17:27:25    -3940.928223        0.048809\n",
      "FIRE:   22 17:27:25    -3940.927734        0.047279\n",
      "FIRE:   23 17:27:26    -3940.927246        0.045278\n",
      "FIRE:   24 17:27:26    -3940.927490        0.042841\n",
      "FIRE:   25 17:27:27    -3940.927246        0.040017\n",
      "FIRE:   26 17:27:27    -3940.927734        0.036856\n",
      "FIRE:   27 17:27:28    -3940.928711        0.033059\n",
      "FIRE:   28 17:27:29    -3940.927246        0.031113\n",
      "FIRE:   29 17:27:29    -3940.928223        0.030499\n",
      "FIRE:   30 17:27:30    -3940.927490        0.029709\n",
      "FIRE:   31 17:27:30    -3940.929443        0.028606\n",
      "FIRE:   32 17:27:31    -3940.929199        0.026984\n",
      "FIRE:   33 17:27:31    -3940.928711        0.024590\n",
      "FIRE:   34 17:27:32    -3940.927246        0.021168\n",
      "FIRE:   35 17:27:33    -3940.927734        0.016507\n",
      "FIRE:   36 17:27:33    -3940.929932        0.010556\n",
      "FIRE:   37 17:27:34    -3940.929443        0.007597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:27:34    -3941.171387        0.966597\n",
      "FIRE:    1 17:27:35    -3941.211670        0.862748\n",
      "FIRE:    2 17:27:36    -3941.281250        0.677228\n",
      "FIRE:    3 17:27:36    -3941.365234        0.608050\n",
      "FIRE:    4 17:27:37    -3941.450439        0.517981\n",
      "FIRE:    5 17:27:37    -3941.527100        0.422880\n",
      "FIRE:    6 17:27:38    -3941.586914        0.327157\n",
      "FIRE:    7 17:27:39    -3941.626709        0.366640\n",
      "FIRE:    8 17:27:40    -3941.645020        0.413035\n",
      "FIRE:    9 17:27:41    -3941.646729        0.401208\n",
      "FIRE:   10 17:27:42    -3941.649170        0.377864\n",
      "FIRE:   11 17:27:42    -3941.654297        0.343677\n",
      "FIRE:   12 17:27:43    -3941.659424        0.299670\n",
      "FIRE:   13 17:27:43    -3941.663818        0.247309\n",
      "FIRE:   14 17:27:44    -3941.667725        0.188533\n",
      "FIRE:   15 17:27:44    -3941.670654        0.126123\n",
      "FIRE:   16 17:27:45    -3941.675781        0.066161\n",
      "FIRE:   17 17:27:45    -3941.677490        0.062654\n",
      "FIRE:   18 17:27:46    -3941.678955        0.092739\n",
      "FIRE:   19 17:27:47    -3941.677979        0.090955\n",
      "FIRE:   20 17:27:47    -3941.677490        0.087431\n",
      "FIRE:   21 17:27:48    -3941.678955        0.082215\n",
      "FIRE:   22 17:27:48    -3941.679932        0.075438\n",
      "FIRE:   23 17:27:49    -3941.679443        0.067230\n",
      "FIRE:   24 17:27:49    -3941.680176        0.059120\n",
      "FIRE:   25 17:27:50    -3941.680176        0.058063\n",
      "FIRE:   26 17:27:50    -3941.682373        0.056725\n",
      "FIRE:   27 17:27:51    -3941.681641        0.055046\n",
      "FIRE:   28 17:27:52    -3941.681885        0.052955\n",
      "FIRE:   29 17:27:52    -3941.683105        0.050374\n",
      "FIRE:   30 17:27:53    -3941.683838        0.047213\n",
      "FIRE:   31 17:27:53    -3941.683350        0.044692\n",
      "FIRE:   32 17:27:54    -3941.683594        0.054284\n",
      "FIRE:   33 17:27:54    -3941.684326        0.057198\n",
      "FIRE:   34 17:27:55    -3941.684082        0.051672\n",
      "FIRE:   35 17:27:55    -3941.686279        0.036913\n",
      "FIRE:   36 17:27:56    -3941.686279        0.016778\n",
      "FIRE:   37 17:27:56    -3941.688965        0.017846\n",
      "FIRE:   38 17:27:57    -3941.689453        0.034821\n",
      "FIRE:   39 17:27:58    -3941.689209        0.040811\n",
      "FIRE:   40 17:27:58    -3941.689209        0.029666\n",
      "FIRE:   41 17:27:59    -3941.692383        0.013015\n",
      "FIRE:   42 17:27:59    -3941.691895        0.022029\n",
      "FIRE:   43 17:28:00    -3941.692139        0.023731\n",
      "FIRE:   44 17:28:00    -3941.692383        0.013933\n",
      "FIRE:   45 17:28:01    -3941.692871        0.020878\n",
      "FIRE:   46 17:28:01    -3941.693604        0.017722\n",
      "FIRE:   47 17:28:02    -3941.694824        0.016523\n",
      "FIRE:   48 17:28:02    -3941.694824        0.019419\n",
      "FIRE:   49 17:28:03    -3941.694824        0.012157\n",
      "FIRE:   50 17:28:03    -3941.695557        0.007364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:28:08    -3940.116455        1.435462\n",
      "FIRE:    1 17:28:11    -3940.189941        1.219754\n",
      "FIRE:    2 17:28:13    -3940.296631        0.811115\n",
      "FIRE:    3 17:28:16    -3940.381104        0.405984\n",
      "FIRE:    4 17:28:19    -3940.426025        0.356761\n",
      "FIRE:    5 17:28:22    -3940.431641        0.482950\n",
      "FIRE:    6 17:28:25    -3940.417969        0.618108\n",
      "FIRE:    7 17:28:27    -3940.421875        0.597055\n",
      "FIRE:    8 17:28:31    -3940.428711        0.555630\n",
      "FIRE:    9 17:28:34    -3940.437012        0.500120\n",
      "FIRE:   10 17:28:37    -3940.445557        0.429275\n",
      "FIRE:   11 17:28:40    -3940.453369        0.345488\n",
      "FIRE:   12 17:28:43    -3940.461426        0.252151\n",
      "FIRE:   13 17:28:46    -3940.466309        0.210672\n",
      "FIRE:   14 17:28:48    -3940.470215        0.188754\n",
      "FIRE:   15 17:28:53    -3940.468994        0.171130\n",
      "FIRE:   16 17:28:57    -3940.467041        0.224386\n",
      "FIRE:   17 17:29:00    -3940.464844        0.292091\n",
      "FIRE:   18 17:29:03    -3940.465088        0.313274\n",
      "FIRE:   19 17:29:06    -3940.467285        0.279185\n",
      "FIRE:   20 17:29:09    -3940.474121        0.205017\n",
      "FIRE:   21 17:29:12    -3940.480225        0.113639\n",
      "FIRE:   22 17:29:16    -3940.483643        0.098958\n",
      "FIRE:   23 17:29:19    -3940.487061        0.198425\n",
      "FIRE:   24 17:29:22    -3940.487549        0.186701\n",
      "FIRE:   25 17:29:24    -3940.489014        0.163981\n",
      "FIRE:   26 17:29:26    -3940.489502        0.131594\n",
      "FIRE:   27 17:29:29    -3940.492920        0.091921\n",
      "FIRE:   28 17:29:32    -3940.494141        0.082745\n",
      "FIRE:   29 17:29:34    -3940.494629        0.083964\n",
      "FIRE:   30 17:29:36    -3940.495361        0.084537\n",
      "FIRE:   31 17:29:38    -3940.495361        0.083288\n",
      "FIRE:   32 17:29:41    -3940.496338        0.097360\n",
      "FIRE:   33 17:29:44    -3940.495850        0.101277\n",
      "FIRE:   34 17:29:46    -3940.498779        0.084281\n",
      "FIRE:   35 17:29:49    -3940.500000        0.082023\n",
      "FIRE:   36 17:29:50    -3940.503418        0.058368\n",
      "FIRE:   37 17:29:51    -3940.503418        0.040899\n",
      "FIRE:   38 17:29:51    -3940.503418        0.040899\n",
      "FIRE:   39 17:29:51    -3940.503418        0.040899\n",
      "FIRE:   40 17:29:51    -3940.503418        0.040899\n",
      "FIRE:   41 17:29:51    -3940.503418        0.040899\n",
      "FIRE:   42 17:29:51    -3940.503418        0.040899\n",
      "FIRE:   43 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   44 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   45 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   46 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   47 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   48 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   49 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   50 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   51 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   52 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   53 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   54 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   55 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   56 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   57 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   58 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   59 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   60 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   61 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   62 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   63 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   64 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   65 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   66 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   67 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   68 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   69 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   70 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   71 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   72 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   73 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   74 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   75 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   76 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   77 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   78 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   79 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   80 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   81 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   82 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   83 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   84 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   85 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   86 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   87 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   88 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   89 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   90 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   91 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   92 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   93 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   94 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   95 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   96 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   97 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   98 17:29:52    -3940.503418        0.040899\n",
      "FIRE:   99 17:29:52    -3940.503418        0.040899\n",
      "FIRE:  100 17:29:52    -3940.503418        0.040899\n",
      "FIRE:  101 17:29:52    -3940.503418        0.040899\n",
      "FIRE:  102 17:29:52    -3940.503418        0.040899\n",
      "FIRE:  103 17:29:52    -3940.503418        0.040899\n",
      "FIRE:  104 17:29:52    -3940.503418        0.040899\n",
      "FIRE:  105 17:29:52    -3940.503418        0.040899\n",
      "FIRE:  106 17:29:52    -3940.503418        0.040899\n",
      "FIRE:  107 17:29:52    -3940.503418        0.040899\n",
      "FIRE:  108 17:29:52    -3940.503418        0.040899\n",
      "FIRE:  109 17:29:52    -3940.503418        0.040899\n",
      "FIRE:  110 17:29:52    -3940.503418        0.040899\n",
      "FIRE:  111 17:29:52    -3940.503418        0.040899\n",
      "FIRE:  112 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  113 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  114 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  115 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  116 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  117 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  118 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  119 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  120 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  121 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  122 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  123 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  124 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  125 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  126 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  127 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  128 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  129 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  130 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  131 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  132 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  133 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  134 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  135 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  136 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  137 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  138 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  139 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  140 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  141 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  142 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  143 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  144 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  145 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  146 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  147 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  148 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  149 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  150 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  151 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  152 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  153 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  154 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  155 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  156 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  157 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  158 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  159 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  160 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  161 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  162 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  163 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  164 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  165 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  166 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  167 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  168 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  169 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  170 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  171 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  172 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  173 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  174 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  175 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  176 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  177 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  178 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  179 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  180 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  181 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  182 17:29:53    -3940.503418        0.040899\n",
      "FIRE:  183 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  184 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  185 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  186 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  187 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  188 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  189 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  190 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  191 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  192 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  193 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  194 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  195 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  196 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  197 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  198 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  199 17:29:54    -3940.503418        0.040899\n",
      "FIRE:  200 17:29:54    -3940.503418        0.040899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:29:54    -3940.666504        0.521325\n",
      "FIRE:    1 17:29:55    -3940.683350        0.490062\n",
      "FIRE:    2 17:29:56    -3940.716553        0.432639\n",
      "FIRE:    3 17:29:56    -3940.758545        0.361904\n",
      "FIRE:    4 17:29:57    -3940.799805        0.325827\n",
      "FIRE:    5 17:29:57    -3940.837402        0.274776\n",
      "FIRE:    6 17:29:58    -3940.869385        0.205748\n",
      "FIRE:    7 17:29:58    -3940.892578        0.172089\n",
      "FIRE:    8 17:29:59    -3940.906006        0.158781\n",
      "FIRE:    9 17:30:00    -3940.905518        0.183582\n",
      "FIRE:   10 17:30:00    -3940.906982        0.179349\n",
      "FIRE:   11 17:30:01    -3940.909912        0.171014\n",
      "FIRE:   12 17:30:01    -3940.912109        0.158814\n",
      "FIRE:   13 17:30:02    -3940.915527        0.143134\n",
      "FIRE:   14 17:30:03    -3940.916504        0.124402\n",
      "FIRE:   15 17:30:03    -3940.919922        0.103209\n",
      "FIRE:   16 17:30:04    -3940.922119        0.080166\n",
      "FIRE:   17 17:30:04    -3940.923828        0.053499\n",
      "FIRE:   18 17:30:05    -3940.925781        0.041349\n",
      "FIRE:   19 17:30:05    -3940.927490        0.050361\n",
      "FIRE:   20 17:30:06    -3940.927246        0.049843\n",
      "FIRE:   21 17:30:06    -3940.928223        0.048813\n",
      "FIRE:   22 17:30:07    -3940.927979        0.047279\n",
      "FIRE:   23 17:30:07    -3940.927246        0.045276\n",
      "FIRE:   24 17:30:08    -3940.927490        0.042840\n",
      "FIRE:   25 17:30:08    -3940.927002        0.040017\n",
      "FIRE:   26 17:30:09    -3940.927490        0.036852\n",
      "FIRE:   27 17:30:10    -3940.928711        0.033049\n",
      "FIRE:   28 17:30:10    -3940.927734        0.031113\n",
      "FIRE:   29 17:30:11    -3940.928223        0.030500\n",
      "FIRE:   30 17:30:11    -3940.927490        0.029711\n",
      "FIRE:   31 17:30:12    -3940.929688        0.028606\n",
      "FIRE:   32 17:30:12    -3940.929443        0.026984\n",
      "FIRE:   33 17:30:13    -3940.928955        0.024590\n",
      "FIRE:   34 17:30:13    -3940.927246        0.021167\n",
      "FIRE:   35 17:30:14    -3940.927979        0.016507\n",
      "FIRE:   36 17:30:14    -3940.930176        0.010552\n",
      "FIRE:   37 17:30:15    -3940.929199        0.007598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:30:16    -3940.657227        0.441555\n",
      "FIRE:    1 17:30:16    -3940.671631        0.432910\n",
      "FIRE:    2 17:30:17    -3940.691895        0.415812\n",
      "FIRE:    3 17:30:18    -3940.723877        0.390431\n",
      "FIRE:    4 17:30:18    -3940.754639        0.356628\n",
      "FIRE:    5 17:30:19    -3940.784912        0.313816\n",
      "FIRE:    6 17:30:19    -3940.808838        0.261410\n",
      "FIRE:    7 17:30:20    -3940.825439        0.200362\n",
      "FIRE:    8 17:30:21    -3940.838623        0.129886\n",
      "FIRE:    9 17:30:21    -3940.838135        0.121522\n",
      "FIRE:   10 17:30:22    -3940.838867        0.119483\n",
      "FIRE:   11 17:30:22    -3940.840332        0.115455\n",
      "FIRE:   12 17:30:23    -3940.841553        0.109544\n",
      "FIRE:   13 17:30:23    -3940.843506        0.101918\n",
      "FIRE:   14 17:30:24    -3940.844727        0.092777\n",
      "FIRE:   15 17:30:24    -3940.848145        0.082375\n",
      "FIRE:   16 17:30:25    -3940.849854        0.071026\n",
      "FIRE:   17 17:30:25    -3940.850830        0.057855\n",
      "FIRE:   18 17:30:26    -3940.850342        0.043463\n",
      "FIRE:   19 17:30:26    -3940.852539        0.040658\n",
      "FIRE:   20 17:30:27    -3940.853027        0.045495\n",
      "FIRE:   21 17:30:27    -3940.852539        0.049499\n",
      "FIRE:   22 17:30:28    -3940.851562        0.049111\n",
      "FIRE:   23 17:30:28    -3940.851807        0.048346\n",
      "FIRE:   24 17:30:29    -3940.853516        0.047227\n",
      "FIRE:   25 17:30:29    -3940.853516        0.045781\n",
      "FIRE:   26 17:30:30    -3940.853027        0.044043\n",
      "FIRE:   27 17:30:31    -3940.853027        0.042069\n",
      "FIRE:   28 17:30:31    -3940.854980        0.039907\n",
      "FIRE:   29 17:30:32    -3940.853271        0.037394\n",
      "FIRE:   30 17:30:32    -3940.855225        0.034559\n",
      "FIRE:   31 17:30:33    -3940.855469        0.031534\n",
      "FIRE:   32 17:30:33    -3940.854736        0.028511\n",
      "FIRE:   33 17:30:34    -3940.855469        0.025695\n",
      "FIRE:   34 17:30:34    -3940.854492        0.023155\n",
      "FIRE:   35 17:30:35    -3940.854492        0.020718\n",
      "FIRE:   36 17:30:36    -3940.855957        0.017942\n",
      "FIRE:   37 17:30:36    -3940.856201        0.016740\n",
      "FIRE:   38 17:30:37    -3940.856445        0.015073\n",
      "FIRE:   39 17:30:37    -3940.855957        0.011319\n",
      "FIRE:   40 17:30:38    -3940.856445        0.007224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:30:45    -3939.779785        1.406793\n",
      "FIRE:    1 17:30:48    -3939.847656        1.178362\n",
      "FIRE:    2 17:30:51    -3939.948486        0.859930\n",
      "FIRE:    3 17:30:53    -3940.040283        0.593271\n",
      "FIRE:    4 17:30:56    -3940.094971        0.321505\n",
      "FIRE:    5 17:30:59    -3940.106934        0.464962\n",
      "FIRE:    6 17:31:02    -3940.104492        0.592485\n",
      "FIRE:    7 17:31:04    -3940.107666        0.571796\n",
      "FIRE:    8 17:31:07    -3940.113525        0.531140\n",
      "FIRE:    9 17:31:10    -3940.120605        0.471931\n",
      "FIRE:   10 17:31:13    -3940.130615        0.396341\n",
      "FIRE:   11 17:31:15    -3940.140381        0.307337\n",
      "FIRE:   12 17:31:18    -3940.148438        0.208861\n",
      "FIRE:   13 17:31:21    -3940.157227        0.116119\n",
      "FIRE:   14 17:31:23    -3940.165039        0.090388\n",
      "FIRE:   15 17:31:26    -3940.169189        0.133195\n",
      "FIRE:   16 17:31:29    -3940.169189        0.217166\n",
      "FIRE:   17 17:31:32    -3940.171387        0.274055\n",
      "FIRE:   18 17:31:35    -3940.174072        0.288468\n",
      "FIRE:   19 17:31:37    -3940.177979        0.252736\n",
      "FIRE:   20 17:31:42    -3940.187256        0.178309\n",
      "FIRE:   21 17:31:45    -3940.196289        0.113121\n",
      "FIRE:   22 17:31:48    -3940.205322        0.102082\n",
      "FIRE:   23 17:31:51    -3940.208740        0.165583\n",
      "FIRE:   24 17:31:54    -3940.211426        0.166682\n",
      "FIRE:   25 17:31:57    -3940.214844        0.151125\n",
      "FIRE:   26 17:31:59    -3940.222168        0.109713\n",
      "FIRE:   27 17:32:02    -3940.229980        0.108154\n",
      "FIRE:   28 17:32:05    -3940.232422        0.103302\n",
      "FIRE:   29 17:32:07    -3940.235840        0.104748\n",
      "FIRE:   30 17:32:13    -3940.242920        0.088302\n",
      "FIRE:   31 17:32:17    -3940.248291        0.099811\n",
      "FIRE:   32 17:32:20    -3940.251953        0.086420\n",
      "FIRE:   33 17:32:22    -3940.257812        0.075330\n",
      "FIRE:   34 17:32:25    -3940.261963        0.073763\n",
      "FIRE:   35 17:32:28    -3940.263672        0.069796\n",
      "FIRE:   36 17:32:30    -3940.267334        0.022422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:32:31    -3940.666504        0.521325\n",
      "FIRE:    1 17:32:31    -3940.683350        0.490062\n",
      "FIRE:    2 17:32:32    -3940.716553        0.432640\n",
      "FIRE:    3 17:32:32    -3940.758545        0.361904\n",
      "FIRE:    4 17:32:33    -3940.799805        0.325828\n",
      "FIRE:    5 17:32:33    -3940.837402        0.274776\n",
      "FIRE:    6 17:32:34    -3940.869385        0.205748\n",
      "FIRE:    7 17:32:35    -3940.892578        0.172088\n",
      "FIRE:    8 17:32:35    -3940.906006        0.158781\n",
      "FIRE:    9 17:32:36    -3940.905762        0.183582\n",
      "FIRE:   10 17:32:36    -3940.906982        0.179353\n",
      "FIRE:   11 17:32:37    -3940.909912        0.171013\n",
      "FIRE:   12 17:32:37    -3940.911865        0.158818\n",
      "FIRE:   13 17:32:38    -3940.915527        0.143132\n",
      "FIRE:   14 17:32:38    -3940.916504        0.124399\n",
      "FIRE:   15 17:32:40    -3940.919922        0.103208\n",
      "FIRE:   16 17:32:40    -3940.921875        0.080161\n",
      "FIRE:   17 17:32:41    -3940.923828        0.053501\n",
      "FIRE:   18 17:32:42    -3940.925781        0.041348\n",
      "FIRE:   19 17:32:43    -3940.927490        0.050360\n",
      "FIRE:   20 17:32:44    -3940.927490        0.049844\n",
      "FIRE:   21 17:32:44    -3940.928223        0.048811\n",
      "FIRE:   22 17:32:45    -3940.927979        0.047278\n",
      "FIRE:   23 17:32:46    -3940.927490        0.045276\n",
      "FIRE:   24 17:32:46    -3940.927490        0.042840\n",
      "FIRE:   25 17:32:47    -3940.927246        0.040017\n",
      "FIRE:   26 17:32:47    -3940.927490        0.036853\n",
      "FIRE:   27 17:32:48    -3940.928711        0.033059\n",
      "FIRE:   28 17:32:48    -3940.927490        0.031112\n",
      "FIRE:   29 17:32:49    -3940.928467        0.030504\n",
      "FIRE:   30 17:32:49    -3940.927490        0.029710\n",
      "FIRE:   31 17:32:50    -3940.929688        0.028605\n",
      "FIRE:   32 17:32:50    -3940.929443        0.026984\n",
      "FIRE:   33 17:32:51    -3940.928955        0.024590\n",
      "FIRE:   34 17:32:51    -3940.927490        0.021165\n",
      "FIRE:   35 17:32:52    -3940.927979        0.016507\n",
      "FIRE:   36 17:32:52    -3940.929932        0.010554\n",
      "FIRE:   37 17:32:53    -3940.929443        0.007594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:32:54    -3940.955811        0.676516\n",
      "FIRE:    1 17:32:55    -3940.978760        0.664339\n",
      "FIRE:    2 17:32:55    -3941.024170        0.640508\n",
      "FIRE:    3 17:32:56    -3941.081055        0.605585\n",
      "FIRE:    4 17:32:56    -3941.144043        0.559460\n",
      "FIRE:    5 17:32:57    -3941.205078        0.500650\n",
      "FIRE:    6 17:32:58    -3941.260010        0.426278\n",
      "FIRE:    7 17:32:58    -3941.299561        0.333891\n",
      "FIRE:    8 17:32:59    -3941.323486        0.213806\n",
      "FIRE:    9 17:32:59    -3941.327881        0.202540\n",
      "FIRE:   10 17:33:00    -3941.328369        0.199114\n",
      "FIRE:   11 17:33:00    -3941.332031        0.192375\n",
      "FIRE:   12 17:33:01    -3941.333740        0.182508\n",
      "FIRE:   13 17:33:01    -3941.337646        0.169819\n",
      "FIRE:   14 17:33:02    -3941.342773        0.154695\n",
      "FIRE:   15 17:33:02    -3941.348633        0.137582\n",
      "FIRE:   16 17:33:03    -3941.351807        0.119033\n",
      "FIRE:   17 17:33:03    -3941.354492        0.106430\n",
      "FIRE:   18 17:33:04    -3941.358643        0.112152\n",
      "FIRE:   19 17:33:05    -3941.361816        0.116772\n",
      "FIRE:   20 17:33:05    -3941.361328        0.118682\n",
      "FIRE:   21 17:33:06    -3941.364014        0.116286\n",
      "FIRE:   22 17:33:07    -3941.366455        0.108179\n",
      "FIRE:   23 17:33:07    -3941.367920        0.093132\n",
      "FIRE:   24 17:33:08    -3941.370605        0.070325\n",
      "FIRE:   25 17:33:08    -3941.370605        0.054101\n",
      "FIRE:   26 17:33:09    -3941.373535        0.040186\n",
      "FIRE:   27 17:33:09    -3941.374268        0.041052\n",
      "FIRE:   28 17:33:10    -3941.374756        0.040067\n",
      "FIRE:   29 17:33:10    -3941.374512        0.038155\n",
      "FIRE:   30 17:33:11    -3941.375732        0.035443\n",
      "FIRE:   31 17:33:11    -3941.375244        0.032109\n",
      "FIRE:   32 17:33:12    -3941.376709        0.028379\n",
      "FIRE:   33 17:33:12    -3941.376465        0.024530\n",
      "FIRE:   34 17:33:13    -3941.376953        0.020873\n",
      "FIRE:   35 17:33:13    -3941.376465        0.017383\n",
      "FIRE:   36 17:33:14    -3941.377441        0.020135\n",
      "FIRE:   37 17:33:14    -3941.377197        0.019158\n",
      "FIRE:   38 17:33:15    -3941.377441        0.014126\n",
      "FIRE:   39 17:33:16    -3941.379395        0.009331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:33:20    -3785.911621      369.470749\n",
      "FIRE:    1 17:33:23    -3819.275879      407.758861\n",
      "FIRE:    2 17:33:26    -3874.337158      324.985308\n",
      "FIRE:    3 17:33:28    -3909.151367      114.811533\n",
      "FIRE:    4 17:33:31    -3914.193604       56.661716\n",
      "FIRE:    5 17:33:34    -3919.214844       48.016712\n",
      "FIRE:    6 17:33:37    -3923.110596       46.555135\n",
      "FIRE:    7 17:33:40    -3923.815918       39.577251\n",
      "FIRE:    8 17:33:44    -3924.750732       29.450146\n",
      "FIRE:    9 17:33:46    -3925.622803       22.219471\n",
      "FIRE:   10 17:33:49    -3925.834229       16.646823\n",
      "FIRE:   11 17:33:52    -3925.312500       26.403074\n",
      "FIRE:   12 17:33:55    -3925.161621       20.973947\n",
      "FIRE:   13 17:33:57    -3924.184326       27.462178\n",
      "FIRE:   14 17:34:00    -3922.467041       40.735790\n",
      "FIRE:   15 17:34:03    -3919.996582       59.252709\n",
      "FIRE:   16 17:34:05    -3917.037598       78.307347\n",
      "FIRE:   17 17:34:08    -3913.967041       94.706724\n",
      "FIRE:   18 17:34:11    -3911.046631      111.922758\n",
      "FIRE:   19 17:34:13    -3908.354004      129.979853\n",
      "FIRE:   20 17:34:16    -3906.004150      144.912211\n",
      "FIRE:   21 17:34:19    -3904.236328      153.457916\n",
      "FIRE:   22 17:34:21    -3903.186523      155.182293\n",
      "FIRE:   23 17:34:24    -3902.807617      154.422640\n",
      "FIRE:   24 17:34:27    -3902.963623      153.876054\n",
      "FIRE:   25 17:34:30    -3903.522949      152.301083\n",
      "FIRE:   26 17:34:32    -3904.409424      147.387330\n",
      "FIRE:   27 17:34:35    -3905.533936      138.567786\n",
      "FIRE:   28 17:34:38    -3906.782959      127.944984\n",
      "FIRE:   29 17:34:41    -3908.076416      117.718730\n",
      "FIRE:   30 17:34:43    -3909.361084      108.250209\n",
      "FIRE:   31 17:34:46    -3910.521729       98.919029\n",
      "FIRE:   32 17:34:49    -3911.385986       90.917538\n",
      "FIRE:   33 17:34:51    -3911.768555       87.721727\n",
      "FIRE:   34 17:34:54    -3911.523926       87.280430\n",
      "FIRE:   35 17:34:56    -3910.545898       90.013967\n",
      "FIRE:   36 17:34:59    -3908.767334       96.518719\n",
      "FIRE:   37 17:35:02    -3906.120117      108.065807\n",
      "FIRE:   38 17:35:04    -3902.530273      125.327054\n",
      "FIRE:   39 17:35:09    -3898.037842      147.656440\n",
      "FIRE:   40 17:35:11    -3892.865479      174.236836\n",
      "FIRE:   41 17:35:14    -3887.203369      204.537828\n",
      "FIRE:   42 17:35:17    -3880.958984      237.113516\n",
      "FIRE:   43 17:35:20    -3873.766602      271.123701\n",
      "FIRE:   44 17:35:22    -3864.840576      310.135335\n",
      "FIRE:   45 17:35:25    -3852.808350      364.786002\n",
      "FIRE:   46 17:35:28    -3837.653076      420.863185\n",
      "FIRE:   47 17:35:30    -3822.788330      463.868071\n",
      "FIRE:   48 17:35:33    -3811.380127      530.177944\n",
      "FIRE:   49 17:35:36    -3803.783691      547.147395\n",
      "FIRE:   50 17:35:38    -3800.617920      529.334535\n",
      "FIRE:   51 17:35:41    -3799.192383      501.462223\n",
      "FIRE:   52 17:35:44    -3797.075195      453.076019\n",
      "FIRE:   53 17:35:47    -3793.270996      416.735739\n",
      "FIRE:   54 17:35:49    -3788.100342      401.597845\n",
      "FIRE:   55 17:35:52    -3782.284180      370.273050\n",
      "FIRE:   56 17:35:55    -3780.231689      398.931893\n",
      "FIRE:   57 17:35:57    -3780.175049      422.747813\n",
      "FIRE:   58 17:36:00    -3772.606689      746.138445\n",
      "FIRE:   59 17:36:03    -3705.356934     3663.738460\n",
      "FIRE:   60 17:36:05    -3568.303467     6324.832217\n",
      "FIRE:   61 17:36:08    -3536.264160     5938.926081\n",
      "FIRE:   62 17:36:11    -3607.283691     3716.900926\n",
      "FIRE:   63 17:36:13    -3672.701416     1693.386469\n",
      "FIRE:   64 17:36:16    -3713.528076      554.053959\n",
      "FIRE:   65 17:36:19    -3749.196533      452.141376\n",
      "FIRE:   66 17:36:22    -3783.914062      356.559094\n",
      "FIRE:   67 17:36:25    -3804.578857      423.143696\n",
      "FIRE:   68 17:36:28    -3829.593018      420.410830\n",
      "FIRE:   69 17:36:31    -3855.361572      290.530029\n",
      "FIRE:   70 17:36:34    -3873.046387      167.259384\n",
      "FIRE:   71 17:36:37    -3881.373291       95.767388\n",
      "FIRE:   72 17:36:40    -3882.269531      100.710828\n",
      "FIRE:   73 17:36:42    -3880.093262      133.096749\n",
      "FIRE:   74 17:36:45    -3880.467041      131.393750\n",
      "FIRE:   75 17:36:48    -3883.869141       92.482154\n",
      "FIRE:   76 17:36:50    -3886.682373      102.182569\n",
      "FIRE:   77 17:36:53    -3890.189209      111.991539\n",
      "FIRE:   78 17:36:56    -3896.901123       86.419313\n",
      "FIRE:   79 17:36:59    -3904.068115       48.771139\n",
      "FIRE:   80 17:37:01    -3909.045410       23.352601\n",
      "FIRE:   81 17:37:04    -3912.095459       21.603258\n",
      "FIRE:   82 17:37:07    -3913.938965       20.147840\n",
      "FIRE:   83 17:37:09    -3915.030273       27.339639\n",
      "FIRE:   84 17:37:12    -3919.648193        8.005052\n",
      "FIRE:   85 17:37:15    -3921.634521        5.548521\n",
      "FIRE:   86 17:37:17    -3923.077637        6.859556\n",
      "FIRE:   87 17:37:20    -3924.399658       32.967061\n",
      "FIRE:   88 17:37:23    -3925.412598       26.533440\n",
      "FIRE:   89 17:37:25    -3920.943359      285.974938\n",
      "FIRE:   90 17:37:28    -3925.915039       26.672182\n",
      "FIRE:   91 17:37:31    -3919.535400       58.943391\n",
      "FIRE:   92 17:37:33    -3923.996582       94.539666\n",
      "FIRE:   93 17:37:36    -3926.569336       26.164196\n",
      "FIRE:   94 17:37:39    -3926.627441       21.323126\n",
      "FIRE:   95 17:37:43    -3926.743408       22.695450\n",
      "FIRE:   96 17:37:46    -3926.913574       51.194231\n",
      "FIRE:   97 17:37:48    -3926.926270       27.955879\n",
      "FIRE:   98 17:37:51    -3926.952881        8.001498\n",
      "FIRE:   99 17:37:54    -3926.956543        7.536713\n",
      "FIRE:  100 17:37:57    -3926.964111        6.617290\n",
      "FIRE:  101 17:37:59    -3926.972900        5.278444\n",
      "FIRE:  102 17:38:02    -3926.987549        3.611032\n",
      "FIRE:  103 17:38:05    -3927.002686        3.344880\n",
      "FIRE:  104 17:38:07    -3927.023682        3.338948\n",
      "FIRE:  105 17:38:10    -3927.046387        3.331986\n",
      "FIRE:  106 17:38:13    -3927.076904        4.020609\n",
      "FIRE:  107 17:38:16    -3927.113281        4.405426\n",
      "FIRE:  108 17:38:18    -3927.159180        3.940308\n",
      "FIRE:  109 17:38:21    -3927.214355        3.282744\n",
      "FIRE:  110 17:38:24    -3927.281494        3.262738\n",
      "FIRE:  111 17:38:27    -3927.365479        3.238191\n",
      "FIRE:  112 17:38:29    -3927.465820        3.208297\n",
      "FIRE:  113 17:38:32    -3927.587158        3.172173\n",
      "FIRE:  114 17:38:35    -3927.730713        3.128675\n",
      "FIRE:  115 17:38:37    -3927.902832        3.076488\n",
      "FIRE:  116 17:38:43    -3928.107910        3.014114\n",
      "FIRE:  117 17:38:46    -3928.344238        2.940174\n",
      "FIRE:  118 17:38:49    -3928.619873        2.852934\n",
      "FIRE:  119 17:38:52    -3928.937500        2.750729\n",
      "FIRE:  120 17:38:54    -3929.300537        4.532850\n",
      "FIRE:  121 17:38:57    -3929.702637        4.911112\n",
      "FIRE:  122 17:39:00    -3930.143311        8.076458\n",
      "FIRE:  123 17:39:02    -3930.610107       22.046165\n",
      "FIRE:  124 17:39:05    -3930.620850        2.151897\n",
      "FIRE:  125 17:39:08    -3930.622803        2.151762\n",
      "FIRE:  126 17:39:11    -3930.627930        2.151501\n",
      "FIRE:  127 17:39:13    -3930.635010        2.151095\n",
      "FIRE:  128 17:39:16    -3930.645752        2.150532\n",
      "FIRE:  129 17:39:19    -3930.657959        2.149821\n",
      "FIRE:  130 17:39:21    -3930.673096        2.148934\n",
      "FIRE:  131 17:39:24    -3930.691650        2.147861\n",
      "FIRE:  132 17:39:27    -3930.714111        2.146430\n",
      "FIRE:  133 17:39:29    -3930.742920        2.144512\n",
      "FIRE:  134 17:39:32    -3930.779053        2.141963\n",
      "FIRE:  135 17:39:35    -3930.822998        2.138510\n",
      "FIRE:  136 17:39:37    -3930.877930        2.133811\n",
      "FIRE:  137 17:39:40    -3930.944336        2.127382\n",
      "FIRE:  138 17:39:43    -3931.025391        2.118519\n",
      "FIRE:  139 17:39:46    -3931.053711        1.750986\n",
      "FIRE:  140 17:39:49    -3931.069336        2.834803\n",
      "FIRE:  141 17:39:51    -3931.074463        5.745443\n",
      "FIRE:  142 17:39:54    -3931.046387        9.722805\n",
      "FIRE:  143 17:39:57    -3930.959229       14.354676\n",
      "FIRE:  144 17:39:59    -3930.745117       18.952864\n",
      "FIRE:  145 17:40:02    -3930.263672       23.422605\n",
      "FIRE:  146 17:40:05    -3929.397949       28.423643\n",
      "FIRE:  147 17:40:08    -3928.343994       34.679361\n",
      "FIRE:  148 17:40:10    -3927.080811       42.692049\n",
      "FIRE:  149 17:40:13    -3925.642090       44.788698\n",
      "FIRE:  150 17:40:16    -3924.125732       39.812487\n",
      "FIRE:  151 17:40:18    -3922.574463       32.600157\n",
      "FIRE:  152 17:40:21    -3921.021729       25.709186\n",
      "FIRE:  153 17:40:24    -3919.535889       19.430713\n",
      "FIRE:  154 17:40:27    -3918.197998       16.124757\n",
      "FIRE:  155 17:40:29    -3916.934082       19.499191\n",
      "FIRE:  156 17:40:32    -3915.746338       24.381148\n",
      "FIRE:  157 17:40:35    -3914.748291       25.883898\n",
      "FIRE:  158 17:40:37    -3913.982422       25.379661\n",
      "FIRE:  159 17:40:40    -3913.434082       25.531427\n",
      "FIRE:  160 17:40:43    -3913.019775       26.785302\n",
      "FIRE:  161 17:40:46    -3912.647705       29.390446\n",
      "FIRE:  162 17:40:49    -3912.230225       34.650087\n",
      "FIRE:  163 17:40:52    -3911.687256       42.441350\n",
      "FIRE:  164 17:40:55    -3910.993164       50.818913\n",
      "FIRE:  165 17:40:57    -3910.220459       57.235094\n",
      "FIRE:  166 17:41:00    -3909.432373       59.910682\n",
      "FIRE:  167 17:41:03    -3908.566162       59.469060\n",
      "FIRE:  168 17:41:06    -3907.482910       60.315504\n",
      "FIRE:  169 17:41:10    -3906.069580       62.892403\n",
      "FIRE:  170 17:41:14    -3904.241455       68.556621\n",
      "FIRE:  171 17:41:18    -3901.933350       77.835273\n",
      "FIRE:  172 17:41:21    -3899.128906       89.767244\n",
      "FIRE:  173 17:41:24    -3895.903564      102.729006\n",
      "FIRE:  174 17:41:26    -3892.402588      115.936611\n",
      "FIRE:  175 17:41:29    -3888.821777      129.836373\n",
      "FIRE:  176 17:41:32    -3885.460205      144.324743\n",
      "FIRE:  177 17:41:34    -3882.639648      159.426808\n",
      "FIRE:  178 17:41:37    -3880.385254      171.512292\n",
      "FIRE:  179 17:41:41    -3878.221680      173.095010\n",
      "FIRE:  180 17:41:44    -3875.480713      176.832528\n",
      "FIRE:  181 17:41:48    -3871.810791      179.956208\n",
      "FIRE:  182 17:41:50    -3867.030273      186.558523\n",
      "FIRE:  183 17:41:53    -3860.817871      205.912163\n",
      "FIRE:  184 17:41:56    -3853.725098      242.731295\n",
      "FIRE:  185 17:41:59    -3846.688965      274.230440\n",
      "FIRE:  186 17:42:01    -3841.156006      289.589158\n",
      "FIRE:  187 17:42:04    -3837.575439      295.415264\n",
      "FIRE:  188 17:42:07    -3835.270020      316.124469\n",
      "FIRE:  189 17:42:09    -3832.982666      339.499939\n",
      "FIRE:  190 17:42:12    -3829.981934      365.843210\n",
      "FIRE:  191 17:42:15    -3827.291504      394.224576\n",
      "FIRE:  192 17:42:17    -3824.587646      418.326036\n",
      "FIRE:  193 17:42:20    -3818.557373      458.639415\n",
      "FIRE:  194 17:42:23    -3806.395508      559.711987\n",
      "FIRE:  195 17:42:25    -3782.607666      831.318821\n",
      "FIRE:  196 17:42:28    -3735.680908     2111.434261\n",
      "FIRE:  197 17:42:31    -3692.201904     3411.385745\n",
      "FIRE:  198 17:42:34    -3693.849609     3061.894113\n",
      "FIRE:  199 17:42:36    -3704.305420     2321.833237\n",
      "FIRE:  200 17:42:40    -3696.877197     1968.326611\n",
      "Progress: 20/42 calculations completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:42:41    -3940.666504        0.521325\n",
      "FIRE:    1 17:42:42    -3940.683350        0.490063\n",
      "FIRE:    2 17:42:42    -3940.716553        0.432639\n",
      "FIRE:    3 17:42:43    -3940.758301        0.361904\n",
      "FIRE:    4 17:42:44    -3940.799805        0.325828\n",
      "FIRE:    5 17:42:44    -3940.837646        0.274776\n",
      "FIRE:    6 17:42:45    -3940.869629        0.205748\n",
      "FIRE:    7 17:42:45    -3940.892334        0.172088\n",
      "FIRE:    8 17:42:46    -3940.906006        0.158780\n",
      "FIRE:    9 17:42:47    -3940.905762        0.183578\n",
      "FIRE:   10 17:42:47    -3940.906982        0.179348\n",
      "FIRE:   11 17:42:48    -3940.909668        0.171008\n",
      "FIRE:   12 17:42:48    -3940.911865        0.158816\n",
      "FIRE:   13 17:42:49    -3940.915283        0.143133\n",
      "FIRE:   14 17:42:49    -3940.916260        0.124401\n",
      "FIRE:   15 17:42:50    -3940.919922        0.103206\n",
      "FIRE:   16 17:42:50    -3940.922363        0.080162\n",
      "FIRE:   17 17:42:51    -3940.923828        0.053499\n",
      "FIRE:   18 17:42:51    -3940.926025        0.041349\n",
      "FIRE:   19 17:42:52    -3940.927490        0.050367\n",
      "FIRE:   20 17:42:52    -3940.927246        0.049845\n",
      "FIRE:   21 17:42:53    -3940.928223        0.048807\n",
      "FIRE:   22 17:42:53    -3940.928223        0.047277\n",
      "FIRE:   23 17:42:54    -3940.927246        0.045277\n",
      "FIRE:   24 17:42:55    -3940.927490        0.042840\n",
      "FIRE:   25 17:42:55    -3940.927246        0.040013\n",
      "FIRE:   26 17:42:56    -3940.927490        0.036853\n",
      "FIRE:   27 17:42:56    -3940.928955        0.033058\n",
      "FIRE:   28 17:42:57    -3940.927490        0.031113\n",
      "FIRE:   29 17:42:57    -3940.928223        0.030503\n",
      "FIRE:   30 17:42:58    -3940.927490        0.029709\n",
      "FIRE:   31 17:42:58    -3940.929443        0.028606\n",
      "FIRE:   32 17:42:59    -3940.929199        0.026983\n",
      "FIRE:   33 17:42:59    -3940.928955        0.024590\n",
      "FIRE:   34 17:43:00    -3940.927246        0.021167\n",
      "FIRE:   35 17:43:00    -3940.927979        0.016506\n",
      "FIRE:   36 17:43:01    -3940.930176        0.010555\n",
      "FIRE:   37 17:43:01    -3940.929199        0.007596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:43:02    -3940.768799        0.493115\n",
      "FIRE:    1 17:43:03    -3940.786133        0.475071\n",
      "FIRE:    2 17:43:03    -3940.816406        0.440111\n",
      "FIRE:    3 17:43:04    -3940.851807        0.390018\n",
      "FIRE:    4 17:43:04    -3940.891113        0.326490\n",
      "FIRE:    5 17:43:05    -3940.926270        0.250771\n",
      "FIRE:    6 17:43:05    -3940.953369        0.190901\n",
      "FIRE:    7 17:43:06    -3940.969971        0.131149\n",
      "FIRE:    8 17:43:07    -3940.974365        0.124988\n",
      "FIRE:    9 17:43:07    -3940.974854        0.122417\n",
      "FIRE:   10 17:43:08    -3940.977783        0.117330\n",
      "FIRE:   11 17:43:08    -3940.978516        0.109860\n",
      "FIRE:   12 17:43:09    -3940.977539        0.100194\n",
      "FIRE:   13 17:43:09    -3940.980713        0.088562\n",
      "FIRE:   14 17:43:10    -3940.982422        0.075280\n",
      "FIRE:   15 17:43:10    -3940.982178        0.060698\n",
      "FIRE:   16 17:43:11    -3940.985107        0.043605\n",
      "FIRE:   17 17:43:12    -3940.986328        0.024456\n",
      "FIRE:   18 17:43:12    -3940.986816        0.022904\n",
      "FIRE:   19 17:43:13    -3940.985352        0.034312\n",
      "FIRE:   20 17:43:13    -3940.985352        0.033788\n",
      "FIRE:   21 17:43:14    -3940.985352        0.032739\n",
      "FIRE:   22 17:43:15    -3940.985352        0.031646\n",
      "FIRE:   23 17:43:15    -3940.984863        0.030766\n",
      "FIRE:   24 17:43:16    -3940.985596        0.029694\n",
      "FIRE:   25 17:43:16    -3940.986328        0.028456\n",
      "FIRE:   26 17:43:17    -3940.986084        0.027059\n",
      "FIRE:   27 17:43:17    -3940.986572        0.025370\n",
      "FIRE:   28 17:43:18    -3940.985840        0.023368\n",
      "FIRE:   29 17:43:18    -3940.988037        0.021056\n",
      "FIRE:   30 17:43:19    -3940.985840        0.018477\n",
      "FIRE:   31 17:43:19    -3940.986084        0.015735\n",
      "FIRE:   32 17:43:20    -3940.985840        0.015251\n",
      "FIRE:   33 17:43:21    -3940.987549        0.017560\n",
      "FIRE:   34 17:43:21    -3940.986572        0.018515\n",
      "FIRE:   35 17:43:22    -3940.986328        0.017745\n",
      "FIRE:   36 17:43:22    -3940.987061        0.014986\n",
      "FIRE:   37 17:43:23    -3940.989014        0.010194\n",
      "FIRE:   38 17:43:23    -3940.989014        0.007402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:43:28    -3940.151855        0.913725\n",
      "FIRE:    1 17:43:31    -3940.188721        0.813153\n",
      "FIRE:    2 17:43:33    -3940.249512        0.634575\n",
      "FIRE:    3 17:43:36    -3940.309570        0.406802\n",
      "FIRE:    4 17:43:40    -3940.347656        0.217250\n",
      "FIRE:    5 17:43:42    -3940.364014        0.191446\n",
      "FIRE:    6 17:43:45    -3940.364502        0.321236\n",
      "FIRE:    7 17:43:49    -3940.365967        0.312112\n",
      "FIRE:    8 17:43:51    -3940.368896        0.294103\n",
      "FIRE:    9 17:43:54    -3940.372314        0.267697\n",
      "FIRE:   10 17:43:57    -3940.376709        0.233627\n",
      "FIRE:   11 17:44:00    -3940.382568        0.192933\n",
      "FIRE:   12 17:44:03    -3940.387451        0.147016\n",
      "FIRE:   13 17:44:05    -3940.391846        0.097807\n",
      "FIRE:   14 17:44:08    -3940.395996        0.084483\n",
      "FIRE:   15 17:44:11    -3940.398438        0.084300\n",
      "FIRE:   16 17:44:14    -3940.399414        0.082944\n",
      "FIRE:   17 17:44:16    -3940.403076        0.128781\n",
      "FIRE:   18 17:44:19    -3940.405029        0.157194\n",
      "FIRE:   19 17:44:21    -3940.410156        0.160929\n",
      "FIRE:   20 17:44:25    -3940.414551        0.135798\n",
      "FIRE:   21 17:44:28    -3940.421387        0.088465\n",
      "FIRE:   22 17:44:31    -3940.427002        0.052313\n",
      "FIRE:   23 17:44:32    -3940.427002        0.054590\n",
      "FIRE:   24 17:44:33    -3940.427002        0.086691\n",
      "FIRE:   25 17:44:34    -3940.427002        0.078391\n",
      "FIRE:   26 17:44:36    -3940.432861        0.066546\n",
      "FIRE:   27 17:44:38    -3940.436279        0.102461\n",
      "FIRE:   28 17:44:40    -3940.443848        0.095873\n",
      "FIRE:   29 17:44:42    -3940.452393        0.074159\n",
      "FIRE:   30 17:44:44    -3940.454590        0.079657\n",
      "FIRE:   31 17:44:46    -3940.457764        0.089416\n",
      "FIRE:   32 17:44:49    -3940.462891        0.080247\n",
      "FIRE:   33 17:44:52    -3940.465820        0.113827\n",
      "FIRE:   34 17:44:54    -3940.469971        0.141945\n",
      "FIRE:   35 17:44:56    -3940.473389        0.090375\n",
      "FIRE:   36 17:44:57    -3940.473389        0.102490\n",
      "FIRE:   37 17:44:59    -3940.473389        0.129041\n",
      "FIRE:   38 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   39 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   40 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   41 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   42 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   43 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   44 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   45 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   46 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   47 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   48 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   49 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   50 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   51 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   52 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   53 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   54 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   55 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   56 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   57 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   58 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   59 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   60 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   61 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   62 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   63 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   64 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   65 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   66 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   67 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   68 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   69 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   70 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   71 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   72 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   73 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   74 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   75 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   76 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   77 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   78 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   79 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   80 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   81 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   82 17:45:00    -3940.473389        0.040151\n",
      "FIRE:   83 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   84 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   85 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   86 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   87 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   88 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   89 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   90 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   91 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   92 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   93 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   94 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   95 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   96 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   97 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   98 17:45:01    -3940.473389        0.040151\n",
      "FIRE:   99 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  100 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  101 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  102 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  103 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  104 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  105 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  106 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  107 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  108 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  109 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  110 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  111 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  112 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  113 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  114 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  115 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  116 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  117 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  118 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  119 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  120 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  121 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  122 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  123 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  124 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  125 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  126 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  127 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  128 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  129 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  130 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  131 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  132 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  133 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  134 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  135 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  136 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  137 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  138 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  139 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  140 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  141 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  142 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  143 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  144 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  145 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  146 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  147 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  148 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  149 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  150 17:45:01    -3940.473389        0.040151\n",
      "FIRE:  151 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  152 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  153 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  154 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  155 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  156 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  157 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  158 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  159 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  160 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  161 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  162 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  163 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  164 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  165 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  166 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  167 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  168 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  169 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  170 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  171 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  172 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  173 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  174 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  175 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  176 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  177 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  178 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  179 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  180 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  181 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  182 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  183 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  184 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  185 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  186 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  187 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  188 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  189 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  190 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  191 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  192 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  193 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  194 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  195 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  196 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  197 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  198 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  199 17:45:02    -3940.473389        0.040151\n",
      "FIRE:  200 17:45:02    -3940.473389        0.040151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:45:03    -3940.666504        0.521325\n",
      "FIRE:    1 17:45:04    -3940.683350        0.490062\n",
      "FIRE:    2 17:45:04    -3940.716553        0.432640\n",
      "FIRE:    3 17:45:05    -3940.758545        0.361904\n",
      "FIRE:    4 17:45:05    -3940.799805        0.325828\n",
      "FIRE:    5 17:45:06    -3940.837646        0.274776\n",
      "FIRE:    6 17:45:06    -3940.869629        0.205748\n",
      "FIRE:    7 17:45:07    -3940.892334        0.172088\n",
      "FIRE:    8 17:45:07    -3940.905762        0.158780\n",
      "FIRE:    9 17:45:08    -3940.905518        0.183581\n",
      "FIRE:   10 17:45:08    -3940.906982        0.179348\n",
      "FIRE:   11 17:45:09    -3940.909912        0.171013\n",
      "FIRE:   12 17:45:09    -3940.911865        0.158816\n",
      "FIRE:   13 17:45:10    -3940.915283        0.143133\n",
      "FIRE:   14 17:45:11    -3940.916260        0.124401\n",
      "FIRE:   15 17:45:11    -3940.919922        0.103206\n",
      "FIRE:   16 17:45:12    -3940.922119        0.080160\n",
      "FIRE:   17 17:45:12    -3940.923584        0.053498\n",
      "FIRE:   18 17:45:13    -3940.925781        0.041347\n",
      "FIRE:   19 17:45:13    -3940.927734        0.050360\n",
      "FIRE:   20 17:45:14    -3940.927246        0.049844\n",
      "FIRE:   21 17:45:14    -3940.928223        0.048811\n",
      "FIRE:   22 17:45:15    -3940.927979        0.047277\n",
      "FIRE:   23 17:45:15    -3940.927246        0.045276\n",
      "FIRE:   24 17:45:16    -3940.927490        0.042840\n",
      "FIRE:   25 17:45:16    -3940.927246        0.040015\n",
      "FIRE:   26 17:45:17    -3940.927734        0.036855\n",
      "FIRE:   27 17:45:17    -3940.928711        0.033057\n",
      "FIRE:   28 17:45:18    -3940.927734        0.031112\n",
      "FIRE:   29 17:45:18    -3940.928223        0.030501\n",
      "FIRE:   30 17:45:19    -3940.927490        0.029709\n",
      "FIRE:   31 17:45:20    -3940.929688        0.028604\n",
      "FIRE:   32 17:45:20    -3940.929199        0.026984\n",
      "FIRE:   33 17:45:21    -3940.928955        0.024590\n",
      "FIRE:   34 17:45:21    -3940.927246        0.021167\n",
      "FIRE:   35 17:45:22    -3940.927979        0.016509\n",
      "FIRE:   36 17:45:22    -3940.930176        0.010555\n",
      "FIRE:   37 17:45:23    -3940.929443        0.007594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:45:23    -3940.813965        0.439019\n",
      "FIRE:    1 17:45:24    -3940.835938        0.432514\n",
      "FIRE:    2 17:45:24    -3940.874756        0.422099\n",
      "FIRE:    3 17:45:25    -3940.929932        0.404532\n",
      "FIRE:    4 17:45:25    -3940.988281        0.376786\n",
      "FIRE:    5 17:45:26    -3941.044922        0.334175\n",
      "FIRE:    6 17:45:26    -3941.092041        0.271189\n",
      "FIRE:    7 17:45:27    -3941.121826        0.184460\n",
      "FIRE:    8 17:45:28    -3941.133057        0.087837\n",
      "FIRE:    9 17:45:28    -3941.134766        0.085593\n",
      "FIRE:   10 17:45:29    -3941.135254        0.081181\n",
      "FIRE:   11 17:45:29    -3941.136230        0.074724\n",
      "FIRE:   12 17:45:30    -3941.137451        0.066453\n",
      "FIRE:   13 17:45:30    -3941.141602        0.057843\n",
      "FIRE:   14 17:45:31    -3941.143555        0.052168\n",
      "FIRE:   15 17:45:31    -3941.145020        0.046184\n",
      "FIRE:   16 17:45:32    -3941.145020        0.039613\n",
      "FIRE:   17 17:45:32    -3941.146240        0.033091\n",
      "FIRE:   18 17:45:33    -3941.145996        0.027855\n",
      "FIRE:   19 17:45:33    -3941.145264        0.027767\n",
      "FIRE:   20 17:45:34    -3941.145752        0.027601\n",
      "FIRE:   21 17:45:34    -3941.147217        0.027351\n",
      "FIRE:   22 17:45:35    -3941.147461        0.027024\n",
      "FIRE:   23 17:45:36    -3941.146973        0.026608\n",
      "FIRE:   24 17:45:36    -3941.146484        0.026121\n",
      "FIRE:   25 17:45:37    -3941.146973        0.025555\n",
      "FIRE:   26 17:45:37    -3941.146973        0.024842\n",
      "FIRE:   27 17:45:38    -3941.146729        0.023953\n",
      "FIRE:   28 17:45:38    -3941.147461        0.022855\n",
      "FIRE:   29 17:45:39    -3941.148193        0.021499\n",
      "FIRE:   30 17:45:40    -3941.147705        0.019845\n",
      "FIRE:   31 17:45:40    -3941.148438        0.017853\n",
      "FIRE:   32 17:45:41    -3941.147705        0.015498\n",
      "FIRE:   33 17:45:41    -3941.148926        0.012761\n",
      "FIRE:   34 17:45:42    -3941.148926        0.010172\n",
      "FIRE:   35 17:45:42    -3941.148438        0.008549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:45:48    -3784.239014      391.962302\n",
      "FIRE:    1 17:45:52    -3805.072510      371.169687\n",
      "FIRE:    2 17:45:55    -3857.030273      387.407787\n",
      "FIRE:    3 17:45:58    -3903.023438      153.655050\n",
      "FIRE:    4 17:46:00    -3914.816162       54.410203\n",
      "FIRE:    5 17:46:03    -3919.085938       48.298798\n",
      "FIRE:    6 17:46:06    -3922.535645       47.893473\n",
      "FIRE:    7 17:46:09    -3923.319580       41.869360\n",
      "FIRE:    8 17:46:11    -3924.262695       32.384367\n",
      "FIRE:    9 17:46:14    -3925.107666       22.655139\n",
      "FIRE:   10 17:46:17    -3925.101074       18.233328\n",
      "FIRE:   11 17:46:20    -3924.339600       30.571781\n",
      "FIRE:   12 17:46:22    -3924.440918       23.877415\n",
      "FIRE:   13 17:46:25    -3923.400879       27.428893\n",
      "FIRE:   14 17:46:28    -3921.538574       38.490833\n",
      "FIRE:   15 17:46:30    -3918.885010       57.878585\n",
      "FIRE:   16 17:46:33    -3915.528320       79.976977\n",
      "FIRE:   17 17:46:36    -3911.964600      104.190683\n",
      "FIRE:   18 17:46:38    -3908.571533      131.714574\n",
      "FIRE:   19 17:46:41    -3905.561523      155.691093\n",
      "FIRE:   20 17:46:44    -3903.129639      169.759047\n",
      "FIRE:   21 17:46:47    -3901.501221      172.056090\n",
      "FIRE:   22 17:46:49    -3900.703857      167.746840\n",
      "FIRE:   23 17:46:52    -3900.496826      165.294615\n",
      "FIRE:   24 17:46:55    -3900.643799      168.498014\n",
      "FIRE:   25 17:46:59    -3901.107422      173.420853\n",
      "FIRE:   26 17:47:02    -3901.932861      174.388734\n",
      "FIRE:   27 17:47:05    -3903.060303      168.772978\n",
      "FIRE:   28 17:47:07    -3904.321045      158.603788\n",
      "FIRE:   29 17:47:10    -3905.547119      147.878806\n",
      "FIRE:   30 17:47:13    -3906.626465      138.415916\n",
      "FIRE:   31 17:47:15    -3907.464111      130.544478\n",
      "FIRE:   32 17:47:18    -3907.957520      128.206189\n",
      "FIRE:   33 17:47:21    -3908.014893      128.215120\n",
      "FIRE:   34 17:47:23    -3907.521240      131.389419\n",
      "FIRE:   35 17:47:26    -3906.317383      138.445589\n",
      "FIRE:   36 17:47:29    -3904.258545      149.710732\n",
      "FIRE:   37 17:47:31    -3901.290039      165.867225\n",
      "FIRE:   38 17:47:34    -3897.384277      187.625084\n",
      "FIRE:   39 17:47:38    -3892.522705      213.222858\n",
      "FIRE:   40 17:47:43    -3886.738037      241.315095\n",
      "FIRE:   41 17:47:46    -3879.911621      272.282333\n",
      "FIRE:   42 17:47:49    -3871.650146      306.834721\n",
      "FIRE:   43 17:47:51    -3860.990234      349.425703\n",
      "FIRE:   44 17:47:54    -3845.095703      445.066496\n",
      "FIRE:   45 17:47:57    -3818.056396      668.396560\n",
      "FIRE:   46 17:47:59    -3771.783691     1396.840275\n",
      "FIRE:   47 17:48:02    -3715.869141     2550.931162\n",
      "FIRE:   48 17:48:05    -3686.645020     2883.603999\n",
      "FIRE:   49 17:48:08    -3694.311523     2155.248155\n",
      "FIRE:   50 17:48:11    -3697.110596     1547.669679\n",
      "FIRE:   51 17:48:14    -3692.660889     1015.682936\n",
      "FIRE:   52 17:48:16    -3693.588867      430.522239\n",
      "FIRE:   53 17:48:19    -3698.093994      327.864926\n",
      "FIRE:   54 17:48:22    -3786.790771      485.621194\n",
      "FIRE:   55 17:48:25    -3829.658203      399.159538\n",
      "FIRE:   56 17:48:27    -3857.729004      323.933339\n",
      "FIRE:   57 17:48:30    -3876.593018      214.326214\n",
      "FIRE:   58 17:48:33    -3888.967041      149.835285\n",
      "FIRE:   59 17:48:36    -3897.058105      108.097977\n",
      "FIRE:   60 17:48:40    -3901.934326       79.462798\n",
      "FIRE:   61 17:48:43    -3904.943604       73.862511\n",
      "FIRE:   62 17:48:45    -3907.980225       78.049954\n",
      "FIRE:   63 17:48:48    -3912.342773       66.821266\n",
      "FIRE:   64 17:48:51    -3917.436523       46.309709\n",
      "FIRE:   65 17:48:53    -3921.903320       27.464112\n",
      "FIRE:   66 17:48:56    -3925.156006       19.733148\n",
      "FIRE:   67 17:48:59    -3927.324951       16.549911\n",
      "FIRE:   68 17:49:02    -3928.677979       13.103917\n",
      "FIRE:   69 17:49:04    -3929.323486       10.745451\n",
      "FIRE:   70 17:49:07    -3929.450195        6.893704\n",
      "FIRE:   71 17:49:10    -3929.583496        7.371393\n",
      "FIRE:   72 17:49:12    -3929.707031        7.454185\n",
      "FIRE:   73 17:49:15    -3929.789795       10.560598\n",
      "FIRE:   74 17:49:18    -3929.792236       15.773605\n",
      "FIRE:   75 17:49:21    -3929.667969       21.850815\n",
      "FIRE:   76 17:49:23    -3929.351318       28.084077\n",
      "FIRE:   77 17:49:26    -3928.788330       33.494543\n",
      "FIRE:   78 17:49:29    -3927.008057       36.934650\n",
      "FIRE:   79 17:49:31    -3925.432861       37.106288\n",
      "FIRE:   80 17:49:34    -3925.026367       38.277613\n",
      "FIRE:   81 17:49:37    -3925.911377       43.711296\n",
      "FIRE:   82 17:49:40    -3916.895020       78.829909\n",
      "FIRE:   83 17:49:42    -3921.243164       33.409092\n",
      "FIRE:   84 17:49:45    -3920.774658       20.996513\n",
      "FIRE:   85 17:49:48    -3919.422607       37.298055\n",
      "FIRE:   86 17:49:50    -3917.293457       52.798435\n",
      "FIRE:   87 17:49:53    -3916.146973       49.441480\n",
      "FIRE:   88 17:49:56    -3915.191650       43.449805\n",
      "FIRE:   89 17:49:58    -3914.282471       41.259418\n",
      "FIRE:   90 17:50:01    -3913.521240       44.418707\n",
      "FIRE:   91 17:50:04    -3912.938965       49.098824\n",
      "FIRE:   92 17:50:06    -3912.581543       51.678932\n",
      "FIRE:   93 17:50:09    -3912.495850       51.412633\n",
      "FIRE:   94 17:50:12    -3912.699951       49.330018\n",
      "FIRE:   95 17:50:15    -3913.197754       46.253802\n",
      "FIRE:   96 17:50:17    -3913.942139       42.650538\n",
      "FIRE:   97 17:50:20    -3914.849854       39.214552\n",
      "FIRE:   98 17:50:23    -3915.837158       36.200745\n",
      "FIRE:   99 17:50:25    -3916.741455       32.948272\n",
      "FIRE:  100 17:50:28    -3917.342529       31.384656\n",
      "FIRE:  101 17:50:31    -3917.493652       32.484746\n",
      "FIRE:  102 17:50:33    -3917.181396       34.477516\n",
      "FIRE:  103 17:50:36    -3916.376709       36.745712\n",
      "FIRE:  104 17:50:39    -3914.984619       40.412511\n",
      "FIRE:  105 17:50:42    -3912.884766       48.601642\n",
      "FIRE:  106 17:50:45    -3909.971924       64.106974\n",
      "FIRE:  107 17:50:47    -3906.113037       89.138985\n",
      "FIRE:  108 17:50:50    -3901.338623      121.418845\n",
      "FIRE:  109 17:50:53    -3895.959473      155.347983\n",
      "FIRE:  110 17:50:56    -3890.162598      187.099763\n",
      "FIRE:  111 17:50:58    -3883.810059      217.453979\n",
      "FIRE:  112 17:51:01    -3876.537109      248.345871\n",
      "FIRE:  113 17:51:04    -3867.775635      284.998777\n",
      "FIRE:  114 17:51:06    -3856.439453      342.215616\n",
      "FIRE:  115 17:51:09    -3842.473145      421.637121\n",
      "FIRE:  116 17:51:12    -3828.149902      501.538029\n",
      "FIRE:  117 17:51:14    -3816.545654      531.086347\n",
      "FIRE:  118 17:51:17    -3807.843262      518.641340\n",
      "FIRE:  119 17:51:19    -3798.383301      540.915442\n",
      "FIRE:  120 17:51:22    -3789.619385      548.741483\n",
      "FIRE:  121 17:51:25    -3782.841797      544.141443\n",
      "FIRE:  122 17:51:27    -3777.229248      552.550035\n",
      "FIRE:  123 17:51:30    -3772.345947      563.975111\n",
      "FIRE:  124 17:51:33    -3768.683594      559.749449\n",
      "FIRE:  125 17:51:36    -3765.384277      553.556869\n",
      "FIRE:  126 17:51:38    -3760.632568      474.568688\n",
      "FIRE:  127 17:51:41    -3757.625977      471.068433\n",
      "FIRE:  128 17:51:44    -3739.175781     1264.988634\n",
      "FIRE:  129 17:51:47    -3639.030762     4520.925902\n",
      "FIRE:  130 17:51:51    -3563.567139     5342.992546\n",
      "FIRE:  131 17:51:54    -3600.219482     4006.653619\n",
      "FIRE:  132 17:51:57    -3655.321777     2242.464773\n",
      "FIRE:  133 17:52:00    -3679.024658      958.164738\n",
      "FIRE:  134 17:52:02    -3700.080811      470.892186\n",
      "FIRE:  135 17:52:05    -3728.966553      576.557196\n",
      "FIRE:  136 17:52:09    -3768.195068      441.321513\n",
      "FIRE:  137 17:52:13    -3791.079346      449.115467\n",
      "FIRE:  138 17:52:16    -3816.176514      441.962484\n",
      "FIRE:  139 17:52:19    -3841.794189      322.543694\n",
      "FIRE:  140 17:52:22    -3861.271729      198.823569\n",
      "FIRE:  141 17:52:25    -3872.333008      115.958211\n",
      "FIRE:  142 17:52:29    -3875.668701       85.030016\n",
      "FIRE:  143 17:52:32    -3874.017090      119.659221\n",
      "FIRE:  144 17:52:35    -3872.528564      138.976304\n",
      "FIRE:  145 17:52:38    -3874.806641      117.795973\n",
      "FIRE:  146 17:52:41    -3878.478760       88.733857\n",
      "FIRE:  147 17:52:44    -3881.591064      113.438421\n",
      "FIRE:  148 17:52:46    -3887.019287      105.228883\n",
      "FIRE:  149 17:52:49    -3894.631348       71.007544\n",
      "FIRE:  150 17:52:51    -3901.196533       37.875996\n",
      "FIRE:  151 17:52:55    -3905.783936       26.342498\n",
      "FIRE:  152 17:52:58    -3909.084717       22.611131\n",
      "FIRE:  153 17:53:00    -3911.346924       16.749764\n",
      "FIRE:  154 17:53:03    -3912.696045       23.813923\n",
      "FIRE:  155 17:53:05    -3913.752197       28.720201\n",
      "FIRE:  156 17:53:08    -3915.245361       28.115786\n",
      "FIRE:  157 17:53:11    -3917.297607       22.151036\n",
      "FIRE:  158 17:53:14    -3919.406982       15.307386\n",
      "FIRE:  159 17:53:16    -3921.109619        9.555021\n",
      "FIRE:  160 17:53:19    -3922.291504        6.099366\n",
      "FIRE:  161 17:53:21    -3923.046631        5.978787\n",
      "FIRE:  162 17:53:24    -3923.508545        5.660226\n",
      "FIRE:  163 17:53:27    -3923.800293        7.635084\n",
      "FIRE:  164 17:53:30    -3924.031494       10.227823\n",
      "FIRE:  165 17:53:34    -3924.317871       11.721848\n",
      "FIRE:  166 17:53:38    -3924.737305       11.552580\n",
      "FIRE:  167 17:53:42    -3925.247070        9.991150\n",
      "FIRE:  168 17:53:47    -3925.732178        7.892214\n",
      "FIRE:  169 17:53:50    -3926.106445        5.937608\n",
      "FIRE:  170 17:53:52    -3926.333740        4.335353\n",
      "FIRE:  171 17:53:55    -3926.416016        3.481233\n",
      "FIRE:  172 17:53:58    -3926.384033        3.588656\n",
      "FIRE:  173 17:54:00    -3926.287598        4.882949\n",
      "FIRE:  174 17:54:03    -3928.022461        2.443394\n",
      "FIRE:  175 17:54:06    -3929.241455        1.879929\n",
      "FIRE:  176 17:54:08    -3930.141113        1.837680\n",
      "FIRE:  177 17:54:11    -3930.810791        1.586902\n",
      "FIRE:  178 17:54:14    -3931.303711        1.333694\n",
      "FIRE:  179 17:54:16    -3931.650146        1.152626\n",
      "FIRE:  180 17:54:19    -3931.880859        1.090964\n",
      "FIRE:  181 17:54:22    -3932.018066        1.128020\n",
      "FIRE:  182 17:54:24    -3932.088867        1.521806\n",
      "FIRE:  183 17:54:27    -3932.128906        1.899489\n",
      "FIRE:  184 17:54:30    -3932.166748        2.130951\n",
      "FIRE:  185 17:54:33    -3932.233154        2.165086\n",
      "FIRE:  186 17:54:36    -3932.333252        1.999896\n",
      "FIRE:  187 17:54:39    -3932.470947        1.677319\n",
      "FIRE:  188 17:54:42    -3932.622803        1.616345\n",
      "FIRE:  189 17:54:44    -3932.775635        1.753843\n",
      "FIRE:  190 17:54:47    -3932.932129        1.908947\n",
      "FIRE:  191 17:54:49    -3933.096436        2.058324\n",
      "FIRE:  192 17:54:51    -3933.273438        2.154742\n",
      "FIRE:  193 17:54:54    -3933.470947        2.153806\n",
      "FIRE:  194 17:54:56    -3933.686035        2.055221\n",
      "FIRE:  195 17:54:58    -3933.919922        1.887294\n",
      "FIRE:  196 17:55:00    -3934.065918        1.872476\n",
      "FIRE:  197 17:55:03    -3934.045654        1.858279\n",
      "FIRE:  198 17:55:05    -3934.018311        1.789737\n",
      "FIRE:  199 17:55:08    -3933.988525        1.677398\n",
      "FIRE:  200 17:55:11    -3933.957764        1.533685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:55:11    -3940.666504        0.521325\n",
      "FIRE:    1 17:55:12    -3940.683350        0.490062\n",
      "FIRE:    2 17:55:13    -3940.716553        0.432640\n",
      "FIRE:    3 17:55:13    -3940.758301        0.361904\n",
      "FIRE:    4 17:55:14    -3940.799805        0.325827\n",
      "FIRE:    5 17:55:14    -3940.837646        0.274779\n",
      "FIRE:    6 17:55:15    -3940.869629        0.205748\n",
      "FIRE:    7 17:55:15    -3940.892334        0.172087\n",
      "FIRE:    8 17:55:16    -3940.906006        0.158781\n",
      "FIRE:    9 17:55:16    -3940.905518        0.183582\n",
      "FIRE:   10 17:55:17    -3940.906982        0.179354\n",
      "FIRE:   11 17:55:17    -3940.909912        0.171014\n",
      "FIRE:   12 17:55:18    -3940.911865        0.158816\n",
      "FIRE:   13 17:55:18    -3940.915283        0.143134\n",
      "FIRE:   14 17:55:19    -3940.916260        0.124400\n",
      "FIRE:   15 17:55:19    -3940.919922        0.103203\n",
      "FIRE:   16 17:55:20    -3940.922363        0.080162\n",
      "FIRE:   17 17:55:20    -3940.923828        0.053500\n",
      "FIRE:   18 17:55:21    -3940.926025        0.041348\n",
      "FIRE:   19 17:55:21    -3940.927734        0.050361\n",
      "FIRE:   20 17:55:22    -3940.927002        0.049846\n",
      "FIRE:   21 17:55:22    -3940.927979        0.048810\n",
      "FIRE:   22 17:55:23    -3940.927979        0.047278\n",
      "FIRE:   23 17:55:24    -3940.927490        0.045277\n",
      "FIRE:   24 17:55:24    -3940.927490        0.042838\n",
      "FIRE:   25 17:55:25    -3940.927246        0.040016\n",
      "FIRE:   26 17:55:25    -3940.927490        0.036855\n",
      "FIRE:   27 17:55:26    -3940.928711        0.033061\n",
      "FIRE:   28 17:55:26    -3940.927490        0.031114\n",
      "FIRE:   29 17:55:27    -3940.928223        0.030502\n",
      "FIRE:   30 17:55:27    -3940.927734        0.029710\n",
      "FIRE:   31 17:55:28    -3940.929443        0.028607\n",
      "FIRE:   32 17:55:28    -3940.929443        0.026983\n",
      "FIRE:   33 17:55:29    -3940.928955        0.024591\n",
      "FIRE:   34 17:55:29    -3940.927490        0.021166\n",
      "FIRE:   35 17:55:30    -3940.927979        0.016508\n",
      "FIRE:   36 17:55:30    -3940.929932        0.010553\n",
      "FIRE:   37 17:55:31    -3940.929199        0.007596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:55:32    -3940.777588        0.764271\n",
      "FIRE:    1 17:55:32    -3940.804199        0.711747\n",
      "FIRE:    2 17:55:33    -3940.851318        0.614349\n",
      "FIRE:    3 17:55:33    -3940.912109        0.485398\n",
      "FIRE:    4 17:55:34    -3940.969238        0.340497\n",
      "FIRE:    5 17:55:34    -3941.022461        0.256903\n",
      "FIRE:    6 17:55:35    -3941.054443        0.215259\n",
      "FIRE:    7 17:55:36    -3941.070557        0.209980\n",
      "FIRE:    8 17:55:36    -3941.072021        0.255777\n",
      "FIRE:    9 17:55:37    -3941.074219        0.247484\n",
      "FIRE:   10 17:55:37    -3941.075684        0.231222\n",
      "FIRE:   11 17:55:38    -3941.077637        0.207570\n",
      "FIRE:   12 17:55:38    -3941.081055        0.185856\n",
      "FIRE:   13 17:55:40    -3941.086426        0.160747\n",
      "FIRE:   14 17:55:40    -3941.088379        0.132788\n",
      "FIRE:   15 17:55:41    -3941.093018        0.103069\n",
      "FIRE:   16 17:55:42    -3941.094727        0.069773\n",
      "FIRE:   17 17:55:43    -3941.097900        0.049785\n",
      "FIRE:   18 17:55:44    -3941.098389        0.073761\n",
      "FIRE:   19 17:55:46    -3941.098633        0.104156\n",
      "FIRE:   20 17:55:47    -3941.099121        0.102722\n",
      "FIRE:   21 17:55:47    -3941.098389        0.099856\n",
      "FIRE:   22 17:55:48    -3941.098389        0.095637\n",
      "FIRE:   23 17:55:48    -3941.098877        0.090151\n",
      "FIRE:   24 17:55:49    -3941.100342        0.083507\n",
      "FIRE:   25 17:55:50    -3941.100586        0.075857\n",
      "FIRE:   26 17:55:50    -3941.100830        0.067386\n",
      "FIRE:   27 17:55:51    -3941.099609        0.057329\n",
      "FIRE:   28 17:55:51    -3941.100098        0.048294\n",
      "FIRE:   29 17:55:52    -3941.099365        0.040543\n",
      "FIRE:   30 17:55:52    -3941.101074        0.032066\n",
      "FIRE:   31 17:55:53    -3941.101318        0.025370\n",
      "FIRE:   32 17:55:53    -3941.102539        0.032822\n",
      "FIRE:   33 17:55:54    -3941.103271        0.038266\n",
      "FIRE:   34 17:55:54    -3941.102295        0.040656\n",
      "FIRE:   35 17:55:55    -3941.101807        0.039132\n",
      "FIRE:   36 17:55:55    -3941.104248        0.033218\n",
      "FIRE:   37 17:55:56    -3941.104492        0.023126\n",
      "FIRE:   38 17:55:56    -3941.105469        0.016090\n",
      "FIRE:   39 17:55:57    -3941.106445        0.021557\n",
      "FIRE:   40 17:55:58    -3941.107910        0.022845\n",
      "FIRE:   41 17:55:58    -3941.107422        0.019203\n",
      "FIRE:   42 17:55:59    -3941.108154        0.018280\n",
      "FIRE:   43 17:55:59    -3941.108154        0.010139\n",
      "FIRE:   44 17:56:01    -3941.109131        0.014439\n",
      "FIRE:   45 17:56:02    -3941.108398        0.013685\n",
      "FIRE:   46 17:56:02    -3941.109863        0.012222\n",
      "FIRE:   47 17:56:03    -3941.110107        0.009101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:56:07    -3935.783447        9.227738\n",
      "FIRE:    1 17:56:10    -3937.023926        5.000568\n",
      "FIRE:    2 17:56:13    -3937.801514        3.581997\n",
      "FIRE:    3 17:56:16    -3938.272949        2.878772\n",
      "FIRE:    4 17:56:18    -3938.553223        1.890334\n",
      "FIRE:    5 17:56:21    -3938.687744        1.465745\n",
      "FIRE:    6 17:56:24    -3938.717773        1.794376\n",
      "FIRE:    7 17:56:27    -3938.693604        1.755683\n",
      "FIRE:    8 17:56:30    -3938.713623        1.679938\n",
      "FIRE:    9 17:56:32    -3938.749756        1.532350\n",
      "FIRE:   10 17:56:35    -3938.796875        1.320337\n",
      "FIRE:   11 17:56:38    -3938.846924        1.054192\n",
      "FIRE:   12 17:56:41    -3938.893066        0.746739\n",
      "FIRE:   13 17:56:44    -3938.930176        0.414189\n",
      "FIRE:   14 17:56:46    -3938.950928        0.164130\n",
      "FIRE:   15 17:56:49    -3938.960449        0.334364\n",
      "FIRE:   16 17:56:52    -3938.954102        0.565408\n",
      "FIRE:   17 17:56:56    -3938.954102        0.554662\n",
      "FIRE:   18 17:56:59    -3938.955566        0.533435\n",
      "FIRE:   19 17:57:02    -3938.957275        0.502252\n",
      "FIRE:   20 17:57:04    -3938.961670        0.461881\n",
      "FIRE:   21 17:57:07    -3938.965088        0.413363\n",
      "FIRE:   22 17:57:10    -3938.967773        0.357971\n",
      "FIRE:   23 17:57:13    -3938.971924        0.297234\n",
      "FIRE:   24 17:57:15    -3938.975342        0.249451\n",
      "FIRE:   25 17:57:18    -3938.979492        0.214059\n",
      "FIRE:   26 17:57:21    -3938.982178        0.171453\n",
      "FIRE:   27 17:57:26    -3938.984375        0.126861\n",
      "FIRE:   28 17:57:30    -3938.987305        0.139603\n",
      "FIRE:   29 17:57:35    -3938.989258        0.213558\n",
      "FIRE:   30 17:57:37    -3938.991211        0.261327\n",
      "FIRE:   31 17:57:40    -3938.992432        0.273050\n",
      "FIRE:   32 17:57:43    -3938.996338        0.241700\n",
      "FIRE:   33 17:57:46    -3939.001221        0.165711\n",
      "FIRE:   34 17:57:49    -3939.004395        0.099368\n",
      "FIRE:   35 17:57:51    -3939.007324        0.130110\n",
      "FIRE:   36 17:57:53    -3939.007080        0.155888\n",
      "FIRE:   37 17:57:56    -3939.006836        0.143155\n",
      "FIRE:   38 17:57:58    -3939.006836        0.164476\n",
      "FIRE:   39 17:58:01    -3939.008057        0.144563\n",
      "FIRE:   40 17:58:04    -3939.009766        0.144876\n",
      "FIRE:   41 17:58:06    -3939.009277        0.129625\n",
      "FIRE:   42 17:58:08    -3939.010986        0.100888\n",
      "FIRE:   43 17:58:10    -3939.010986        0.061937\n",
      "FIRE:   44 17:58:11    -3939.010986        0.036928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:58:12    -3940.666504        0.521325\n",
      "FIRE:    1 17:58:12    -3940.683350        0.490063\n",
      "FIRE:    2 17:58:13    -3940.716553        0.432641\n",
      "FIRE:    3 17:58:14    -3940.758545        0.361904\n",
      "FIRE:    4 17:58:14    -3940.799805        0.325827\n",
      "FIRE:    5 17:58:15    -3940.837402        0.274776\n",
      "FIRE:    6 17:58:15    -3940.869629        0.205748\n",
      "FIRE:    7 17:58:16    -3940.892334        0.172087\n",
      "FIRE:    8 17:58:16    -3940.906006        0.158781\n",
      "FIRE:    9 17:58:17    -3940.905518        0.183581\n",
      "FIRE:   10 17:58:17    -3940.906982        0.179350\n",
      "FIRE:   11 17:58:18    -3940.909668        0.171013\n",
      "FIRE:   12 17:58:18    -3940.911865        0.158816\n",
      "FIRE:   13 17:58:19    -3940.915283        0.143133\n",
      "FIRE:   14 17:58:19    -3940.916260        0.124400\n",
      "FIRE:   15 17:58:20    -3940.919922        0.103203\n",
      "FIRE:   16 17:58:20    -3940.922119        0.080162\n",
      "FIRE:   17 17:58:21    -3940.923828        0.053499\n",
      "FIRE:   18 17:58:22    -3940.926025        0.041348\n",
      "FIRE:   19 17:58:22    -3940.927490        0.050361\n",
      "FIRE:   20 17:58:23    -3940.927002        0.049845\n",
      "FIRE:   21 17:58:23    -3940.927979        0.048809\n",
      "FIRE:   22 17:58:24    -3940.927979        0.047279\n",
      "FIRE:   23 17:58:24    -3940.927490        0.045278\n",
      "FIRE:   24 17:58:25    -3940.927490        0.042839\n",
      "FIRE:   25 17:58:25    -3940.927246        0.040016\n",
      "FIRE:   26 17:58:26    -3940.927734        0.036852\n",
      "FIRE:   27 17:58:26    -3940.928955        0.033060\n",
      "FIRE:   28 17:58:27    -3940.927979        0.031113\n",
      "FIRE:   29 17:58:27    -3940.928223        0.030501\n",
      "FIRE:   30 17:58:28    -3940.927734        0.029710\n",
      "FIRE:   31 17:58:28    -3940.929443        0.028607\n",
      "FIRE:   32 17:58:29    -3940.929199        0.026982\n",
      "FIRE:   33 17:58:29    -3940.928955        0.024590\n",
      "FIRE:   34 17:58:30    -3940.927246        0.021170\n",
      "FIRE:   35 17:58:31    -3940.927979        0.016506\n",
      "FIRE:   36 17:58:31    -3940.930176        0.010553\n",
      "FIRE:   37 17:58:32    -3940.929443        0.007594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:58:32    -3940.700684        0.592498\n",
      "FIRE:    1 17:58:33    -3940.721191        0.561888\n",
      "FIRE:    2 17:58:33    -3940.760742        0.505123\n",
      "FIRE:    3 17:58:34    -3940.807617        0.430713\n",
      "FIRE:    4 17:58:34    -3940.859131        0.350559\n",
      "FIRE:    5 17:58:35    -3940.909180        0.300133\n",
      "FIRE:    6 17:58:36    -3940.950684        0.262261\n",
      "FIRE:    7 17:58:36    -3940.980713        0.237939\n",
      "FIRE:    8 17:58:37    -3940.999756        0.201862\n",
      "FIRE:    9 17:58:37    -3941.003906        0.174222\n",
      "FIRE:   10 17:58:38    -3941.004395        0.171131\n",
      "FIRE:   11 17:58:38    -3941.006592        0.165032\n",
      "FIRE:   12 17:58:39    -3941.010742        0.156058\n",
      "FIRE:   13 17:58:40    -3941.013184        0.144424\n",
      "FIRE:   14 17:58:41    -3941.017334        0.130397\n",
      "FIRE:   15 17:58:42    -3941.020752        0.114285\n",
      "FIRE:   16 17:58:43    -3941.022461        0.096441\n",
      "FIRE:   17 17:58:43    -3941.025391        0.075261\n",
      "FIRE:   18 17:58:44    -3941.028076        0.069892\n",
      "FIRE:   19 17:58:44    -3941.027832        0.070931\n",
      "FIRE:   20 17:58:45    -3941.030029        0.074008\n",
      "FIRE:   21 17:58:45    -3941.029541        0.084200\n",
      "FIRE:   22 17:58:46    -3941.029785        0.083270\n",
      "FIRE:   23 17:58:46    -3941.028320        0.081434\n",
      "FIRE:   24 17:58:47    -3941.029541        0.078718\n",
      "FIRE:   25 17:58:47    -3941.030518        0.075168\n",
      "FIRE:   26 17:58:48    -3941.031982        0.070852\n",
      "FIRE:   27 17:58:48    -3941.033203        0.065868\n",
      "FIRE:   28 17:58:49    -3941.032959        0.060297\n",
      "FIRE:   29 17:58:50    -3941.032715        0.054604\n",
      "FIRE:   30 17:58:51    -3941.033203        0.050576\n",
      "FIRE:   31 17:58:51    -3941.033936        0.046737\n",
      "FIRE:   32 17:58:52    -3941.035645        0.043429\n",
      "FIRE:   33 17:58:52    -3941.035400        0.040738\n",
      "FIRE:   34 17:58:53    -3941.036621        0.038250\n",
      "FIRE:   35 17:58:53    -3941.036377        0.035190\n",
      "FIRE:   36 17:58:54    -3941.037842        0.033451\n",
      "FIRE:   37 17:58:54    -3941.039551        0.033835\n",
      "FIRE:   38 17:58:55    -3941.038330        0.030500\n",
      "FIRE:   39 17:58:56    -3941.040771        0.022943\n",
      "FIRE:   40 17:58:56    -3941.040283        0.019691\n",
      "FIRE:   41 17:58:57    -3941.041992        0.020919\n",
      "FIRE:   42 17:58:57    -3941.043213        0.016883\n",
      "FIRE:   43 17:58:58    -3941.043213        0.018058\n",
      "FIRE:   44 17:58:58    -3941.043457        0.014158\n",
      "FIRE:   45 17:58:59    -3941.042480        0.013118\n",
      "FIRE:   46 17:58:59    -3941.042725        0.012170\n",
      "FIRE:   47 17:59:00    -3941.043213        0.010422\n",
      "FIRE:   48 17:59:00    -3941.043701        0.008142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 17:59:05    -3935.490723        9.321303\n",
      "FIRE:    1 17:59:07    -3936.783691        5.303261\n",
      "FIRE:    2 17:59:10    -3937.641846        3.829739\n",
      "FIRE:    3 17:59:13    -3938.179688        2.878039\n",
      "FIRE:    4 17:59:15    -3938.499756        1.832629\n",
      "FIRE:    5 17:59:18    -3938.646240        1.198560\n",
      "FIRE:    6 17:59:21    -3938.675293        1.646086\n",
      "FIRE:    7 17:59:23    -3938.644775        1.724527\n",
      "FIRE:    8 17:59:26    -3938.664551        1.645100\n",
      "FIRE:    9 17:59:29    -3938.699707        1.491700\n",
      "FIRE:   10 17:59:32    -3938.745117        1.274472\n",
      "FIRE:   11 17:59:35    -3938.796387        1.007004\n",
      "FIRE:   12 17:59:38    -3938.839844        0.705466\n",
      "FIRE:   13 17:59:43    -3938.874512        0.390373\n",
      "FIRE:   14 17:59:46    -3938.892578        0.197274\n",
      "FIRE:   15 17:59:49    -3938.900635        0.305167\n",
      "FIRE:   16 17:59:52    -3938.896484        0.539871\n",
      "FIRE:   17 17:59:54    -3938.886230        0.718545\n",
      "FIRE:   18 17:59:58    -3938.888428        0.704519\n",
      "FIRE:   19 18:00:02    -3938.890137        0.676785\n",
      "FIRE:   20 18:00:05    -3938.895508        0.635937\n",
      "FIRE:   21 18:00:07    -3938.900391        0.582961\n",
      "FIRE:   22 18:00:10    -3938.905273        0.519082\n",
      "FIRE:   23 18:00:13    -3938.911377        0.445839\n",
      "FIRE:   24 18:00:16    -3938.918213        0.365098\n",
      "FIRE:   25 18:00:18    -3938.923340        0.269789\n",
      "FIRE:   26 18:00:21    -3938.927734        0.186614\n",
      "FIRE:   27 18:00:24    -3938.932861        0.113579\n",
      "FIRE:   28 18:00:27    -3938.934814        0.094487\n",
      "FIRE:   29 18:00:30    -3938.934570        0.195145\n",
      "FIRE:   30 18:00:33    -3938.931641        0.277521\n",
      "FIRE:   31 18:00:37    -3938.931641        0.322012\n",
      "FIRE:   32 18:00:41    -3938.935303        0.318651\n",
      "FIRE:   33 18:00:45    -3938.938477        0.262136\n",
      "FIRE:   34 18:00:49    -3938.944092        0.158419\n",
      "FIRE:   35 18:00:52    -3938.947510        0.106368\n",
      "FIRE:   36 18:00:55    -3938.946045        0.145126\n",
      "FIRE:   37 18:00:56    -3938.947021        0.137143\n",
      "FIRE:   38 18:00:58    -3938.947754        0.122286\n",
      "FIRE:   39 18:01:01    -3938.947754        0.102510\n",
      "FIRE:   40 18:01:03    -3938.947998        0.084384\n",
      "FIRE:   41 18:01:06    -3938.947021        0.081834\n",
      "FIRE:   42 18:01:06    -3938.948242        0.080438\n",
      "FIRE:   43 18:01:07    -3938.949707        0.080239\n",
      "FIRE:   44 18:01:08    -3938.948975        0.080583\n",
      "FIRE:   45 18:01:09    -3938.947998        0.080486\n",
      "FIRE:   46 18:01:10    -3938.948975        0.079093\n",
      "FIRE:   47 18:01:11    -3938.948975        0.075400\n",
      "FIRE:   48 18:01:11    -3938.948975        0.069866\n",
      "FIRE:   49 18:01:12    -3938.949219        0.064113\n",
      "FIRE:   50 18:01:12    -3938.949463        0.058991\n",
      "FIRE:   51 18:01:13    -3938.949707        0.051117\n",
      "FIRE:   52 18:01:13    -3938.947998        0.049913\n",
      "FIRE:   53 18:01:13    -3938.947998        0.049913\n",
      "FIRE:   54 18:01:13    -3938.947998        0.049913\n",
      "FIRE:   55 18:01:13    -3938.947998        0.049913\n",
      "FIRE:   56 18:01:13    -3938.947998        0.049913\n",
      "FIRE:   57 18:01:13    -3938.947998        0.049913\n",
      "FIRE:   58 18:01:13    -3938.947998        0.049913\n",
      "FIRE:   59 18:01:13    -3938.947998        0.049913\n",
      "FIRE:   60 18:01:13    -3938.947998        0.049913\n",
      "FIRE:   61 18:01:13    -3938.947998        0.049913\n",
      "FIRE:   62 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   63 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   64 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   65 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   66 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   67 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   68 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   69 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   70 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   71 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   72 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   73 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   74 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   75 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   76 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   77 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   78 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   79 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   80 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   81 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   82 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   83 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   84 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   85 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   86 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   87 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   88 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   89 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   90 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   91 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   92 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   93 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   94 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   95 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   96 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   97 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   98 18:01:14    -3938.947998        0.049913\n",
      "FIRE:   99 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  100 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  101 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  102 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  103 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  104 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  105 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  106 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  107 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  108 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  109 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  110 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  111 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  112 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  113 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  114 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  115 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  116 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  117 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  118 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  119 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  120 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  121 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  122 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  123 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  124 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  125 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  126 18:01:14    -3938.947998        0.049913\n",
      "FIRE:  127 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  128 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  129 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  130 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  131 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  132 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  133 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  134 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  135 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  136 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  137 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  138 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  139 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  140 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  141 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  142 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  143 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  144 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  145 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  146 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  147 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  148 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  149 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  150 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  151 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  152 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  153 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  154 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  155 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  156 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  157 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  158 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  159 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  160 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  161 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  162 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  163 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  164 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  165 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  166 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  167 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  168 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  169 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  170 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  171 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  172 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  173 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  174 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  175 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  176 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  177 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  178 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  179 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  180 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  181 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  182 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  183 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  184 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  185 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  186 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  187 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  188 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  189 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  190 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  191 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  192 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  193 18:01:15    -3938.947998        0.049913\n",
      "FIRE:  194 18:01:16    -3938.947998        0.049913\n",
      "FIRE:  195 18:01:16    -3938.947998        0.049913\n",
      "FIRE:  196 18:01:16    -3938.947998        0.049913\n",
      "FIRE:  197 18:01:16    -3938.947998        0.049913\n",
      "FIRE:  198 18:01:16    -3938.947998        0.049913\n",
      "FIRE:  199 18:01:16    -3938.947998        0.049913\n",
      "FIRE:  200 18:01:16    -3938.947998        0.049913\n",
      "Progress: 24/42 calculations completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 18:01:16    -3940.666504        0.521325\n",
      "FIRE:    1 18:01:17    -3940.683350        0.490062\n",
      "FIRE:    2 18:01:17    -3940.716553        0.432639\n",
      "FIRE:    3 18:01:18    -3940.758301        0.361904\n",
      "FIRE:    4 18:01:19    -3940.799805        0.325828\n",
      "FIRE:    5 18:01:19    -3940.837402        0.274776\n",
      "FIRE:    6 18:01:20    -3940.869141        0.205748\n",
      "FIRE:    7 18:01:20    -3940.892334        0.172087\n",
      "FIRE:    8 18:01:21    -3940.906006        0.158781\n",
      "FIRE:    9 18:01:21    -3940.905762        0.183580\n",
      "FIRE:   10 18:01:22    -3940.907471        0.179352\n",
      "FIRE:   11 18:01:22    -3940.909912        0.171014\n",
      "FIRE:   12 18:01:23    -3940.911865        0.158817\n",
      "FIRE:   13 18:01:23    -3940.915283        0.143134\n",
      "FIRE:   14 18:01:24    -3940.916016        0.124401\n",
      "FIRE:   15 18:01:24    -3940.919922        0.103204\n",
      "FIRE:   16 18:01:25    -3940.922119        0.080162\n",
      "FIRE:   17 18:01:25    -3940.923828        0.053498\n",
      "FIRE:   18 18:01:26    -3940.926025        0.041350\n",
      "FIRE:   19 18:01:26    -3940.927490        0.050360\n",
      "FIRE:   20 18:01:27    -3940.927490        0.049844\n",
      "FIRE:   21 18:01:27    -3940.927979        0.048811\n",
      "FIRE:   22 18:01:28    -3940.928467        0.047278\n",
      "FIRE:   23 18:01:28    -3940.927246        0.045276\n",
      "FIRE:   24 18:01:29    -3940.927490        0.042840\n",
      "FIRE:   25 18:01:29    -3940.927002        0.040015\n",
      "FIRE:   26 18:01:30    -3940.927490        0.036851\n",
      "FIRE:   27 18:01:31    -3940.928711        0.033056\n",
      "FIRE:   28 18:01:31    -3940.927246        0.031112\n",
      "FIRE:   29 18:01:32    -3940.928223        0.030503\n",
      "FIRE:   30 18:01:32    -3940.927734        0.029710\n",
      "FIRE:   31 18:01:33    -3940.929443        0.028605\n",
      "FIRE:   32 18:01:33    -3940.929199        0.026983\n",
      "FIRE:   33 18:01:34    -3940.928955        0.024588\n",
      "FIRE:   34 18:01:34    -3940.927246        0.021168\n",
      "FIRE:   35 18:01:35    -3940.927979        0.016508\n",
      "FIRE:   36 18:01:36    -3940.930176        0.010555\n",
      "FIRE:   37 18:01:36    -3940.929443        0.007595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 18:01:37    -3940.763672        0.627402\n",
      "FIRE:    1 18:01:38    -3940.787354        0.582082\n",
      "FIRE:    2 18:01:39    -3940.828125        0.541407\n",
      "FIRE:    3 18:01:41    -3940.883545        0.507324\n",
      "FIRE:    4 18:01:41    -3940.939697        0.457275\n",
      "FIRE:    5 18:01:42    -3940.990723        0.386965\n",
      "FIRE:    6 18:01:42    -3941.032471        0.294059\n",
      "FIRE:    7 18:01:43    -3941.057861        0.184968\n",
      "FIRE:    8 18:01:44    -3941.066895        0.201372\n",
      "FIRE:    9 18:01:44    -3941.067139        0.195286\n",
      "FIRE:   10 18:01:45    -3941.069092        0.183314\n",
      "FIRE:   11 18:01:45    -3941.069824        0.165843\n",
      "FIRE:   12 18:01:46    -3941.073486        0.143454\n",
      "FIRE:   13 18:01:47    -3941.075684        0.116938\n",
      "FIRE:   14 18:01:48    -3941.079590        0.094618\n",
      "FIRE:   15 18:01:48    -3941.082764        0.078162\n",
      "FIRE:   16 18:01:49    -3941.083252        0.068644\n",
      "FIRE:   17 18:01:50    -3941.084961        0.068885\n",
      "FIRE:   18 18:01:50    -3941.086670        0.070650\n",
      "FIRE:   19 18:01:51    -3941.087646        0.085818\n",
      "FIRE:   20 18:01:52    -3941.087646        0.084646\n",
      "FIRE:   21 18:01:53    -3941.087646        0.082321\n",
      "FIRE:   22 18:01:53    -3941.087646        0.078890\n",
      "FIRE:   23 18:01:54    -3941.088135        0.074408\n",
      "FIRE:   24 18:01:55    -3941.089111        0.068974\n",
      "FIRE:   25 18:01:55    -3941.088867        0.066516\n",
      "FIRE:   26 18:01:56    -3941.089355        0.064485\n",
      "FIRE:   27 18:01:56    -3941.089355        0.061964\n",
      "FIRE:   28 18:01:57    -3941.088379        0.058904\n",
      "FIRE:   29 18:01:57    -3941.088623        0.055254\n",
      "FIRE:   30 18:01:58    -3941.090332        0.050988\n",
      "FIRE:   31 18:01:58    -3941.091064        0.046162\n",
      "FIRE:   32 18:01:59    -3941.090332        0.040914\n",
      "FIRE:   33 18:01:59    -3941.092285        0.035419\n",
      "FIRE:   34 18:02:00    -3941.092773        0.031686\n",
      "FIRE:   35 18:02:00    -3941.092285        0.030183\n",
      "FIRE:   36 18:02:01    -3941.093262        0.026101\n",
      "FIRE:   37 18:02:01    -3941.093018        0.019457\n",
      "FIRE:   38 18:02:02    -3941.093506        0.015152\n",
      "FIRE:   39 18:02:03    -3941.093994        0.020813\n",
      "FIRE:   40 18:02:03    -3941.095215        0.020841\n",
      "FIRE:   41 18:02:04    -3941.094727        0.011872\n",
      "FIRE:   42 18:02:04    -3941.096924        0.009359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 18:02:09    -3936.558594        5.071737\n",
      "FIRE:    1 18:02:12    -3937.279785        4.120524\n",
      "FIRE:    2 18:02:15    -3938.114258        2.948197\n",
      "FIRE:    3 18:02:18    -3938.686279        1.920880\n",
      "FIRE:    4 18:02:21    -3939.025879        1.348677\n",
      "FIRE:    5 18:02:23    -3939.170898        0.854736\n",
      "FIRE:    6 18:02:26    -3939.189209        0.627711\n",
      "FIRE:    7 18:02:29    -3939.143066        0.874175\n",
      "FIRE:    8 18:02:32    -3939.154541        0.843958\n",
      "FIRE:    9 18:02:34    -3939.172607        0.784746\n",
      "FIRE:   10 18:02:37    -3939.199951        0.698972\n",
      "FIRE:   11 18:02:40    -3939.231201        0.590144\n",
      "FIRE:   12 18:02:43    -3939.259521        0.462884\n",
      "FIRE:   13 18:02:45    -3939.284424        0.322810\n",
      "FIRE:   14 18:02:48    -3939.302979        0.232474\n",
      "FIRE:   15 18:02:51    -3939.315674        0.182673\n",
      "FIRE:   16 18:02:55    -3939.317383        0.157338\n",
      "FIRE:   17 18:02:59    -3939.312500        0.288073\n",
      "FIRE:   18 18:03:03    -3939.304932        0.391505\n",
      "FIRE:   19 18:03:06    -3939.300781        0.435866\n",
      "FIRE:   20 18:03:09    -3939.306152        0.406505\n",
      "FIRE:   21 18:03:12    -3939.315674        0.299417\n",
      "FIRE:   22 18:03:15    -3939.323242        0.228347\n",
      "FIRE:   23 18:03:17    -3939.320312        0.203955\n",
      "FIRE:   24 18:03:20    -3939.322021        0.202211\n",
      "FIRE:   25 18:03:23    -3939.323242        0.198830\n",
      "FIRE:   26 18:03:26    -3939.322510        0.194004\n",
      "FIRE:   27 18:03:29    -3939.323975        0.188011\n",
      "FIRE:   28 18:03:32    -3939.325928        0.181126\n",
      "FIRE:   29 18:03:35    -3939.326416        0.173553\n",
      "FIRE:   30 18:03:36    -3939.326660        0.165108\n",
      "FIRE:   31 18:03:38    -3939.327393        0.154351\n",
      "FIRE:   32 18:03:39    -3939.326416        0.140969\n",
      "FIRE:   33 18:03:41    -3939.325195        0.120501\n",
      "FIRE:   34 18:03:43    -3939.324219        0.093502\n",
      "FIRE:   35 18:03:45    -3939.322998        0.062256\n",
      "FIRE:   36 18:03:47    -3939.322754        0.067648\n",
      "FIRE:   37 18:03:48    -3939.322754        0.056129\n",
      "FIRE:   38 18:03:49    -3939.322754        0.039921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 18:03:51    -3940.666504        0.521325\n",
      "FIRE:    1 18:03:51    -3940.683350        0.490063\n",
      "FIRE:    2 18:03:52    -3940.716553        0.432639\n",
      "FIRE:    3 18:03:52    -3940.758545        0.361904\n",
      "FIRE:    4 18:03:53    -3940.799805        0.325827\n",
      "FIRE:    5 18:03:53    -3940.837402        0.274776\n",
      "FIRE:    6 18:03:54    -3940.869629        0.205748\n",
      "FIRE:    7 18:03:54    -3940.892334        0.172088\n",
      "FIRE:    8 18:03:55    -3940.905762        0.158783\n",
      "FIRE:    9 18:03:55    -3940.905518        0.183579\n",
      "FIRE:   10 18:03:56    -3940.906982        0.179351\n",
      "FIRE:   11 18:03:57    -3940.909912        0.171009\n",
      "FIRE:   12 18:03:57    -3940.911865        0.158817\n",
      "FIRE:   13 18:03:58    -3940.915283        0.143133\n",
      "FIRE:   14 18:03:58    -3940.916260        0.124401\n",
      "FIRE:   15 18:03:59    -3940.919922        0.103208\n",
      "FIRE:   16 18:04:00    -3940.922119        0.080161\n",
      "FIRE:   17 18:04:00    -3940.923584        0.053501\n",
      "FIRE:   18 18:04:01    -3940.925781        0.041349\n",
      "FIRE:   19 18:04:01    -3940.927490        0.050361\n",
      "FIRE:   20 18:04:02    -3940.927002        0.049846\n",
      "FIRE:   21 18:04:02    -3940.928223        0.048811\n",
      "FIRE:   22 18:04:03    -3940.928223        0.047278\n",
      "FIRE:   23 18:04:04    -3940.927246        0.045277\n",
      "FIRE:   24 18:04:04    -3940.927490        0.042840\n",
      "FIRE:   25 18:04:05    -3940.927002        0.040016\n",
      "FIRE:   26 18:04:05    -3940.927734        0.036854\n",
      "FIRE:   27 18:04:06    -3940.928711        0.033061\n",
      "FIRE:   28 18:04:06    -3940.927734        0.031113\n",
      "FIRE:   29 18:04:07    -3940.928467        0.030502\n",
      "FIRE:   30 18:04:08    -3940.927490        0.029710\n",
      "FIRE:   31 18:04:08    -3940.929443        0.028607\n",
      "FIRE:   32 18:04:09    -3940.929443        0.026983\n",
      "FIRE:   33 18:04:09    -3940.928955        0.024590\n",
      "FIRE:   34 18:04:10    -3940.927734        0.021167\n",
      "FIRE:   35 18:04:10    -3940.927979        0.016508\n",
      "FIRE:   36 18:04:11    -3940.930176        0.010553\n",
      "FIRE:   37 18:04:11    -3940.929199        0.007594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 18:04:12    -3940.422852        0.628415\n",
      "FIRE:    1 18:04:13    -3940.451416        0.578535\n",
      "FIRE:    2 18:04:13    -3940.501221        0.501885\n",
      "FIRE:    3 18:04:14    -3940.561279        0.465146\n",
      "FIRE:    4 18:04:14    -3940.619873        0.416751\n",
      "FIRE:    5 18:04:15    -3940.669922        0.355592\n",
      "FIRE:    6 18:04:15    -3940.709229        0.295420\n",
      "FIRE:    7 18:04:16    -3940.737793        0.273007\n",
      "FIRE:    8 18:04:17    -3940.761230        0.261956\n",
      "FIRE:    9 18:04:17    -3940.781006        0.298084\n",
      "FIRE:   10 18:04:18    -3940.791016        0.296935\n",
      "FIRE:   11 18:04:18    -3940.792725        0.287189\n",
      "FIRE:   12 18:04:19    -3940.797119        0.268178\n",
      "FIRE:   13 18:04:19    -3940.803711        0.240829\n",
      "FIRE:   14 18:04:20    -3940.808594        0.206533\n",
      "FIRE:   15 18:04:20    -3940.815430        0.167129\n",
      "FIRE:   16 18:04:21    -3940.821289        0.128291\n",
      "FIRE:   17 18:04:21    -3940.826660        0.110150\n",
      "FIRE:   18 18:04:22    -3940.830078        0.090904\n",
      "FIRE:   19 18:04:22    -3940.833496        0.072852\n",
      "FIRE:   20 18:04:23    -3940.834473        0.073840\n",
      "FIRE:   21 18:04:23    -3940.833496        0.094683\n",
      "FIRE:   22 18:04:24    -3940.832764        0.092863\n",
      "FIRE:   23 18:04:25    -3940.833496        0.089275\n",
      "FIRE:   24 18:04:26    -3940.833984        0.084019\n",
      "FIRE:   25 18:04:26    -3940.834473        0.077235\n",
      "FIRE:   26 18:04:27    -3940.835205        0.069141\n",
      "FIRE:   27 18:04:28    -3940.836670        0.060029\n",
      "FIRE:   28 18:04:29    -3940.836670        0.050297\n",
      "FIRE:   29 18:04:29    -3940.837891        0.040706\n",
      "FIRE:   30 18:04:30    -3940.839844        0.037455\n",
      "FIRE:   31 18:04:31    -3940.839844        0.034386\n",
      "FIRE:   32 18:04:32    -3940.839355        0.030915\n",
      "FIRE:   33 18:04:33    -3940.840332        0.035954\n",
      "FIRE:   34 18:04:34    -3940.840088        0.043226\n",
      "FIRE:   35 18:04:34    -3940.841064        0.045976\n",
      "FIRE:   36 18:04:35    -3940.842041        0.042559\n",
      "FIRE:   37 18:04:35    -3940.841309        0.032459\n",
      "FIRE:   38 18:04:36    -3940.843262        0.024813\n",
      "FIRE:   39 18:04:37    -3940.844482        0.014773\n",
      "FIRE:   40 18:04:37    -3940.843262        0.022924\n",
      "FIRE:   41 18:04:38    -3940.844238        0.031309\n",
      "FIRE:   42 18:04:38    -3940.843994        0.030576\n",
      "FIRE:   43 18:04:39    -3940.843506        0.029157\n",
      "FIRE:   44 18:04:40    -3940.845703        0.027125\n",
      "FIRE:   45 18:04:40    -3940.844482        0.024613\n",
      "FIRE:   46 18:04:41    -3940.844971        0.021758\n",
      "FIRE:   47 18:04:41    -3940.844482        0.018736\n",
      "FIRE:   48 18:04:42    -3940.845459        0.015742\n",
      "FIRE:   49 18:04:42    -3940.844482        0.012668\n",
      "FIRE:   50 18:04:43    -3940.845215        0.011153\n",
      "FIRE:   51 18:04:43    -3940.846436        0.012018\n",
      "FIRE:   52 18:04:44    -3940.846436        0.010871\n",
      "FIRE:   53 18:04:44    -3940.844971        0.007610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 18:04:49    -3936.612793        7.738652\n",
      "FIRE:    1 18:04:52    -3937.583740        4.853464\n",
      "FIRE:    2 18:04:54    -3938.359863        3.278211\n",
      "FIRE:    3 18:04:57    -3938.843018        2.657070\n",
      "FIRE:    4 18:05:00    -3939.134766        1.855411\n",
      "FIRE:    5 18:05:05    -3939.297363        1.470839\n",
      "FIRE:    6 18:05:10    -3939.371826        1.742436\n",
      "FIRE:    7 18:05:14    -3939.383057        1.615543\n",
      "FIRE:    8 18:05:18    -3939.370850        1.455926\n",
      "FIRE:    9 18:05:22    -3939.393555        1.383424\n",
      "FIRE:   10 18:05:26    -3939.430908        1.243461\n",
      "FIRE:   11 18:05:30    -3939.479736        1.045326\n",
      "FIRE:   12 18:05:33    -3939.531250        0.801339\n",
      "FIRE:   13 18:05:36    -3939.573730        0.525513\n",
      "FIRE:   14 18:05:40    -3939.603760        0.233822\n",
      "FIRE:   15 18:05:43    -3939.619385        0.166294\n",
      "FIRE:   16 18:05:46    -3939.616699        0.333634\n",
      "FIRE:   17 18:05:49    -3939.609863        0.565936\n",
      "FIRE:   18 18:05:52    -3939.610352        0.556959\n",
      "FIRE:   19 18:05:56    -3939.612549        0.539166\n",
      "FIRE:   20 18:05:59    -3939.614258        0.512867\n",
      "FIRE:   21 18:06:04    -3939.619385        0.478501\n",
      "FIRE:   22 18:06:09    -3939.621338        0.436719\n",
      "FIRE:   23 18:06:12    -3939.627686        0.388251\n",
      "FIRE:   24 18:06:15    -3939.631348        0.334077\n",
      "FIRE:   25 18:06:18    -3939.637207        0.268880\n",
      "FIRE:   26 18:06:22    -3939.639893        0.192506\n",
      "FIRE:   27 18:06:26    -3939.644287        0.127793\n",
      "FIRE:   28 18:06:29    -3939.646484        0.083586\n",
      "FIRE:   29 18:06:32    -3939.647949        0.125486\n",
      "FIRE:   30 18:06:34    -3939.647949        0.168359\n",
      "FIRE:   31 18:06:38    -3939.647949        0.211250\n",
      "FIRE:   32 18:06:41    -3939.650879        0.234019\n",
      "FIRE:   33 18:06:44    -3939.654053        0.214654\n",
      "FIRE:   34 18:06:47    -3939.660889        0.147996\n",
      "FIRE:   35 18:06:49    -3939.665039        0.081779\n",
      "FIRE:   36 18:06:50    -3939.665039        0.096994\n",
      "FIRE:   37 18:06:52    -3939.665039        0.104692\n",
      "FIRE:   38 18:06:54    -3939.665039        0.142073\n",
      "FIRE:   39 18:06:57    -3939.665039        0.127207\n",
      "FIRE:   40 18:06:59    -3939.665039        0.053724\n",
      "FIRE:   41 18:07:00    -3939.665039        0.125451\n",
      "FIRE:   42 18:07:01    -3939.665039        0.155479\n",
      "FIRE:   43 18:07:02    -3939.665039        0.051084\n",
      "FIRE:   44 18:07:03    -3939.665039        0.106212\n",
      "FIRE:   45 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   46 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   47 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   48 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   49 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   50 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   51 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   52 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   53 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   54 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   55 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   56 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   57 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   58 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   59 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   60 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   61 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   62 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   63 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   64 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   65 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   66 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   67 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   68 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   69 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   70 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   71 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   72 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   73 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   74 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   75 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   76 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   77 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   78 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   79 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   80 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   81 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   82 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   83 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   84 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   85 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   86 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   87 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   88 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   89 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   90 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   91 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   92 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   93 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   94 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   95 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   96 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   97 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   98 18:07:04    -3939.665039        0.043931\n",
      "FIRE:   99 18:07:04    -3939.665039        0.043931\n",
      "FIRE:  100 18:07:04    -3939.665039        0.043931\n",
      "FIRE:  101 18:07:04    -3939.665039        0.043931\n",
      "FIRE:  102 18:07:04    -3939.665039        0.043931\n",
      "FIRE:  103 18:07:04    -3939.665039        0.043931\n",
      "FIRE:  104 18:07:04    -3939.665039        0.043931\n",
      "FIRE:  105 18:07:04    -3939.665039        0.043931\n",
      "FIRE:  106 18:07:04    -3939.665039        0.043931\n",
      "FIRE:  107 18:07:04    -3939.665039        0.043931\n",
      "FIRE:  108 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  109 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  110 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  111 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  112 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  113 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  114 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  115 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  116 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  117 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  118 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  119 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  120 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  121 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  122 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  123 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  124 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  125 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  126 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  127 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  128 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  129 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  130 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  131 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  132 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  133 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  134 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  135 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  136 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  137 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  138 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  139 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  140 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  141 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  142 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  143 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  144 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  145 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  146 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  147 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  148 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  149 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  150 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  151 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  152 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  153 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  154 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  155 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  156 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  157 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  158 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  159 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  160 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  161 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  162 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  163 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  164 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  165 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  166 18:07:05    -3939.665039        0.043931\n",
      "FIRE:  167 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  168 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  169 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  170 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  171 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  172 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  173 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  174 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  175 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  176 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  177 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  178 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  179 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  180 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  181 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  182 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  183 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  184 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  185 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  186 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  187 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  188 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  189 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  190 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  191 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  192 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  193 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  194 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  195 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  196 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  197 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  198 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  199 18:07:06    -3939.665039        0.043931\n",
      "FIRE:  200 18:07:06    -3939.665039        0.043931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 18:07:07    -3940.666504        0.521325\n",
      "FIRE:    1 18:07:07    -3940.683350        0.490062\n",
      "FIRE:    2 18:07:08    -3940.716553        0.432640\n",
      "FIRE:    3 18:07:08    -3940.758545        0.361904\n",
      "FIRE:    4 18:07:09    -3940.799805        0.325828\n",
      "FIRE:    5 18:07:10    -3940.837402        0.274775\n",
      "FIRE:    6 18:07:10    -3940.869141        0.205748\n",
      "FIRE:    7 18:07:11    -3940.892578        0.172088\n",
      "FIRE:    8 18:07:11    -3940.906006        0.158781\n",
      "FIRE:    9 18:07:12    -3940.905762        0.183580\n",
      "FIRE:   10 18:07:12    -3940.907471        0.179347\n",
      "FIRE:   11 18:07:13    -3940.909912        0.171010\n",
      "FIRE:   12 18:07:13    -3940.912109        0.158817\n",
      "FIRE:   13 18:07:14    -3940.915283        0.143133\n",
      "FIRE:   14 18:07:14    -3940.916504        0.124401\n",
      "FIRE:   15 18:07:15    -3940.919922        0.103204\n",
      "FIRE:   16 18:07:16    -3940.922119        0.080166\n",
      "FIRE:   17 18:07:16    -3940.923828        0.053501\n",
      "FIRE:   18 18:07:17    -3940.925781        0.041349\n",
      "FIRE:   19 18:07:17    -3940.927490        0.050359\n",
      "FIRE:   20 18:07:18    -3940.927246        0.049844\n",
      "FIRE:   21 18:07:18    -3940.928223        0.048809\n",
      "FIRE:   22 18:07:19    -3940.928223        0.047278\n",
      "FIRE:   23 18:07:19    -3940.927490        0.045277\n",
      "FIRE:   24 18:07:20    -3940.927490        0.042841\n",
      "FIRE:   25 18:07:20    -3940.927246        0.040015\n",
      "FIRE:   26 18:07:21    -3940.927490        0.036853\n",
      "FIRE:   27 18:07:22    -3940.928955        0.033063\n",
      "FIRE:   28 18:07:22    -3940.927734        0.031114\n",
      "FIRE:   29 18:07:23    -3940.928223        0.030502\n",
      "FIRE:   30 18:07:23    -3940.927734        0.029710\n",
      "FIRE:   31 18:07:24    -3940.929688        0.028605\n",
      "FIRE:   32 18:07:24    -3940.929199        0.026983\n",
      "FIRE:   33 18:07:25    -3940.928955        0.024589\n",
      "FIRE:   34 18:07:25    -3940.927246        0.021166\n",
      "FIRE:   35 18:07:26    -3940.927979        0.016508\n",
      "FIRE:   36 18:07:27    -3940.930176        0.010555\n",
      "FIRE:   37 18:07:27    -3940.929443        0.007594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 18:07:28    -3941.008789        0.785275\n",
      "FIRE:    1 18:07:28    -3941.040527        0.761434\n",
      "FIRE:    2 18:07:29    -3941.095459        0.714746\n",
      "FIRE:    3 18:07:29    -3941.167725        0.646516\n",
      "FIRE:    4 18:07:30    -3941.243164        0.557631\n",
      "FIRE:    5 18:07:30    -3941.308838        0.448668\n",
      "FIRE:    6 18:07:31    -3941.355713        0.322052\n",
      "FIRE:    7 18:07:31    -3941.380127        0.188256\n",
      "FIRE:    8 18:07:32    -3941.380615        0.264446\n",
      "FIRE:    9 18:07:33    -3941.382324        0.259133\n",
      "FIRE:   10 18:07:33    -3941.385254        0.248700\n",
      "FIRE:   11 18:07:34    -3941.389893        0.233459\n",
      "FIRE:   12 18:07:34    -3941.392822        0.213904\n",
      "FIRE:   13 18:07:35    -3941.396729        0.190666\n",
      "FIRE:   14 18:07:35    -3941.401855        0.164946\n",
      "FIRE:   15 18:07:36    -3941.407227        0.137443\n",
      "FIRE:   16 18:07:36    -3941.409912        0.105767\n",
      "FIRE:   17 18:07:37    -3941.414795        0.098138\n",
      "FIRE:   18 18:07:37    -3941.415283        0.112837\n",
      "FIRE:   19 18:07:38    -3941.417480        0.123736\n",
      "FIRE:   20 18:07:39    -3941.418701        0.128358\n",
      "FIRE:   21 18:07:40    -3941.420898        0.125382\n",
      "FIRE:   22 18:07:40    -3941.421387        0.114419\n",
      "FIRE:   23 18:07:41    -3941.423828        0.095911\n",
      "FIRE:   24 18:07:41    -3941.427002        0.071369\n",
      "FIRE:   25 18:07:42    -3941.430420        0.048772\n",
      "FIRE:   26 18:07:42    -3941.431396        0.040164\n",
      "FIRE:   27 18:07:43    -3941.431885        0.046879\n",
      "FIRE:   28 18:07:43    -3941.431885        0.045277\n",
      "FIRE:   29 18:07:44    -3941.432861        0.042194\n",
      "FIRE:   30 18:07:44    -3941.433350        0.037823\n",
      "FIRE:   31 18:07:45    -3941.434082        0.032462\n",
      "FIRE:   32 18:07:46    -3941.434814        0.026474\n",
      "FIRE:   33 18:07:46    -3941.436035        0.020317\n",
      "FIRE:   34 18:07:47    -3941.434814        0.015100\n",
      "FIRE:   35 18:07:47    -3941.434326        0.019728\n",
      "FIRE:   36 18:07:48    -3941.434326        0.023058\n",
      "FIRE:   37 18:07:48    -3941.436035        0.023171\n",
      "FIRE:   38 18:07:49    -3941.435791        0.019320\n",
      "FIRE:   39 18:07:50    -3941.436035        0.011445\n",
      "FIRE:   40 18:07:51    -3941.436768        0.009250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 18:07:59    -3751.577881      475.476200\n",
      "FIRE:    1 18:08:04    -3751.696045      381.275703\n",
      "FIRE:    2 18:08:09    -3751.455566      390.729933\n",
      "FIRE:    3 18:08:12    -3750.512939      394.608862\n",
      "FIRE:    4 18:08:16    -3748.709717      390.803620\n",
      "FIRE:    5 18:08:21    -3746.102539      382.110214\n",
      "FIRE:    6 18:08:25    -3742.990723      373.073214\n",
      "FIRE:    7 18:08:30    -3740.721436      360.776481\n",
      "FIRE:    8 18:08:35    -3741.173096      351.307346\n",
      "FIRE:    9 18:08:40    -3744.489502      350.867688\n",
      "FIRE:   10 18:08:45    -3748.170898      363.769857\n",
      "FIRE:   11 18:08:50    -3750.565918      422.189648\n",
      "FIRE:   12 18:08:55    -3750.992188      493.170949\n",
      "FIRE:   13 18:08:59    -3747.694580      548.959547\n",
      "FIRE:   14 18:09:05    -3737.475586      563.536134\n",
      "FIRE:   15 18:09:10    -3718.423340      508.155099\n",
      "FIRE:   16 18:09:14    -3702.574219      334.641975\n",
      "FIRE:   17 18:09:18    -3694.728027      263.049424\n",
      "FIRE:   18 18:09:23    -3693.161133      264.160961\n",
      "FIRE:   19 18:09:26    -3703.332520      344.047384\n",
      "FIRE:   20 18:09:29    -3735.781494      498.366413\n",
      "FIRE:   21 18:09:32    -3786.073975      348.596073\n",
      "FIRE:   22 18:09:35    -3834.684082      260.547776\n",
      "FIRE:   23 18:09:39    -3869.286621      140.674214\n",
      "FIRE:   24 18:09:44    -3889.417969       67.051829\n",
      "FIRE:   25 18:09:47    -3900.144775       33.966306\n",
      "FIRE:   26 18:09:51    -3906.056152       23.114141\n",
      "FIRE:   27 18:09:54    -3909.023193       15.457395\n",
      "FIRE:   28 18:09:59    -3910.015137       19.273717\n",
      "FIRE:   29 18:10:03    -3909.772949       25.954547\n",
      "FIRE:   30 18:10:06    -3908.882568       32.366371\n",
      "FIRE:   31 18:10:08    -3907.932861       36.133698\n",
      "FIRE:   32 18:10:13    -3907.354004       36.351251\n",
      "FIRE:   33 18:10:16    -3907.230713       32.171176\n",
      "FIRE:   34 18:10:19    -3907.399658       25.863722\n",
      "FIRE:   35 18:10:24    -3907.764648       25.854468\n",
      "FIRE:   36 18:10:28    -3908.412354       25.453828\n",
      "FIRE:   37 18:10:33    -3909.350098       22.959075\n",
      "FIRE:   38 18:10:38    -3910.418945       19.186031\n",
      "FIRE:   39 18:10:44    -3911.401123       15.133530\n",
      "FIRE:   40 18:10:47    -3912.161621       16.521732\n",
      "FIRE:   41 18:10:52    -3912.673828       31.154863\n",
      "FIRE:   42 18:10:55    -3912.924316       41.940149\n",
      "FIRE:   43 18:10:57    -3912.935547       25.310663\n",
      "FIRE:   44 18:11:00    -3912.804688       12.576221\n",
      "FIRE:   45 18:11:04    -3912.658691       12.924311\n",
      "FIRE:   46 18:11:07    -3912.468506       15.084945\n",
      "FIRE:   47 18:11:10    -3912.176758       15.769887\n",
      "FIRE:   48 18:11:13    -3911.743652       16.380103\n",
      "FIRE:   49 18:11:16    -3911.135498       17.299993\n",
      "FIRE:   50 18:11:19    -3910.257568       16.337082\n",
      "FIRE:   51 18:11:22    -3909.000488       22.879815\n",
      "FIRE:   52 18:11:25    -3907.358154       34.021286\n",
      "FIRE:   53 18:11:28    -3905.546631       46.522975\n",
      "FIRE:   54 18:11:31    -3904.279053       55.274389\n",
      "FIRE:   55 18:11:36    -3904.313477       54.891273\n",
      "FIRE:   56 18:11:39    -3905.661621       45.577203\n",
      "FIRE:   57 18:11:44    -3907.576416       34.018108\n",
      "FIRE:   58 18:11:49    -3909.268555       22.473788\n",
      "FIRE:   59 18:11:52    -3910.339111       29.109710\n",
      "FIRE:   60 18:11:57    -3910.685791       34.009043\n",
      "FIRE:   61 18:12:03    -3910.522949       14.870603\n",
      "FIRE:   62 18:12:06    -3910.181152       16.434002\n",
      "FIRE:   63 18:12:09    -3909.772949       24.943744\n",
      "FIRE:   64 18:12:12    -3909.265625       25.085170\n",
      "FIRE:   65 18:12:16    -3908.634766       15.485309\n",
      "FIRE:   66 18:12:21    -3907.947021       21.788665\n",
      "FIRE:   67 18:12:24    -3907.322021       34.745551\n",
      "FIRE:   68 18:12:27    -3906.915039       25.136386\n",
      "FIRE:   69 18:12:30    -3906.890625       18.883069\n",
      "FIRE:   70 18:12:33    -3907.220703       20.217194\n",
      "FIRE:   71 18:12:36    -3907.702637       25.503189\n",
      "FIRE:   72 18:12:39    -3908.112061       28.136889\n",
      "FIRE:   73 18:12:42    -3908.321533       21.650638\n",
      "FIRE:   74 18:12:45    -3908.336426       25.399037\n",
      "FIRE:   75 18:12:48    -3908.197021       25.758029\n",
      "FIRE:   76 18:12:51    -3907.980957       20.363734\n",
      "FIRE:   77 18:12:54    -3907.800537       23.261928\n",
      "FIRE:   78 18:12:57    -3907.708984       37.521770\n",
      "FIRE:   79 18:13:00    -3907.663818       14.918428\n",
      "FIRE:   80 18:13:03    -3907.711914       31.461759\n",
      "FIRE:   81 18:13:06    -3909.113037       13.937506\n",
      "FIRE:   82 18:13:09    -3910.882080       28.597897\n",
      "FIRE:   83 18:13:12    -3912.064697        7.336546\n",
      "FIRE:   84 18:13:15    -3913.561523       26.816651\n",
      "FIRE:   85 18:13:19    -3914.437744       17.177955\n",
      "FIRE:   86 18:13:22    -3914.812500       13.190303\n",
      "FIRE:   87 18:13:24    -3915.528809       11.200604\n",
      "FIRE:   88 18:13:28    -3915.608643       11.871712\n",
      "FIRE:   89 18:13:31    -3915.767334       16.247033\n",
      "FIRE:   90 18:13:33    -3915.995605       26.870808\n",
      "FIRE:   91 18:13:37    -3916.283936       25.701540\n",
      "FIRE:   92 18:13:40    -3916.302246       24.114080\n",
      "FIRE:   93 18:13:43    -3916.338379       16.585008\n",
      "FIRE:   94 18:13:46    -3916.393799        5.384503\n",
      "FIRE:   95 18:13:48    -3916.398926        4.862573\n",
      "FIRE:   96 18:13:53    -3916.407227        3.886288\n",
      "FIRE:   97 18:13:55    -3916.420654        3.811153\n",
      "FIRE:   98 18:13:58    -3916.439453        3.810204\n",
      "FIRE:   99 18:14:01    -3916.461914        3.809010\n",
      "FIRE:  100 18:14:07    -3916.488525        3.807556\n",
      "FIRE:  101 18:14:12    -3916.520020        3.805853\n",
      "FIRE:  102 18:14:16    -3916.558594        3.803661\n",
      "FIRE:  103 18:14:19    -3916.609619        3.800864\n",
      "FIRE:  104 18:14:22    -3916.671875        3.797298\n",
      "FIRE:  105 18:14:24    -3916.748047        3.792733\n",
      "FIRE:  106 18:14:27    -3916.843018        3.786849\n",
      "FIRE:  107 18:14:30    -3916.960205        3.779295\n",
      "FIRE:  108 18:14:33    -3917.104248        3.769546\n",
      "FIRE:  109 18:14:36    -3917.277832        3.756858\n",
      "FIRE:  110 18:14:40    -3917.488037        3.740241\n",
      "FIRE:  111 18:14:42    -3917.743896        3.718378\n",
      "FIRE:  112 18:14:45    -3918.049072        3.689374\n",
      "FIRE:  113 18:14:48    -3918.412842        3.650801\n",
      "FIRE:  114 18:14:52    -3918.844482        3.599250\n",
      "FIRE:  115 18:14:54    -3919.353027        3.530725\n",
      "FIRE:  116 18:14:57    -3919.943115        6.852144\n",
      "FIRE:  117 18:15:00    -3920.614014       23.389375\n",
      "FIRE:  118 18:15:03    -3921.343262       56.138824\n",
      "FIRE:  119 18:15:06    -3921.361572       18.767051\n",
      "FIRE:  120 18:15:09    -3921.396729       46.452472\n",
      "FIRE:  121 18:15:14    -3921.401367       40.758699\n",
      "FIRE:  122 18:15:17    -3921.409668       23.992617\n",
      "FIRE:  123 18:15:20    -3921.423584       11.617099\n",
      "FIRE:  124 18:15:23    -3921.424316       11.066237\n",
      "FIRE:  125 18:15:26    -3921.426270        9.985033\n",
      "FIRE:  126 18:15:29    -3921.429443        8.425618\n",
      "FIRE:  127 18:15:32    -3921.434082        6.510998\n",
      "FIRE:  128 18:15:35    -3921.439941        4.529379\n",
      "FIRE:  129 18:15:40    -3921.447266        3.211297\n",
      "FIRE:  130 18:15:43    -3921.455322        3.593402\n",
      "FIRE:  131 18:15:47    -3921.466064        5.370409\n",
      "FIRE:  132 18:15:50    -3921.479736        6.367985\n",
      "FIRE:  133 18:15:53    -3921.495605        6.251565\n",
      "FIRE:  134 18:15:56    -3921.512939        5.479597\n",
      "FIRE:  135 18:15:59    -3921.536865        5.400747\n",
      "FIRE:  136 18:16:02    -3921.565186        5.745932\n",
      "FIRE:  137 18:16:05    -3921.599365        4.505585\n",
      "FIRE:  138 18:16:07    -3921.641846        3.120541\n",
      "FIRE:  139 18:16:10    -3921.694336        4.309660\n",
      "FIRE:  140 18:16:13    -3921.756836        5.014020\n",
      "FIRE:  141 18:16:16    -3921.833496        3.082478\n",
      "FIRE:  142 18:16:19    -3921.924561        4.469286\n",
      "FIRE:  143 18:16:22    -3922.038330        3.721649\n",
      "FIRE:  144 18:16:25    -3922.174316        4.947844\n",
      "FIRE:  145 18:16:28    -3922.335449        3.763298\n",
      "FIRE:  146 18:16:31    -3922.529297        2.933696\n",
      "FIRE:  147 18:16:33    -3922.763184        3.614567\n",
      "FIRE:  148 18:16:36    -3923.041748       15.111733\n",
      "FIRE:  149 18:16:39    -3923.359131       50.455222\n",
      "FIRE:  150 18:16:42    -3923.367920        6.827654\n",
      "FIRE:  151 18:16:45    -3923.370605        5.812090\n",
      "FIRE:  152 18:16:48    -3923.375244        4.169397\n",
      "FIRE:  153 18:16:51    -3923.380615        2.935870\n",
      "FIRE:  154 18:16:53    -3923.388672        3.033712\n",
      "FIRE:  155 18:16:56    -3923.400146        3.111029\n",
      "FIRE:  156 18:16:59    -3923.412598        2.726914\n",
      "FIRE:  157 18:17:01    -3923.426025        2.723746\n",
      "FIRE:  158 18:17:04    -3923.444824        4.070706\n",
      "FIRE:  159 18:17:07    -3923.466797        3.954774\n",
      "FIRE:  160 18:17:10    -3923.496338        2.708684\n",
      "FIRE:  161 18:17:13    -3923.530029        2.701029\n",
      "FIRE:  162 18:17:15    -3923.573242        2.691528\n",
      "FIRE:  163 18:17:18    -3923.627197        2.878207\n",
      "FIRE:  164 18:17:21    -3923.691895        2.665323\n",
      "FIRE:  165 18:17:24    -3923.772217        2.647625\n",
      "FIRE:  166 18:17:27    -3923.870605        2.626014\n",
      "FIRE:  167 18:17:30    -3923.989990        2.599691\n",
      "FIRE:  168 18:17:33    -3924.134766        2.567763\n",
      "FIRE:  169 18:17:36    -3924.311035        4.616534\n",
      "FIRE:  170 18:17:39    -3924.521484       18.349884\n",
      "FIRE:  171 18:17:41    -3924.760498       72.686187\n",
      "FIRE:  172 18:17:44    -3924.766846       15.047576\n",
      "FIRE:  173 18:17:47    -3924.768799       10.729546\n",
      "FIRE:  174 18:17:50    -3924.770752        3.906969\n",
      "FIRE:  175 18:17:52    -3924.776367        6.032431\n",
      "FIRE:  176 18:17:55    -3924.776611        5.580055\n",
      "FIRE:  177 18:17:58    -3924.777100        4.720165\n",
      "FIRE:  178 18:18:01    -3924.778564        3.554466\n",
      "FIRE:  179 18:18:04    -3924.779785        2.429595\n",
      "FIRE:  180 18:18:07    -3924.782471        2.429215\n",
      "FIRE:  181 18:18:09    -3924.784912        2.428757\n",
      "FIRE:  182 18:18:12    -3924.787598        2.791366\n",
      "FIRE:  183 18:18:15    -3924.791016        3.365664\n",
      "FIRE:  184 18:18:18    -3924.795410        3.342757\n",
      "FIRE:  185 18:18:20    -3924.801025        2.525495\n",
      "FIRE:  186 18:18:23    -3924.807861        2.424366\n",
      "FIRE:  187 18:18:26    -3924.816406        2.422750\n",
      "FIRE:  188 18:18:29    -3924.825928        2.734302\n",
      "FIRE:  189 18:18:31    -3924.839844        2.816500\n",
      "FIRE:  190 18:18:34    -3924.856689        2.415308\n",
      "FIRE:  191 18:18:37    -3924.876221        2.411645\n",
      "FIRE:  192 18:18:41    -3924.900146        2.407187\n",
      "FIRE:  193 18:18:44    -3924.930664        2.401773\n",
      "FIRE:  194 18:18:47    -3924.966309        2.395193\n",
      "FIRE:  195 18:18:50    -3925.010986        2.387258\n",
      "FIRE:  196 18:18:53    -3925.063965        2.377692\n",
      "FIRE:  197 18:18:55    -3925.129639        2.366261\n",
      "FIRE:  198 18:18:58    -3925.208252        2.352659\n",
      "FIRE:  199 18:19:01    -3925.303223        2.794700\n",
      "FIRE:  200 18:19:04    -3925.418213        6.379900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 18:19:05    -3940.666504        0.521325\n",
      "FIRE:    1 18:19:06    -3940.683350        0.490062\n",
      "FIRE:    2 18:19:07    -3940.716553        0.432640\n",
      "FIRE:    3 18:19:08    -3940.758545        0.361904\n",
      "FIRE:    4 18:19:09    -3940.799805        0.325828\n",
      "FIRE:    5 18:19:09    -3940.837646        0.274775\n",
      "FIRE:    6 18:19:10    -3940.869629        0.205748\n",
      "FIRE:    7 18:19:11    -3940.892334        0.172087\n",
      "FIRE:    8 18:19:12    -3940.906006        0.158781\n",
      "FIRE:    9 18:19:12    -3940.905518        0.183580\n",
      "FIRE:   10 18:19:13    -3940.906982        0.179352\n",
      "FIRE:   11 18:19:14    -3940.909912        0.171014\n",
      "FIRE:   12 18:19:15    -3940.911865        0.158815\n",
      "FIRE:   13 18:19:17    -3940.915039        0.143131\n",
      "FIRE:   14 18:19:18    -3940.916016        0.124401\n",
      "FIRE:   15 18:19:19    -3940.919922        0.103205\n",
      "FIRE:   16 18:19:19    -3940.922119        0.080161\n",
      "FIRE:   17 18:19:20    -3940.923828        0.053500\n",
      "FIRE:   18 18:19:21    -3940.925781        0.041350\n",
      "FIRE:   19 18:19:22    -3940.927490        0.050360\n",
      "FIRE:   20 18:19:22    -3940.927246        0.049845\n",
      "FIRE:   21 18:19:23    -3940.928223        0.048814\n",
      "FIRE:   22 18:19:23    -3940.928223        0.047278\n",
      "FIRE:   23 18:19:24    -3940.927246        0.045276\n",
      "FIRE:   24 18:19:25    -3940.927734        0.042841\n",
      "FIRE:   25 18:19:25    -3940.927002        0.040014\n",
      "FIRE:   26 18:19:26    -3940.927734        0.036855\n",
      "FIRE:   27 18:19:26    -3940.928711        0.033061\n",
      "FIRE:   28 18:19:27    -3940.927734        0.031111\n",
      "FIRE:   29 18:19:28    -3940.928223        0.030500\n",
      "FIRE:   30 18:19:28    -3940.927734        0.029709\n",
      "FIRE:   31 18:19:29    -3940.929688        0.028605\n",
      "FIRE:   32 18:19:29    -3940.929199        0.026984\n",
      "FIRE:   33 18:19:30    -3940.928955        0.024590\n",
      "FIRE:   34 18:19:30    -3940.927246        0.021167\n",
      "FIRE:   35 18:19:31    -3940.927734        0.016509\n",
      "FIRE:   36 18:19:31    -3940.930176        0.010555\n",
      "FIRE:   37 18:19:32    -3940.929443        0.007593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 18:19:33    -3940.818848        0.429994\n",
      "FIRE:    1 18:19:33    -3940.839111        0.424538\n",
      "FIRE:    2 18:19:34    -3940.874512        0.412890\n",
      "FIRE:    3 18:19:34    -3940.922363        0.393411\n",
      "FIRE:    4 18:19:35    -3940.975586        0.363261\n",
      "FIRE:    5 18:19:36    -3941.027588        0.318527\n",
      "FIRE:    6 18:19:36    -3941.072266        0.255269\n",
      "FIRE:    7 18:19:37    -3941.103271        0.172500\n",
      "FIRE:    8 18:19:37    -3941.117676        0.079991\n",
      "FIRE:    9 18:19:38    -3941.110840        0.183846\n",
      "FIRE:   10 18:19:39    -3941.111572        0.180014\n",
      "FIRE:   11 18:19:39    -3941.113525        0.172450\n",
      "FIRE:   12 18:19:40    -3941.115723        0.161342\n",
      "FIRE:   13 18:19:41    -3941.118408        0.146967\n",
      "FIRE:   14 18:19:41    -3941.121094        0.129679\n",
      "FIRE:   15 18:19:42    -3941.125732        0.109900\n",
      "FIRE:   16 18:19:43    -3941.126465        0.088118\n",
      "FIRE:   17 18:19:44    -3941.130127        0.062402\n",
      "FIRE:   18 18:19:45    -3941.130371        0.033170\n",
      "FIRE:   19 18:19:45    -3941.130859        0.020407\n",
      "FIRE:   20 18:19:46    -3941.130859        0.020167\n",
      "FIRE:   21 18:19:47    -3941.131592        0.019674\n",
      "FIRE:   22 18:19:47    -3941.131348        0.018954\n",
      "FIRE:   23 18:19:48    -3941.131836        0.018009\n",
      "FIRE:   24 18:19:48    -3941.130859        0.016852\n",
      "FIRE:   25 18:19:49    -3941.130371        0.015493\n",
      "FIRE:   26 18:19:50    -3941.131104        0.014268\n",
      "FIRE:   27 18:19:50    -3941.130859        0.013358\n",
      "FIRE:   28 18:19:51    -3941.131836        0.013087\n",
      "FIRE:   29 18:19:51    -3941.130859        0.012771\n",
      "FIRE:   30 18:19:52    -3941.130615        0.012415\n",
      "FIRE:   31 18:19:52    -3941.132080        0.012013\n",
      "FIRE:   32 18:19:53    -3941.132324        0.011545\n",
      "FIRE:   33 18:19:53    -3941.133057        0.010992\n",
      "FIRE:   34 18:19:54    -3941.132324        0.010315\n",
      "FIRE:   35 18:19:55    -3941.133301        0.009449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Step     Time          Energy          fmax\n",
      "FIRE:    0 18:19:59    -3817.594238      492.128237\n",
      "FIRE:    1 18:20:02    -3819.157959      476.015182\n",
      "FIRE:    2 18:20:05    -3820.819092      458.141142\n",
      "FIRE:    3 18:20:08    -3822.368408      449.787244\n",
      "FIRE:    4 18:20:11    -3823.756348      449.800839\n",
      "FIRE:    5 18:20:13    -3825.143555      453.456813\n",
      "FIRE:    6 18:20:16    -3826.668945      460.320681\n",
      "FIRE:    7 18:20:19    -3828.635986      467.051817\n",
      "FIRE:    8 18:20:22    -3831.541016      468.737169\n",
      "FIRE:    9 18:20:25    -3835.761719      461.396051\n",
      "FIRE:   10 18:20:27    -3841.427246      442.330507\n",
      "FIRE:   11 18:20:30    -3848.399658      410.619707\n",
      "FIRE:   12 18:20:33    -3856.011475      360.986123\n",
      "FIRE:   13 18:20:36    -3862.225098      287.979911\n",
      "FIRE:   14 18:20:39    -3864.318115      214.944273\n",
      "FIRE:   15 18:20:44    -3863.635986      179.099144\n",
      "FIRE:   16 18:20:50    -3866.053955      198.773353\n",
      "FIRE:   17 18:20:55    -3875.448730      177.031817\n",
      "FIRE:   18 18:20:57    -3888.652100      123.453893\n",
      "FIRE:   19 18:21:00    -3899.718506       71.819297\n",
      "FIRE:   20 18:21:03    -3906.634521       38.091775\n",
      "FIRE:   21 18:21:06    -3910.641846       29.072172\n",
      "FIRE:   22 18:21:09    -3913.512207       25.674029\n",
      "FIRE:   23 18:21:12    -3915.859863       20.068317\n",
      "FIRE:   24 18:21:14    -3917.651855       17.303187\n",
      "FIRE:   25 18:21:17    -3918.888672       13.468815\n",
      "FIRE:   26 18:21:20    -3919.658691        9.786664\n",
      "FIRE:   27 18:21:23    -3920.040527        8.099138\n",
      "FIRE:   28 18:21:25    -3920.088135        8.268452\n",
      "FIRE:   29 18:21:28    -3919.876709        9.228991\n",
      "FIRE:   30 18:21:31    -3919.502930       11.472747\n",
      "FIRE:   31 18:21:35    -3919.062500       13.299655\n",
      "FIRE:   32 18:21:38    -3918.625000       14.285746\n",
      "FIRE:   33 18:21:42    -3918.207031       14.251626\n",
      "FIRE:   34 18:21:45    -3917.767090       14.987267\n",
      "FIRE:   35 18:21:49    -3917.236816       16.072153\n",
      "FIRE:   36 18:21:54    -3916.579346       17.162430\n",
      "FIRE:   37 18:21:58    -3915.831299       18.356858\n",
      "FIRE:   38 18:22:01    -3915.093506       20.474895\n",
      "FIRE:   39 18:22:04    -3914.492188       22.110622\n",
      "FIRE:   40 18:22:07    -3914.098877       21.273831\n",
      "FIRE:   41 18:22:10    -3913.892822       19.325140\n",
      "FIRE:   42 18:22:13    -3913.804443       17.674972\n",
      "FIRE:   43 18:22:15    -3913.776367       15.727342\n",
      "FIRE:   44 18:22:18    -3913.786377       13.858993\n",
      "FIRE:   45 18:22:21    -3913.829102       12.255275\n",
      "FIRE:   46 18:22:24    -3913.915527       11.445602\n",
      "FIRE:   47 18:22:27    -3914.080566       11.232350\n",
      "FIRE:   48 18:22:30    -3914.356201       13.015098\n",
      "FIRE:   49 18:22:33    -3914.740234       14.194761\n",
      "FIRE:   50 18:22:36    -3915.140381       12.353666\n",
      "FIRE:   51 18:22:39    -3915.406738       12.638346\n",
      "FIRE:   52 18:22:43    -3915.475830       17.458019\n",
      "FIRE:   53 18:22:46    -3915.467773       21.950789\n",
      "FIRE:   54 18:22:49    -3915.649902       24.460895\n",
      "FIRE:   55 18:22:52    -3916.247314       24.379180\n",
      "FIRE:   56 18:22:55    -3917.215576       21.242100\n",
      "FIRE:   57 18:22:58    -3918.269043       16.516610\n",
      "FIRE:   58 18:23:01    -3919.108398       19.559533\n",
      "FIRE:   59 18:23:04    -3919.593994       20.628696\n",
      "FIRE:   60 18:23:08    -3919.723389       15.097378\n",
      "FIRE:   61 18:23:11    -3919.606445       11.351082\n",
      "FIRE:   62 18:23:14    -3919.344482       23.301317\n",
      "FIRE:   63 18:23:17    -3918.983154       20.363513\n",
      "FIRE:   64 18:23:20    -3918.566895       20.225729\n",
      "FIRE:   65 18:23:23    -3918.149170       24.202973\n",
      "FIRE:   66 18:23:26    -3917.771484       26.537396\n",
      "FIRE:   67 18:23:28    -3917.459961       19.408443\n",
      "FIRE:   68 18:23:32    -3917.262451       24.761586\n",
      "FIRE:   69 18:23:35    -3917.156250       31.689244\n",
      "FIRE:   70 18:23:37    -3917.075684       14.031882\n",
      "FIRE:   71 18:23:41    -3916.994873       31.278503\n",
      "FIRE:   72 18:23:44    -3916.827637       26.790406\n",
      "FIRE:   73 18:23:47    -3916.556641       25.313052\n",
      "FIRE:   74 18:23:50    -3917.274170       61.635317\n",
      "FIRE:   75 18:23:54    -3917.558105       44.293651\n",
      "FIRE:   76 18:23:57    -3917.885010       67.034491\n",
      "FIRE:   77 18:24:00    -3918.139648      121.135778\n",
      "FIRE:   78 18:24:03    -3918.247070       33.866860\n",
      "FIRE:   79 18:24:06    -3918.274170       36.245848\n",
      "FIRE:   80 18:24:11    -3918.326660       41.752883\n",
      "FIRE:   81 18:24:15    -3918.403564       42.865368\n",
      "FIRE:   82 18:24:19    -3918.507812       28.387677\n",
      "FIRE:   83 18:24:23    -3918.637207      103.890439\n",
      "FIRE:   84 18:24:28    -3918.644043       90.127599\n",
      "FIRE:   85 18:24:31    -3918.656006       52.662875\n",
      "FIRE:   86 18:24:34    -3918.676270       10.311383\n",
      "FIRE:   87 18:24:36    -3918.711182       27.999617\n",
      "FIRE:   88 18:24:40    -3918.712402       27.338406\n",
      "FIRE:   89 18:24:42    -3918.715576       25.990131\n",
      "FIRE:   90 18:24:45    -3918.720947       23.907897\n",
      "FIRE:   91 18:24:48    -3918.727051       21.037824\n",
      "FIRE:   92 18:24:50    -3918.735107       17.347117\n",
      "FIRE:   93 18:24:53    -3918.745117       12.883589\n",
      "FIRE:   94 18:24:56    -3918.755859        7.970098\n",
      "FIRE:   95 18:24:59    -3918.772461        5.443920\n",
      "FIRE:   96 18:25:01    -3918.792480        8.520253\n",
      "FIRE:   97 18:25:04    -3918.819824       14.437023\n",
      "FIRE:   98 18:25:07    -3918.820801       14.117989\n",
      "FIRE:   99 18:25:10    -3918.822021       13.489271\n",
      "FIRE:  100 18:25:13    -3918.824463       12.567024\n",
      "FIRE:  101 18:25:17    -3918.826416       11.381751\n",
      "FIRE:  102 18:25:20    -3918.829102        9.969044\n",
      "FIRE:  103 18:25:23    -3918.833496        8.375048\n",
      "FIRE:  104 18:25:26    -3918.838379        6.656530\n",
      "FIRE:  105 18:25:29    -3918.844971        5.408174\n",
      "FIRE:  106 18:25:32    -3918.852783        5.404265\n",
      "FIRE:  107 18:25:36    -3918.863281        5.399275\n",
      "FIRE:  108 18:25:40    -3918.876465        5.392979\n",
      "FIRE:  109 18:25:43    -3918.892822        5.385147\n",
      "FIRE:  110 18:25:46    -3918.911133        5.375522\n",
      "FIRE:  111 18:25:48    -3918.936035        5.812973\n",
      "FIRE:  112 18:25:51    -3918.964355        5.529686\n",
      "FIRE:  113 18:25:54    -3918.999023        5.332592\n",
      "FIRE:  114 18:25:57    -3919.042236        5.311891\n",
      "FIRE:  115 18:26:00    -3919.093994        5.286697\n",
      "FIRE:  116 18:26:03    -3919.155518        5.256034\n",
      "FIRE:  117 18:26:06    -3919.229492        5.389822\n",
      "FIRE:  118 18:26:09    -3919.322266        5.174230\n",
      "FIRE:  119 18:26:12    -3919.429932        5.120441\n",
      "FIRE:  120 18:26:15    -3919.563477        5.055693\n",
      "FIRE:  121 18:26:18    -3919.718750        4.978247\n",
      "FIRE:  122 18:26:21    -3919.904785        4.885805\n",
      "FIRE:  123 18:26:24    -3920.125732        4.775818\n",
      "FIRE:  124 18:26:28    -3920.384521        4.645966\n",
      "FIRE:  125 18:26:31    -3920.688477        4.493291\n",
      "FIRE:  126 18:26:34    -3921.040527        5.254691\n",
      "FIRE:  127 18:26:37    -3921.445068        4.114033\n",
      "FIRE:  128 18:26:41    -3921.909668        3.886475\n",
      "FIRE:  129 18:26:44    -3922.435547        7.752973\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m multi_neb_results \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_neb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_multiple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvacancy_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_vac_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mnum_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneb_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdyneb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mclimb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                       \u001b[49m\u001b[43msave_xyz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                       \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./multi_vac_diff_results/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrelax_fmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrelax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneb_fmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.04\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneb_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrng_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Packages/forge/forge/workflows/neb.py:827\u001b[0m, in \u001b[0;36mVacancyDiffusion.run_multiple\u001b[0;34m(self, vacancy_indices, n_nearest, n_next_nearest, num_images, neb_method, climb, relax_fmax, relax_steps, neb_fmax, neb_steps, save_xyz, output_dir, rng_seed)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;66;03m# Process next-nearest neighbors\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target_idx \u001b[38;5;129;01min\u001b[39;00m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnnn\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 827\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvacancy_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvac_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneb_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneb_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclimb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclimb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelax_fmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelax_fmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneb_fmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneb_fmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneb_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneb_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_xyz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_xyz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;66;03m# Mark as next-nearest neighbor\u001b[39;00m\n\u001b[1;32m    841\u001b[0m     result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_nearest_neighbor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Packages/forge/forge/workflows/neb.py:699\u001b[0m, in \u001b[0;36mVacancyDiffusion.run_single\u001b[0;34m(self, vacancy_index, target_index, num_images, neb_method, climb, relax_fmax, relax_steps, neb_fmax, neb_steps, save_xyz, output_dir)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;66;03m# Run NEB with seed\u001b[39;00m\n\u001b[1;32m    686\u001b[0m neb_calc \u001b[38;5;241m=\u001b[39m NEBCalculation(\n\u001b[1;32m    687\u001b[0m     start_atoms\u001b[38;5;241m=\u001b[39mstart_atoms,\n\u001b[1;32m    688\u001b[0m     end_atoms\u001b[38;5;241m=\u001b[39mend_atoms,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    697\u001b[0m     steps\u001b[38;5;241m=\u001b[39mneb_steps\n\u001b[1;32m    698\u001b[0m )\n\u001b[0;32m--> 699\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mneb_calc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;66;03m# Combine results and metadata\u001b[39;00m\n\u001b[1;32m    702\u001b[0m output \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata,\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbarrier\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mbarrier,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_nearest_neighbor\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Add this flag for the analyzer\u001b[39;00m\n\u001b[1;32m    711\u001b[0m }\n",
      "File \u001b[0;32m~/Packages/forge/forge/workflows/neb.py:123\u001b[0m, in \u001b[0;36mNEBCalculation.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[1;32m    122\u001b[0m opt \u001b[38;5;241m=\u001b[39m FIRE(neb)\n\u001b[0;32m--> 123\u001b[0m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Get energies for intermediate images\u001b[39;00m\n\u001b[1;32m    126\u001b[0m intermediate_energies \u001b[38;5;241m=\u001b[39m [image\u001b[38;5;241m.\u001b[39mget_potential_energy() \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/ase/optimize/optimize.py:430\u001b[0m, in \u001b[0;36mOptimizer.run\u001b[0;34m(self, fmax, steps)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run optimizer.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m    True if the forces on atoms are converged.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmax \u001b[38;5;241m=\u001b[39m fmax\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDynamics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/ase/optimize/optimize.py:275\u001b[0m, in \u001b[0;36mDynamics.run\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, steps\u001b[38;5;241m=\u001b[39mDEFAULT_MAX_STEPS):\n\u001b[1;32m    258\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run dynamics algorithm.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m    This method will return when the forces on all individual\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        True if the forces on atoms are converged.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconverged\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDynamics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mirun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converged\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/ase/optimize/optimize.py:246\u001b[0m, in \u001b[0;36mDynamics.irun\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# run the algorithm until converged or max_steps reached\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_converged \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnsteps \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# compute the next step\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnsteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# log the step\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/ase/optimize/fire.py:216\u001b[0m, in \u001b[0;36mFIRE.step\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    214\u001b[0m     dr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxstep \u001b[38;5;241m*\u001b[39m dr \u001b[38;5;241m/\u001b[39m normdr\n\u001b[1;32m    215\u001b[0m r \u001b[38;5;241m=\u001b[39m optimizable\u001b[38;5;241m.\u001b[39mget_positions()\n\u001b[0;32m--> 216\u001b[0m \u001b[43moptimizable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdump((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/ase/mep/neb.py:277\u001b[0m, in \u001b[0;36mNEBOptimizable.set_positions\u001b[0;34m(self, positions)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_positions\u001b[39m(\u001b[38;5;28mself\u001b[39m, positions):\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/ase/mep/neb.py:710\u001b[0m, in \u001b[0;36mDyNEB.set_positions\u001b[0;34m(self, positions)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     forces_dyn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fmax_all\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m forces_dyn[i] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmax:\n\u001b[1;32m    712\u001b[0m         n1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnatoms\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/ase/mep/neb.py:722\u001b[0m, in \u001b[0;36mDyNEB._fmax_all\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Store maximum force acting on each image in list. This is used in\u001b[39;00m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;124;03m   the dynamic optimization routine in the set_positions() function.\"\"\"\u001b[39;00m\n\u001b[1;32m    721\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnatoms\n\u001b[0;32m--> 722\u001b[0m forces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_forces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m fmax_images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    724\u001b[0m     np\u001b[38;5;241m.\u001b[39msqrt((forces[n \u001b[38;5;241m*\u001b[39m i:n \u001b[38;5;241m+\u001b[39m n \u001b[38;5;241m*\u001b[39m i] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnimages \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m)]\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fmax_images\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/ase/mep/neb.py:729\u001b[0m, in \u001b[0;36mDyNEB.get_forces\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_forces\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 729\u001b[0m     forces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_forces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_relaxation:\n\u001b[1;32m    731\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m forces\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/ase/mep/neb.py:449\u001b[0m, in \u001b[0;36mBaseNEB.get_forces\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# Do all images - one at a time:\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnimages \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 449\u001b[0m         forces[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_forces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m         energies[i] \u001b[38;5;241m=\u001b[39m images[i]\u001b[38;5;241m.\u001b[39mget_potential_energy()\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/ase/atoms.py:812\u001b[0m, in \u001b[0;36mAtoms.get_forces\u001b[0;34m(self, apply_constraint, md)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAtoms object has no calculator.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 812\u001b[0m forces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_forces\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apply_constraint:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;66;03m# We need a special md flag here because for MD we want\u001b[39;00m\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;66;03m# to skip real constraints but include special \"constraints\"\u001b[39;00m\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;66;03m# Like Hookean.\u001b[39;00m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m constraint \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraints:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/ase/calculators/abc.py:30\u001b[0m, in \u001b[0;36mGetPropertiesMixin.get_forces\u001b[0;34m(self, atoms)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_forces\u001b[39m(\u001b[38;5;28mself\u001b[39m, atoms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_property\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforces\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matoms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/ase/calculators/calculator.py:538\u001b[0m, in \u001b[0;36mBaseCalculator.get_property\u001b[0;34m(self, name, atoms, allow_calculation)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cache:\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matoms \u001b[38;5;241m=\u001b[39m atoms\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate\u001b[49m\u001b[43m(\u001b[49m\u001b[43matoms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_changes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults:\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# For some reason the calculator was not able to do what we want,\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;66;03m# and that is OK.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PropertyNotImplementedError(\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not present in this \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalculation\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)\n\u001b[1;32m    545\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/calculators/mace.py:252\u001b[0m, in \u001b[0;36mMACECalculator.calculate\u001b[0;34m(self, atoms, properties, system_changes)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels):\n\u001b[1;32m    251\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clone_batch(batch_base)\n\u001b[0;32m--> 252\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_stress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_stress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_compile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMACE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnergyDipoleMACE\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;66;03m#print(f\"out['node_energy'] shape: {out['node_energy'].shape}\")\u001b[39;00m\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m#print(f\"node_e0 shape: {node_e0.shape}\")\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         ret_tensors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menergies\u001b[39m\u001b[38;5;124m\"\u001b[39m][i] \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/modules/models.py:431\u001b[0m, in \u001b[0;36mScaleShiftMACE.forward\u001b[0;34m(self, data, training, compute_force, compute_virials, compute_stress, compute_displacement, compute_hessian)\u001b[0m\n\u001b[1;32m    429\u001b[0m total_energy \u001b[38;5;241m=\u001b[39m e0 \u001b[38;5;241m+\u001b[39m inter_e\n\u001b[1;32m    430\u001b[0m node_energy \u001b[38;5;241m=\u001b[39m node_e0 \u001b[38;5;241m+\u001b[39m node_inter_es\n\u001b[0;32m--> 431\u001b[0m forces, virials, stress, hessian \u001b[38;5;241m=\u001b[39m \u001b[43mget_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43menergy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minter_e\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpositions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplacement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_force\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_force\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_virials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_virials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_stress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_stress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_hessian\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m output \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_energy,\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_energy\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_energy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_feats_out,\n\u001b[1;32m    452\u001b[0m }\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/modules/utils.py:183\u001b[0m, in \u001b[0;36mget_outputs\u001b[0;34m(energy, positions, displacement, cell, training, compute_force, compute_virials, compute_stress, compute_hessian)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_outputs\u001b[39m(\n\u001b[1;32m    167\u001b[0m     energy: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    168\u001b[0m     positions: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m     Optional[torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    181\u001b[0m ]:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (compute_virials \u001b[38;5;129;01mor\u001b[39;00m compute_stress) \u001b[38;5;129;01mand\u001b[39;00m displacement \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m         forces, virials, stress \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_forces_virials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m            \u001b[49m\u001b[43menergy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menergy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplacement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompute_stress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_stress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompute_hessian\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m compute_force:\n\u001b[1;32m    192\u001b[0m         forces, virials, stress \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    193\u001b[0m             compute_forces(\n\u001b[1;32m    194\u001b[0m                 energy\u001b[38;5;241m=\u001b[39menergy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    200\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/mace/modules/utils.py:51\u001b[0m, in \u001b[0;36mcompute_forces_virials\u001b[0;34m(energy, positions, displacement, cell, training, compute_stress)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_forces_virials\u001b[39m(\n\u001b[1;32m     43\u001b[0m     energy: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m     44\u001b[0m     positions: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     compute_stress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m     50\u001b[0m     grad_outputs: List[Optional[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mones_like(energy)]\n\u001b[0;32m---> 51\u001b[0m     forces, virials \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43menergy\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# [n_graphs, ]\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplacement\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# [n_nodes, 3]\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Make sure the graph is not destroyed during training\u001b[39;49;00m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Create graph for second derivative\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     stress \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(displacement)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compute_stress \u001b[38;5;129;01mand\u001b[39;00m virials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mace/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "multi_neb_results = multi_neb.run_multiple(vacancy_indices=multi_vac_indices, \n",
    "                       num_images=5, \n",
    "                       neb_method='dyneb',\n",
    "                       climb=True,\n",
    "                       save_xyz=True,\n",
    "                       output_dir=Path(\"./multi_vac_diff_results/\"),\n",
    "                       relax_fmax=0.01,\n",
    "                       relax_steps=200,\n",
    "                       neb_fmax=0.05,\n",
    "                       neb_steps=200,\n",
    "                       rng_seed=seed\n",
    "                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting analysis on multiple vacancy diffusion calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
